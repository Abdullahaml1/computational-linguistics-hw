{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c4e367-076a-4220-b9f8-5fd0ab0aca14",
   "metadata": {},
   "source": [
    "# Binary Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c69afb-1036-4df5-8172-d39852be298b",
   "metadata": {},
   "source": [
    "# Top Test Accuracy \n",
    "**Selected Features with normalization no any thing else**\n",
    "```\n",
    "Train logP=-168.695843 Test logP=-12.268658 Train Acc=0.924812 Test Acc=0.969925 TrainLoss=0.228737 TestLoss=0.133082\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec8d08-6009-4d3d-afd7-544a1e8f26b3",
   "metadata": {},
   "source": [
    "## Learning Rate schedules:\n",
    "* time based\n",
    "* exponenetial\n",
    "* step\n",
    "* adaptive learning rate:\n",
    "    * Momentum\n",
    "    * adagrade\n",
    "    * RMS prob\n",
    "    * Adam\n",
    "    * Deep learning learning rate (using LSTMs, [reiformcnt learning](https://arxiv.org/pdf/1909.09712.pdf)\n",
    "---------------\n",
    "| Adaptive Learning Rate Schedule | Non adaptive |\n",
    "|------------|--------------|\n",
    "|faster in convergence | slower in convergence |\n",
    "|generalizes worst in simple problems and images (data close to mean -> Guassian) and bad test and train loss [[1]](https://proceedings.neurips.cc/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf)  | generalizes better in simple problems and images (data close to mean -> Guassian) and better test and train l\n",
    "|better in heavy tailed didtribution (date is very var from mean) like: BERT[[2]](https://proceedings.neurips.cc/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf) | worst |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3016459-1a66-40ba-a109-22e9d05c2fcc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf32e4-c419-4071-8f25-a5863c7d50e5",
   "metadata": {},
   "source": [
    "## All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca29b4-edda-4557-9971-4d24232b07f9",
   "metadata": {},
   "source": [
    "### Training all features without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4551b246-56ea-420c-9f93-45488e109a25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-521.886868 Test logP=-139.687949 Train Acc=0.869361 Test Acc=0.744361 TrainLoss=0.701773 TestLoss=1.515146\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-226.514885 Test logP=-87.782940 Train Acc=0.931391 Test Acc=0.796992 TrainLoss=0.304714 TestLoss=0.952199\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-124.998800 Test logP=-70.294938 Train Acc=0.959586 Test Acc=0.864662 TrainLoss=0.169409 TestLoss=0.762496\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-74.796920 Test logP=-66.716665 Train Acc=0.970865 Test Acc=0.857143 TrainLoss=0.101418 TestLoss=0.723682\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-57.068593 Test logP=-63.854687 Train Acc=0.976504 Test Acc=0.857143 TrainLoss=0.077380 TestLoss=0.692640\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-47.332756 Test logP=-61.826434 Train Acc=0.980263 Test Acc=0.864662 TrainLoss=0.064179 TestLoss=0.670635\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-39.166191 Test logP=-60.750814 Train Acc=0.984023 Test Acc=0.864662 TrainLoss=0.053106 TestLoss=0.658963\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-32.965126 Test logP=-59.420194 Train Acc=0.986842 Test Acc=0.864662 TrainLoss=0.044698 TestLoss=0.644535\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-28.570416 Test logP=-58.226355 Train Acc=0.987782 Test Acc=0.872180 TrainLoss=0.038739 TestLoss=0.631586\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-25.249203 Test logP=-57.661908 Train Acc=0.988722 Test Acc=0.872180 TrainLoss=0.034236 TestLoss=0.625458\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-21.976233 Test logP=-57.000650 Train Acc=0.991541 Test Acc=0.887218 TrainLoss=0.029798 TestLoss=0.618289\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-19.503346 Test logP=-56.458043 Train Acc=0.993421 Test Acc=0.887218 TrainLoss=0.026445 TestLoss=0.612404\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Train logP=-17.461800 Test logP=-56.020263 Train Acc=0.993421 Test Acc=0.887218 TrainLoss=0.023677 TestLoss=0.607650\n",
      "----------------------------\n",
      "Epoch:14\n",
      "Train logP=-15.383930 Test logP=-55.659509 Train Acc=0.995301 Test Acc=0.887218 TrainLoss=0.020859 TestLoss=0.603740\n",
      "----------------------------\n",
      "Epoch:15\n",
      "Train logP=-14.029501 Test logP=-55.018584 Train Acc=0.995301 Test Acc=0.894737 TrainLoss=0.019023 TestLoss=0.596787\n",
      "----------------------------\n",
      "Epoch:16\n",
      "Train logP=-12.592777 Test logP=-54.668470 Train Acc=0.997180 Test Acc=0.894737 TrainLoss=0.017075 TestLoss=0.592986\n",
      "----------------------------\n",
      "Epoch:17\n",
      "Train logP=-11.424959 Test logP=-54.242125 Train Acc=0.997180 Test Acc=0.902256 TrainLoss=0.015491 TestLoss=0.588362\n",
      "----------------------------\n",
      "Epoch:18\n",
      "Train logP=-10.537664 Test logP=-53.770922 Train Acc=0.997180 Test Acc=0.902256 TrainLoss=0.014288 TestLoss=0.583249\n",
      "----------------------------\n",
      "Epoch:19\n",
      "Train logP=-9.636531 Test logP=-53.538373 Train Acc=0.997180 Test Acc=0.909774 TrainLoss=0.013066 TestLoss=0.580725\n",
      "----------------------------\n",
      "Epoch:20\n",
      "Train logP=-8.886958 Test logP=-53.266458 Train Acc=0.997180 Test Acc=0.909774 TrainLoss=0.012050 TestLoss=0.577775\n",
      "----------------------------\n",
      "Epoch:21\n",
      "Train logP=-8.288194 Test logP=-52.937652 Train Acc=0.997180 Test Acc=0.909774 TrainLoss=0.011238 TestLoss=0.574206\n",
      "----------------------------\n",
      "Epoch:22\n",
      "Train logP=-7.672359 Test logP=-52.821213 Train Acc=0.997180 Test Acc=0.909774 TrainLoss=0.010403 TestLoss=0.572942\n",
      "----------------------------\n",
      "Epoch:23\n",
      "Train logP=-7.149912 Test logP=-52.663712 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.009695 TestLoss=0.571232\n",
      "----------------------------\n",
      "Epoch:24\n",
      "Train logP=-6.705935 Test logP=-52.397354 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.009093 TestLoss=0.568342\n",
      "----------------------------\n",
      "Epoch:25\n",
      "Train logP=-6.270186 Test logP=-52.265040 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.008502 TestLoss=0.566907\n",
      "----------------------------\n",
      "Epoch:26\n",
      "Train logP=-5.843606 Test logP=-52.195037 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.007923 TestLoss=0.566144\n",
      "----------------------------\n",
      "Epoch:27\n",
      "Train logP=-5.493595 Test logP=-51.921912 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.007449 TestLoss=0.563180\n",
      "----------------------------\n",
      "Epoch:28\n",
      "Train logP=-5.141542 Test logP=-51.901252 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.006971 TestLoss=0.562958\n",
      "----------------------------\n",
      "Epoch:29\n",
      "Train logP=-4.795662 Test logP=-51.732506 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.006503 TestLoss=0.561123\n",
      "----------------------------\n",
      "Epoch:30\n",
      "Train logP=-4.524147 Test logP=-51.637029 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.006134 TestLoss=0.560088\n",
      "----------------------------\n",
      "Epoch:31\n",
      "Train logP=-4.250421 Test logP=-51.589675 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.005763 TestLoss=0.559575\n",
      "----------------------------\n",
      "Epoch:32\n",
      "Train logP=-3.993836 Test logP=-51.471290 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.005415 TestLoss=0.558287\n",
      "----------------------------\n",
      "Epoch:33\n",
      "Train logP=-3.776401 Test logP=-51.417970 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.005120 TestLoss=0.557710\n",
      "----------------------------\n",
      "Epoch:34\n",
      "Early Stop\n",
      "Train logP=-3.607897 Test logP=-51.435974 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.004892 TestLoss=0.557905\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=2000  --step=0.08 --early_stop=1 --normalize=no \\\n",
    "    --log=no --log_step=200 --plot_name=all_features_no_normalization \\\n",
    "    --save_weights_path=weights/all_features_no_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52930e-bca4-492c-b77a-5fabc710f41d",
   "metadata": {},
   "source": [
    "![plot](figures/all_features_no_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5eb9f-3d00-422e-88dc-ff79dbecca28",
   "metadata": {},
   "source": [
    "### Training All features with Noramlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1795b41e-246a-4d2d-84e1-2cb89730a030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-1420.191893 Test logP=-441.904951 Train Acc=0.899436 Test Acc=0.774436 TrainLoss=1.800750 TestLoss=4.513933\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-577.089904 Test logP=-313.043072 Train Acc=0.958647 Test Acc=0.812030 TrainLoss=0.735404 TestLoss=3.224215\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-230.104997 Test logP=-264.091017 Train Acc=0.980263 Test Acc=0.842105 TrainLoss=0.297457 TestLoss=2.709858\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-90.458810 Test logP=-273.853875 Train Acc=0.993421 Test Acc=0.834586 TrainLoss=0.115412 TestLoss=2.831325\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-56.232588 Test logP=-275.117839 Train Acc=0.995301 Test Acc=0.834586 TrainLoss=0.073656 TestLoss=2.825542\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-29.995159 Test logP=-273.283748 Train Acc=0.997180 Test Acc=0.842105 TrainLoss=0.039644 TestLoss=2.804382\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-0.690955 Test logP=-266.554652 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000937 TestLoss=2.744959\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-0.623940 Test logP=-257.498769 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000846 TestLoss=2.645692\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Early Stop\n",
      "Train logP=-0.039144 Test logP=-259.866864 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000053 TestLoss=2.672140\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=100  --step=0.08 --early_stop=2 --normalize=yes \\\n",
    "    --log=no --log_step=200 --plot_name=all_features_with_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d4306-094d-4022-97bd-6438ef923a5b",
   "metadata": {},
   "source": [
    "![all_normalized](figures/all_features_with_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4ce1c-050f-4372-be08-9813e62a3f63",
   "metadata": {},
   "source": [
    "## Selected Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74b8f0-cd6e-4884-b465-4087a682c6b0",
   "metadata": {},
   "source": [
    "### Traing selected features without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888d5e74-fa5a-42c5-b205-16e373915c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-376.803548 Test logP=-33.523141 Train Acc=0.824248 Test Acc=0.894737 TrainLoss=0.510914 TestLoss=0.363637\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-294.315856 Test logP=-21.720168 Train Acc=0.869361 Test Acc=0.924812 TrainLoss=0.399068 TestLoss=0.235606\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-263.643651 Test logP=-17.809771 Train Acc=0.886278 Test Acc=0.939850 TrainLoss=0.357479 TestLoss=0.193188\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-233.106118 Test logP=-15.712409 Train Acc=0.905075 Test Acc=0.939850 TrainLoss=0.316072 TestLoss=0.170438\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-229.082612 Test logP=-15.098323 Train Acc=0.907895 Test Acc=0.939850 TrainLoss=0.310617 TestLoss=0.163776\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-228.302310 Test logP=-14.971840 Train Acc=0.909774 Test Acc=0.947368 TrainLoss=0.309559 TestLoss=0.162404\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-225.645991 Test logP=-14.926568 Train Acc=0.910714 Test Acc=0.939850 TrainLoss=0.305957 TestLoss=0.161913\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-206.189496 Test logP=-14.245021 Train Acc=0.912594 Test Acc=0.939850 TrainLoss=0.279576 TestLoss=0.154520\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-206.125661 Test logP=-14.311458 Train Acc=0.914474 Test Acc=0.939850 TrainLoss=0.279489 TestLoss=0.155241\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-209.889328 Test logP=-14.541022 Train Acc=0.916353 Test Acc=0.939850 TrainLoss=0.284592 TestLoss=0.157731\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-194.502566 Test logP=-14.032557 Train Acc=0.918233 Test Acc=0.939850 TrainLoss=0.263729 TestLoss=0.152216\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Early Stop\n",
      "Train logP=-196.152397 Test logP=-14.179428 Train Acc=0.921053 Test Acc=0.947368 TrainLoss=0.265966 TestLoss=0.153809\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=200  --step=.08 --early_stop=3 --normalize=no \\\n",
    "    --log=no --log_step=50 --plot_name=selected_features_no_normalization \\\n",
    "    --save_weights_path='weights/selected_features_no_normalization' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e58cd8-945a-4bd7-b7d8-8eb2d61e5fee",
   "metadata": {},
   "source": [
    "![selected_no_normalization](figures/selected_features_no_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0638cbc-3da5-4d6d-a101-4a99cd6e82ad",
   "metadata": {},
   "source": [
    "### Trainign Selected Features with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "456b1814-b331-4359-a504-da6b4458c3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "    Update 1\tTP -4868.335080\tHP -467.869872\tTA 0.471805\tHA 0.533835\t lr 0.080000\n",
      "    Update 201\tTP -1381.331320\tHP -129.813547\tTA 0.767857\tHA 0.812030\t lr 0.080000\n",
      "    Update 401\tTP -791.024044\tHP -59.193294\tTA 0.828947\tHA 0.909774\t lr 0.080000\n",
      "    Update 601\tTP -385.155447\tHP -19.255807\tTA 0.880639\tHA 0.954887\t lr 0.080000\n",
      "    Update 801\tTP -265.245576\tHP -15.788990\tTA 0.906015\tHA 0.947368\t lr 0.080000\n",
      "    Update 1001\tTP -242.604030\tHP -13.608165\tTA 0.910714\tHA 0.954887\t lr 0.080000\n",
      "Train logP=-242.604030 Test logP=-13.608165 Train Acc=0.910714 Test Acc=0.954887 TrainLoss=0.327237 TestLoss=0.147612\n",
      "----------------------------\n",
      "Epoch:2\n",
      "    Update 1201\tTP -247.129708\tHP -16.365733\tTA 0.911654\tHA 0.954887\t lr 0.080000\n",
      "    Update 1401\tTP -230.016644\tHP -16.194813\tTA 0.909774\tHA 0.954887\t lr 0.080000\n",
      "    Update 1601\tTP -213.073879\tHP -14.124529\tTA 0.913534\tHA 0.962406\t lr 0.080000\n",
      "    Update 1801\tTP -171.760137\tHP -11.596382\tTA 0.921053\tHA 0.969925\t lr 0.080000\n",
      "    Update 2001\tTP -191.911454\tHP -13.545474\tTA 0.918233\tHA 0.969925\t lr 0.080000\n",
      "Train logP=-191.911454 Test logP=-13.545474 Train Acc=0.918233 Test Acc=0.969925 TrainLoss=0.260216 TestLoss=0.146932\n",
      "----------------------------\n",
      "Epoch:3\n",
      "    Update 2201\tTP -212.903497\tHP -15.579409\tTA 0.918233\tHA 0.969925\t lr 0.080000\n",
      "    Update 2401\tTP -216.655000\tHP -16.635138\tTA 0.917293\tHA 0.962406\t lr 0.080000\n",
      "    Update 2601\tTP -176.547068\tHP -14.135285\tTA 0.918233\tHA 0.962406\t lr 0.080000\n",
      "    Update 2801\tTP -204.492229\tHP -15.385857\tTA 0.921053\tHA 0.969925\t lr 0.080000\n",
      "    Update 3001\tTP -168.695843\tHP -12.268658\tTA 0.924812\tHA 0.969925\t lr 0.080000\n",
      "Train logP=-168.695843 Test logP=-12.268658 Train Acc=0.924812 Test Acc=0.969925 TrainLoss=0.228737 TestLoss=0.133082\n",
      "----------------------------\n",
      "Train logP=-168.695843 Test logP=-12.268658 Train Acc=0.924812 Test Acc=0.969925 TrainLoss=0.228737 TestLoss=0.133082\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=3  --step=.08 --early_stop=1 --normalize=yes \\\n",
    "    --log=yes --log_step=200 --plot_name='selected_features_with_normalization' \\\n",
    "    --save_weights_path='weights/selected_features_with_normalization' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a0ac7-f352-4b41-ac65-718cfe30fe9f",
   "metadata": {},
   "source": [
    "![slected with normalization](figures/selected_features_with_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d10977-03eb-40ed-ab3c-30b3f96b1bc7",
   "metadata": {},
   "source": [
    "## Selected Features with Exp Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59358fa-ab62-4ece-99a4-cff7d47d3e05",
   "metadata": {},
   "source": [
    "### Training Selected Features without normalization with Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c152af-d4d4-40f4-9d3f-d73e194bf116",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "    Update 1\tTP -1605.064304\tHP -178.226647\tTA 0.547932\tHA 0.646617\t lr 0.099992\n",
      "    Update 101\tTP -1036.591873\tHP -117.812268\tTA 0.651316\tHA 0.736842\t lr 0.099195\n",
      "    Update 201\tTP -758.867617\tHP -79.377533\tTA 0.710526\tHA 0.759398\t lr 0.098405\n",
      "    Update 301\tTP -631.121515\tHP -62.497862\tTA 0.741541\tHA 0.819549\t lr 0.097621\n",
      "    Update 401\tTP -629.481022\tHP -59.725128\tTA 0.740602\tHA 0.796992\t lr 0.096843\n",
      "    Update 501\tTP -552.260262\tHP -50.854844\tTA 0.765038\tHA 0.834586\t lr 0.096071\n",
      "    Update 601\tTP -468.758606\tHP -41.587282\tTA 0.785714\tHA 0.857143\t lr 0.095306\n",
      "    Update 701\tTP -448.947694\tHP -39.361339\tTA 0.799812\tHA 0.872180\t lr 0.094546\n",
      "    Update 801\tTP -412.597418\tHP -35.631021\tTA 0.815789\tHA 0.872180\t lr 0.093793\n",
      "    Update 901\tTP -389.212104\tHP -34.195169\tTA 0.829887\tHA 0.872180\t lr 0.093046\n",
      "    Update 1001\tTP -363.411809\tHP -30.470914\tTA 0.833647\tHA 0.887218\t lr 0.092304\n",
      "Train logP=-363.411809 Test logP=-30.470914 Train Acc=0.833647 Test Acc=0.887218 TrainLoss=0.492756 TestLoss=0.330528\n",
      "----------------------------\n",
      "Epoch:2\n",
      "    Update 1101\tTP -352.547321\tHP -29.362984\tTA 0.833647\tHA 0.887218\t lr 0.091569\n",
      "    Update 1201\tTP -345.945053\tHP -28.183292\tTA 0.842105\tHA 0.887218\t lr 0.090839\n",
      "    Update 1301\tTP -344.195263\tHP -27.189339\tTA 0.848684\tHA 0.909774\t lr 0.090115\n",
      "    Update 1401\tTP -346.082686\tHP -26.889527\tTA 0.853383\tHA 0.902256\t lr 0.089397\n",
      "    Update 1501\tTP -346.061668\tHP -26.980040\tTA 0.851504\tHA 0.909774\t lr 0.088685\n",
      "    Update 1601\tTP -355.906443\tHP -26.808230\tTA 0.853383\tHA 0.909774\t lr 0.087978\n",
      "    Update 1701\tTP -304.410754\tHP -21.924723\tTA 0.870301\tHA 0.917293\t lr 0.087277\n",
      "    Update 1801\tTP -289.568917\tHP -21.017658\tTA 0.875000\tHA 0.924812\t lr 0.086582\n",
      "    Update 1901\tTP -293.722929\tHP -20.964095\tTA 0.871241\tHA 0.932331\t lr 0.085892\n",
      "    Update 2001\tTP -295.527721\tHP -20.871634\tTA 0.874060\tHA 0.924812\t lr 0.085208\n",
      "    Update 2101\tTP -282.605239\tHP -19.896451\tTA 0.875940\tHA 0.932331\t lr 0.084529\n",
      "Train logP=-282.605239 Test logP=-19.896451 Train Acc=0.875940 Test Acc=0.932331 TrainLoss=0.383189 TestLoss=0.215823\n",
      "----------------------------\n",
      "Epoch:3\n",
      "    Update 2201\tTP -286.581871\tHP -19.940444\tTA 0.873120\tHA 0.932331\t lr 0.083855\n",
      "    Update 2301\tTP -288.358484\tHP -19.902161\tTA 0.876880\tHA 0.939850\t lr 0.083187\n",
      "    Update 2401\tTP -286.835304\tHP -19.546229\tTA 0.878759\tHA 0.947368\t lr 0.082524\n",
      "    Update 2501\tTP -280.128771\tHP -19.331522\tTA 0.877820\tHA 0.947368\t lr 0.081867\n",
      "    Update 2601\tTP -289.081000\tHP -19.900074\tTA 0.873120\tHA 0.932331\t lr 0.081214\n",
      "    Update 2701\tTP -279.495591\tHP -18.771478\tTA 0.876880\tHA 0.932331\t lr 0.080567\n",
      "    Update 2801\tTP -277.685831\tHP -18.357058\tTA 0.878759\tHA 0.939850\t lr 0.079925\n",
      "    Update 2901\tTP -255.062152\tHP -17.234520\tTA 0.890038\tHA 0.932331\t lr 0.079288\n",
      "    Update 3001\tTP -254.838391\tHP -17.162943\tTA 0.892857\tHA 0.932331\t lr 0.078656\n",
      "    Update 3101\tTP -263.635864\tHP -17.419191\tTA 0.887218\tHA 0.939850\t lr 0.078030\n",
      "Train logP=-263.635864 Test logP=-17.419191 Train Acc=0.887218 Test Acc=0.939850 TrainLoss=0.357468 TestLoss=0.188952\n",
      "----------------------------\n",
      "Epoch:4\n",
      "    Update 3201\tTP -246.004721\tHP -16.652077\tTA 0.898496\tHA 0.939850\t lr 0.077408\n",
      "    Update 3301\tTP -262.289395\tHP -17.299538\tTA 0.890038\tHA 0.947368\t lr 0.076791\n",
      "    Update 3401\tTP -250.149106\tHP -16.573844\tTA 0.895677\tHA 0.939850\t lr 0.076179\n",
      "    Update 3501\tTP -257.071733\tHP -16.844729\tTA 0.891917\tHA 0.947368\t lr 0.075572\n",
      "    Update 3601\tTP -277.796394\tHP -18.302431\tTA 0.886278\tHA 0.947368\t lr 0.074970\n",
      "    Update 3701\tTP -276.616463\tHP -18.142694\tTA 0.885338\tHA 0.947368\t lr 0.074373\n",
      "    Update 3801\tTP -247.206196\tHP -16.077925\tTA 0.897556\tHA 0.939850\t lr 0.073780\n",
      "    Update 3901\tTP -247.224871\tHP -15.987494\tTA 0.898496\tHA 0.939850\t lr 0.073192\n",
      "    Update 4001\tTP -245.654368\tHP -15.914904\tTA 0.898496\tHA 0.939850\t lr 0.072609\n",
      "    Update 4101\tTP -247.026901\tHP -16.068918\tTA 0.898496\tHA 0.947368\t lr 0.072031\n",
      "    Update 4201\tTP -247.186042\tHP -15.932599\tTA 0.897556\tHA 0.947368\t lr 0.071457\n",
      "Train logP=-247.186042 Test logP=-15.932599 Train Acc=0.897556 Test Acc=0.947368 TrainLoss=0.335164 TestLoss=0.172826\n",
      "----------------------------\n",
      "Epoch:5\n",
      "    Update 4301\tTP -238.999454\tHP -15.546360\tTA 0.900376\tHA 0.939850\t lr 0.070887\n",
      "    Update 4401\tTP -243.431501\tHP -15.915333\tTA 0.898496\tHA 0.947368\t lr 0.070322\n",
      "    Update 4501\tTP -246.256169\tHP -15.988572\tTA 0.897556\tHA 0.947368\t lr 0.069762\n",
      "    Update 4601\tTP -251.142747\tHP -16.240989\tTA 0.894737\tHA 0.947368\t lr 0.069206\n",
      "    Update 4701\tTP -263.771944\tHP -17.150434\tTA 0.893797\tHA 0.947368\t lr 0.068655\n",
      "    Update 4801\tTP -253.278815\tHP -16.330445\tTA 0.894737\tHA 0.947368\t lr 0.068108\n",
      "    Update 4901\tTP -234.480172\tHP -15.214718\tTA 0.905075\tHA 0.947368\t lr 0.067565\n",
      "    Update 5001\tTP -225.433293\tHP -14.856962\tTA 0.908835\tHA 0.939850\t lr 0.067027\n",
      "    Update 5101\tTP -227.178916\tHP -14.912406\tTA 0.906955\tHA 0.939850\t lr 0.066493\n",
      "    Update 5201\tTP -237.492003\tHP -15.351842\tTA 0.903195\tHA 0.947368\t lr 0.065963\n",
      "    Update 5301\tTP -229.278770\tHP -14.991284\tTA 0.907895\tHA 0.947368\t lr 0.065437\n",
      "Train logP=-229.278770 Test logP=-14.991284 Train Acc=0.907895 Test Acc=0.947368 TrainLoss=0.310883 TestLoss=0.162615\n",
      "----------------------------\n",
      "Epoch:6\n",
      "    Update 5401\tTP -233.177524\tHP -15.228248\tTA 0.906015\tHA 0.947368\t lr 0.064916\n",
      "    Update 5501\tTP -235.894543\tHP -15.376083\tTA 0.906955\tHA 0.947368\t lr 0.064398\n",
      "    Update 5601\tTP -240.515007\tHP -15.568822\tTA 0.899436\tHA 0.947368\t lr 0.063885\n",
      "    Update 5701\tTP -236.488076\tHP -15.455117\tTA 0.905075\tHA 0.947368\t lr 0.063376\n",
      "    Update 5801\tTP -240.347316\tHP -15.628952\tTA 0.902256\tHA 0.947368\t lr 0.062871\n",
      "    Update 5901\tTP -232.108542\tHP -14.916763\tTA 0.908835\tHA 0.947368\t lr 0.062370\n",
      "    Update 6001\tTP -233.227151\tHP -14.995206\tTA 0.909774\tHA 0.947368\t lr 0.061873\n",
      "    Update 6101\tTP -221.070685\tHP -14.560562\tTA 0.910714\tHA 0.947368\t lr 0.061380\n",
      "    Update 6201\tTP -226.368927\tHP -14.774399\tTA 0.909774\tHA 0.947368\t lr 0.060891\n",
      "    Update 6301\tTP -228.066662\tHP -14.928597\tTA 0.909774\tHA 0.947368\t lr 0.060406\n",
      "Train logP=-228.066662 Test logP=-14.928597 Train Acc=0.909774 Test Acc=0.947368 TrainLoss=0.309239 TestLoss=0.161935\n",
      "----------------------------\n",
      "Epoch:7\n",
      "    Update 6401\tTP -217.057250\tHP -14.441609\tTA 0.911654\tHA 0.947368\t lr 0.059925\n",
      "    Update 6501\tTP -231.032220\tHP -15.201383\tTA 0.909774\tHA 0.947368\t lr 0.059447\n",
      "    Update 6601\tTP -226.558825\tHP -14.925653\tTA 0.910714\tHA 0.947368\t lr 0.058974\n",
      "    Update 6701\tTP -231.816354\tHP -15.199235\tTA 0.909774\tHA 0.947368\t lr 0.058504\n",
      "    Update 6801\tTP -236.067062\tHP -15.493360\tTA 0.904135\tHA 0.939850\t lr 0.058038\n",
      "    Update 6901\tTP -239.539852\tHP -15.681021\tTA 0.904135\tHA 0.939850\t lr 0.057575\n",
      "    Update 7001\tTP -220.365840\tHP -14.372920\tTA 0.910714\tHA 0.947368\t lr 0.057116\n",
      "    Update 7101\tTP -222.167960\tHP -14.548987\tTA 0.910714\tHA 0.947368\t lr 0.056661\n",
      "    Update 7201\tTP -217.435470\tHP -14.353946\tTA 0.911654\tHA 0.947368\t lr 0.056210\n",
      "    Update 7301\tTP -226.080084\tHP -14.885919\tTA 0.910714\tHA 0.939850\t lr 0.055762\n",
      "    Update 7401\tTP -225.974081\tHP -14.838590\tTA 0.910714\tHA 0.939850\t lr 0.055318\n",
      "Train logP=-225.974081 Test logP=-14.838590 Train Acc=0.910714 Test Acc=0.939850 TrainLoss=0.306402 TestLoss=0.160959\n",
      "----------------------------\n",
      "Epoch:8\n",
      "    Update 7501\tTP -219.060786\tHP -14.530726\tTA 0.911654\tHA 0.939850\t lr 0.054877\n",
      "    Update 7601\tTP -226.709888\tHP -14.984976\tTA 0.910714\tHA 0.939850\t lr 0.054440\n",
      "    Update 7701\tTP -225.967149\tHP -14.923915\tTA 0.910714\tHA 0.939850\t lr 0.054006\n",
      "    Update 7801\tTP -228.437222\tHP -15.098815\tTA 0.910714\tHA 0.939850\t lr 0.053575\n",
      "    Update 7901\tTP -234.538295\tHP -15.499939\tTA 0.906955\tHA 0.939850\t lr 0.053149\n",
      "    Update 8001\tTP -227.599930\tHP -14.918306\tTA 0.910714\tHA 0.939850\t lr 0.052725\n",
      "    Update 8101\tTP -218.971939\tHP -14.490686\tTA 0.910714\tHA 0.939850\t lr 0.052305\n",
      "    Update 8201\tTP -213.320162\tHP -14.285926\tTA 0.912594\tHA 0.939850\t lr 0.051888\n",
      "    Update 8301\tTP -212.992418\tHP -14.258420\tTA 0.912594\tHA 0.939850\t lr 0.051475\n",
      "    Update 8401\tTP -215.948523\tHP -14.412953\tTA 0.912594\tHA 0.939850\t lr 0.051065\n",
      "    Update 8501\tTP -212.343983\tHP -14.343422\tTA 0.912594\tHA 0.939850\t lr 0.050658\n",
      "Train logP=-212.343983 Test logP=-14.343422 Train Acc=0.912594 Test Acc=0.939850 TrainLoss=0.287921 TestLoss=0.155588\n",
      "----------------------------\n",
      "Epoch:9\n",
      "    Update 8601\tTP -214.348577\tHP -14.410744\tTA 0.912594\tHA 0.939850\t lr 0.050254\n",
      "    Update 8701\tTP -217.901072\tHP -14.587595\tTA 0.912594\tHA 0.939850\t lr 0.049854\n",
      "    Update 8801\tTP -223.597939\tHP -14.872365\tTA 0.911654\tHA 0.939850\t lr 0.049456\n",
      "    Update 8901\tTP -225.096560\tHP -15.045241\tTA 0.911654\tHA 0.939850\t lr 0.049062\n",
      "    Update 9001\tTP -226.644628\tHP -15.100089\tTA 0.911654\tHA 0.939850\t lr 0.048671\n",
      "    Update 9101\tTP -218.834168\tHP -14.476465\tTA 0.911654\tHA 0.939850\t lr 0.048284\n",
      "    Update 9201\tTP -220.929905\tHP -14.632011\tTA 0.911654\tHA 0.939850\t lr 0.047899\n",
      "    Update 9301\tTP -210.120565\tHP -14.204622\tTA 0.912594\tHA 0.939850\t lr 0.047517\n",
      "    Update 9401\tTP -212.096586\tHP -14.284917\tTA 0.912594\tHA 0.939850\t lr 0.047139\n",
      "    Update 9501\tTP -212.185840\tHP -14.342708\tTA 0.912594\tHA 0.939850\t lr 0.046763\n",
      "Train logP=-212.185840 Test logP=-14.342708 Train Acc=0.912594 Test Acc=0.939850 TrainLoss=0.287706 TestLoss=0.155580\n",
      "----------------------------\n",
      "Epoch:10\n",
      "    Update 9601\tTP -209.542089\tHP -14.252343\tTA 0.912594\tHA 0.939850\t lr 0.046390\n",
      "    Update 9701\tTP -216.478447\tHP -14.625018\tTA 0.912594\tHA 0.939850\t lr 0.046021\n",
      "    Update 9801\tTP -215.831779\tHP -14.584080\tTA 0.912594\tHA 0.939850\t lr 0.045654\n",
      "    Update 9901\tTP -224.076889\tHP -15.035079\tTA 0.912594\tHA 0.939850\t lr 0.045290\n",
      "    Update 10001\tTP -225.466146\tHP -15.157925\tTA 0.912594\tHA 0.939850\t lr 0.044929\n",
      "    Update 10101\tTP -226.058285\tHP -15.175692\tTA 0.914474\tHA 0.939850\t lr 0.044571\n",
      "    Update 10201\tTP -214.667195\tHP -14.348457\tTA 0.912594\tHA 0.939850\t lr 0.044216\n",
      "    Update 10301\tTP -213.537070\tHP -14.393182\tTA 0.912594\tHA 0.939850\t lr 0.043864\n",
      "    Update 10401\tTP -206.231371\tHP -14.055288\tTA 0.913534\tHA 0.939850\t lr 0.043514\n",
      "    Update 10501\tTP -214.598742\tHP -14.535290\tTA 0.913534\tHA 0.939850\t lr 0.043168\n",
      "    Update 10601\tTP -213.881986\tHP -14.491807\tTA 0.913534\tHA 0.939850\t lr 0.042824\n",
      "Early Stop\n",
      "Train logP=-213.881986 Test logP=-14.491807 Train Acc=0.913534 Test Acc=0.939850 TrainLoss=0.290006 TestLoss=0.157197\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=40  --step=.1 --early_stop=1 --normalize=no \\\n",
    "    --log=yes --log_step=100 --plot_name='selected_features_no_normalization_with_scheduler_exp' \\\n",
    "    --save_weights_path='weights/selected_features_no_normalization_with_scheduler_exp' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy' \\\n",
    "    --learning_rate_scheduler='exp' --learning_rate_scheduler_params='{\"decay\":8e-5}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1cbe4-3c8d-4429-919f-38b51a626054",
   "metadata": {},
   "source": [
    "![selected no normalization exp](figures/selected_features_no_normalization_with_scheduler_exp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a050f7c-daec-4370-b667-e5d6dc5241da",
   "metadata": {},
   "source": [
    "### Training Selected Features with normalization with Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55847a0d-3dc3-4d26-a153-8a56538c19bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "    Update 1\tTP -1605.089480\tHP -175.455775\tTA 0.545113\tHA 0.624060\t lr 0.399996\n",
      "    Update 101\tTP -674.646116\tHP -68.866914\tTA 0.761278\tHA 0.812030\t lr 0.399596\n",
      "    Update 201\tTP -502.813003\tHP -35.981697\tTA 0.813910\tHA 0.879699\t lr 0.399197\n",
      "    Update 301\tTP -394.719280\tHP -29.628713\tTA 0.847744\tHA 0.872180\t lr 0.398798\n",
      "    Update 401\tTP -492.374617\tHP -40.178143\tTA 0.839286\tHA 0.902256\t lr 0.398399\n",
      "    Update 501\tTP -503.833457\tHP -45.067627\tTA 0.843045\tHA 0.872180\t lr 0.398001\n",
      "    Update 601\tTP -316.617486\tHP -22.849960\tTA 0.878759\tHA 0.924812\t lr 0.397603\n",
      "    Update 701\tTP -288.923026\tHP -18.009747\tTA 0.893797\tHA 0.939850\t lr 0.397206\n",
      "    Update 801\tTP -299.008728\tHP -17.605046\tTA 0.895677\tHA 0.939850\t lr 0.396809\n",
      "    Update 901\tTP -329.426825\tHP -21.224052\tTA 0.883459\tHA 0.932331\t lr 0.396412\n",
      "    Update 1001\tTP -256.157395\tHP -15.792718\tTA 0.905075\tHA 0.939850\t lr 0.396016\n",
      "Train logP=-256.157395 Test logP=-15.792718 Train Acc=0.905075 Test Acc=0.939850 TrainLoss=0.346110 TestLoss=0.171309\n",
      "----------------------------\n",
      "Epoch:2\n",
      "    Update 1101\tTP -270.021535\tHP -16.532962\tTA 0.906015\tHA 0.939850\t lr 0.395620\n",
      "    Update 1201\tTP -289.208852\tHP -19.128567\tTA 0.905075\tHA 0.924812\t lr 0.395225\n",
      "    Update 1301\tTP -264.633624\tHP -18.763549\tTA 0.904135\tHA 0.932331\t lr 0.394830\n",
      "    Update 1401\tTP -287.667870\tHP -20.595486\tTA 0.898496\tHA 0.932331\t lr 0.394435\n",
      "    Update 1501\tTP -250.608534\tHP -19.384773\tTA 0.908835\tHA 0.932331\t lr 0.394041\n",
      "    Update 1601\tTP -234.375274\tHP -17.817106\tTA 0.913534\tHA 0.939850\t lr 0.393647\n",
      "    Update 1701\tTP -203.779995\tHP -14.674294\tTA 0.913534\tHA 0.939850\t lr 0.393254\n",
      "    Update 1801\tTP -183.727922\tHP -15.801762\tTA 0.917293\tHA 0.947368\t lr 0.392860\n",
      "    Update 1901\tTP -202.510998\tHP -15.173487\tTA 0.917293\tHA 0.947368\t lr 0.392468\n",
      "    Update 2001\tTP -217.859982\tHP -16.234956\tTA 0.912594\tHA 0.947368\t lr 0.392076\n",
      "    Update 2101\tTP -197.548326\tHP -15.514417\tTA 0.918233\tHA 0.939850\t lr 0.391684\n",
      "Train logP=-197.548326 Test logP=-15.514417 Train Acc=0.918233 Test Acc=0.939850 TrainLoss=0.267859 TestLoss=0.168290\n",
      "----------------------------\n",
      "Epoch:3\n",
      "    Update 2201\tTP -209.083525\tHP -15.785049\tTA 0.918233\tHA 0.939850\t lr 0.391292\n",
      "    Update 2301\tTP -231.405679\tHP -17.392218\tTA 0.916353\tHA 0.939850\t lr 0.390901\n",
      "    Update 2401\tTP -233.796415\tHP -17.909670\tTA 0.917293\tHA 0.939850\t lr 0.390510\n",
      "    Update 2501\tTP -195.885571\tHP -16.731873\tTA 0.919173\tHA 0.939850\t lr 0.390120\n",
      "    Update 2601\tTP -198.411175\tHP -15.894037\tTA 0.920113\tHA 0.939850\t lr 0.389730\n",
      "    Update 2701\tTP -188.202050\tHP -15.677323\tTA 0.920113\tHA 0.954887\t lr 0.389341\n",
      "    Update 2801\tTP -235.892282\tHP -17.169620\tTA 0.917293\tHA 0.947368\t lr 0.388951\n",
      "    Update 2901\tTP -171.153235\tHP -15.951468\tTA 0.919173\tHA 0.939850\t lr 0.388563\n",
      "    Update 3001\tTP -197.986966\tHP -15.707448\tTA 0.922932\tHA 0.947368\t lr 0.388174\n",
      "    Update 3101\tTP -189.564495\tHP -15.262450\tTA 0.922932\tHA 0.947368\t lr 0.387786\n",
      "Train logP=-189.564495 Test logP=-15.262450 Train Acc=0.922932 Test Acc=0.947368 TrainLoss=0.257034 TestLoss=0.165557\n",
      "----------------------------\n",
      "Epoch:4\n",
      "    Update 3201\tTP -172.205596\tHP -15.720963\tTA 0.921053\tHA 0.939850\t lr 0.387399\n",
      "    Update 3301\tTP -241.249893\tHP -18.489950\tTA 0.919173\tHA 0.947368\t lr 0.387012\n",
      "    Update 3401\tTP -180.924425\tHP -15.237430\tTA 0.922932\tHA 0.947368\t lr 0.386625\n",
      "    Update 3501\tTP -208.100580\tHP -17.183406\tTA 0.923872\tHA 0.947368\t lr 0.386238\n",
      "    Update 3601\tTP -232.414442\tHP -18.786474\tTA 0.920113\tHA 0.939850\t lr 0.385852\n",
      "    Update 3701\tTP -230.303697\tHP -18.652148\tTA 0.920113\tHA 0.947368\t lr 0.385467\n",
      "    Update 3801\tTP -176.835486\tHP -15.001914\tTA 0.926692\tHA 0.939850\t lr 0.385081\n",
      "    Update 3901\tTP -181.261680\tHP -15.771171\tTA 0.924812\tHA 0.939850\t lr 0.384696\n",
      "    Update 4001\tTP -188.833788\tHP -15.961774\tTA 0.925752\tHA 0.947368\t lr 0.384312\n",
      "    Update 4101\tTP -210.232666\tHP -16.932384\tTA 0.921992\tHA 0.947368\t lr 0.383928\n",
      "    Update 4201\tTP -210.987068\tHP -16.865084\tTA 0.921053\tHA 0.947368\t lr 0.383544\n",
      "Train logP=-210.987068 Test logP=-16.865084 Train Acc=0.921053 Test Acc=0.947368 TrainLoss=0.286081 TestLoss=0.182941\n",
      "----------------------------\n",
      "Epoch:5\n",
      "    Update 4301\tTP -191.908481\tHP -16.324227\tTA 0.922932\tHA 0.947368\t lr 0.383161\n",
      "    Update 4401\tTP -207.376600\tHP -16.867132\tTA 0.923872\tHA 0.947368\t lr 0.382778\n",
      "    Update 4501\tTP -205.345415\tHP -17.115738\tTA 0.924812\tHA 0.947368\t lr 0.382395\n",
      "    Update 4601\tTP -203.720051\tHP -16.955771\tTA 0.923872\tHA 0.947368\t lr 0.382013\n",
      "    Update 4701\tTP -217.183825\tHP -17.945633\tTA 0.922932\tHA 0.947368\t lr 0.381631\n",
      "    Update 4801\tTP -197.210593\tHP -16.706925\tTA 0.923872\tHA 0.947368\t lr 0.381250\n",
      "    Update 4901\tTP -183.005911\tHP -16.350684\tTA 0.927632\tHA 0.939850\t lr 0.380869\n",
      "    Update 5001\tTP -161.948911\tHP -16.285594\tTA 0.925752\tHA 0.947368\t lr 0.380488\n",
      "    Update 5101\tTP -170.599687\tHP -15.910151\tTA 0.927632\tHA 0.939850\t lr 0.380108\n",
      "    Update 5201\tTP -194.287000\tHP -16.360837\tTA 0.926692\tHA 0.947368\t lr 0.379728\n",
      "    Update 5301\tTP -175.963465\tHP -16.463247\tTA 0.924812\tHA 0.939850\t lr 0.379348\n",
      "Train logP=-175.963465 Test logP=-16.463247 Train Acc=0.924812 Test Acc=0.939850 TrainLoss=0.238592 TestLoss=0.178582\n",
      "----------------------------\n",
      "Epoch:6\n",
      "    Update 5401\tTP -209.254802\tHP -17.298092\tTA 0.922932\tHA 0.947368\t lr 0.378969\n",
      "    Update 5501\tTP -199.785519\tHP -16.727180\tTA 0.924812\tHA 0.947368\t lr 0.378590\n",
      "    Update 5601\tTP -208.753826\tHP -17.578640\tTA 0.923872\tHA 0.947368\t lr 0.378212\n",
      "    Update 5701\tTP -189.226059\tHP -16.681679\tTA 0.925752\tHA 0.947368\t lr 0.377834\n",
      "    Update 5801\tTP -180.386409\tHP -16.091671\tTA 0.924812\tHA 0.939850\t lr 0.377456\n",
      "    Update 5901\tTP -174.716200\tHP -15.579521\tTA 0.926692\tHA 0.939850\t lr 0.377079\n",
      "    Update 6001\tTP -193.711670\tHP -16.636903\tTA 0.925752\tHA 0.947368\t lr 0.376702\n",
      "    Update 6101\tTP -170.505206\tHP -16.191293\tTA 0.928571\tHA 0.939850\t lr 0.376326\n",
      "    Update 6201\tTP -196.523460\tHP -16.960826\tTA 0.925752\tHA 0.947368\t lr 0.375949\n",
      "    Update 6301\tTP -180.046967\tHP -16.394476\tTA 0.928571\tHA 0.947368\t lr 0.375574\n",
      "Train logP=-180.046967 Test logP=-16.394476 Train Acc=0.928571 Test Acc=0.947368 TrainLoss=0.244129 TestLoss=0.177836\n",
      "----------------------------\n",
      "Epoch:7\n",
      "    Update 6401\tTP -167.971369\tHP -16.396255\tTA 0.928571\tHA 0.939850\t lr 0.375198\n",
      "    Update 6501\tTP -221.929212\tHP -18.592473\tTA 0.924812\tHA 0.954887\t lr 0.374823\n",
      "    Update 6601\tTP -186.815211\tHP -16.689166\tTA 0.926692\tHA 0.947368\t lr 0.374449\n",
      "    Update 6701\tTP -205.824760\tHP -17.978806\tTA 0.924812\tHA 0.947368\t lr 0.374074\n",
      "    Update 6801\tTP -188.668973\tHP -17.023256\tTA 0.925752\tHA 0.947368\t lr 0.373700\n",
      "    Update 6901\tTP -202.667991\tHP -17.952498\tTA 0.923872\tHA 0.947368\t lr 0.373327\n",
      "    Update 7001\tTP -169.772057\tHP -15.569649\tTA 0.927632\tHA 0.939850\t lr 0.372954\n",
      "    Update 7101\tTP -171.509705\tHP -16.359382\tTA 0.927632\tHA 0.939850\t lr 0.372581\n",
      "    Update 7201\tTP -167.800752\tHP -16.201157\tTA 0.929511\tHA 0.939850\t lr 0.372209\n",
      "    Update 7301\tTP -199.017094\tHP -17.475090\tTA 0.925752\tHA 0.947368\t lr 0.371837\n",
      "    Update 7401\tTP -207.090324\tHP -18.008657\tTA 0.925752\tHA 0.947368\t lr 0.371465\n",
      "Train logP=-207.090324 Test logP=-18.008657 Train Acc=0.925752 Test Acc=0.947368 TrainLoss=0.280797 TestLoss=0.195346\n",
      "----------------------------\n",
      "Epoch:8\n",
      "    Update 7501\tTP -189.164805\tHP -17.308320\tTA 0.927632\tHA 0.947368\t lr 0.371094\n",
      "    Update 7601\tTP -209.336377\tHP -18.028396\tTA 0.924812\tHA 0.954887\t lr 0.370723\n",
      "    Update 7701\tTP -204.924978\tHP -17.963343\tTA 0.927632\tHA 0.947368\t lr 0.370352\n",
      "    Update 7801\tTP -198.252623\tHP -17.937168\tTA 0.926692\tHA 0.947368\t lr 0.369982\n",
      "    Update 7901\tTP -191.330133\tHP -17.332859\tTA 0.926692\tHA 0.954887\t lr 0.369612\n",
      "    Update 8001\tTP -180.501943\tHP -16.661426\tTA 0.926692\tHA 0.939850\t lr 0.369243\n",
      "    Update 8101\tTP -176.986781\tHP -16.710094\tTA 0.929511\tHA 0.939850\t lr 0.368874\n",
      "    Update 8201\tTP -168.384478\tHP -16.638464\tTA 0.928571\tHA 0.939850\t lr 0.368505\n",
      "    Update 8301\tTP -165.766674\tHP -16.357415\tTA 0.931391\tHA 0.939850\t lr 0.368137\n",
      "    Update 8401\tTP -169.812953\tHP -16.202667\tTA 0.928571\tHA 0.939850\t lr 0.367769\n",
      "    Update 8501\tTP -162.916948\tHP -17.013139\tTA 0.929511\tHA 0.939850\t lr 0.367401\n",
      "Train logP=-162.916948 Test logP=-17.013139 Train Acc=0.929511 Test Acc=0.939850 TrainLoss=0.220902 TestLoss=0.184547\n",
      "----------------------------\n",
      "Epoch:9\n",
      "    Update 8601\tTP -182.977124\tHP -16.727510\tTA 0.929511\tHA 0.939850\t lr 0.367034\n",
      "    Update 8701\tTP -185.798986\tHP -16.847630\tTA 0.928571\tHA 0.939850\t lr 0.366667\n",
      "    Update 8801\tTP -202.901821\tHP -18.042329\tTA 0.928571\tHA 0.947368\t lr 0.366301\n",
      "    Update 8901\tTP -197.813057\tHP -18.002241\tTA 0.926692\tHA 0.954887\t lr 0.365935\n",
      "    Update 9001\tTP -185.278542\tHP -17.116311\tTA 0.929511\tHA 0.947368\t lr 0.365569\n",
      "    Update 9101\tTP -173.522816\tHP -16.140183\tTA 0.929511\tHA 0.939850\t lr 0.365203\n",
      "    Update 9201\tTP -190.259989\tHP -17.237651\tTA 0.926692\tHA 0.939850\t lr 0.364838\n",
      "    Update 9301\tTP -166.606549\tHP -16.563559\tTA 0.930451\tHA 0.939850\t lr 0.364474\n",
      "    Update 9401\tTP -176.697136\tHP -16.799644\tTA 0.930451\tHA 0.939850\t lr 0.364109\n",
      "    Update 9501\tTP -166.067669\tHP -16.496965\tTA 0.930451\tHA 0.939850\t lr 0.363746\n",
      "Train logP=-166.067669 Test logP=-16.496965 Train Acc=0.930451 Test Acc=0.939850 TrainLoss=0.225174 TestLoss=0.178948\n",
      "----------------------------\n",
      "Epoch:10\n",
      "    Update 9601\tTP -174.362856\tHP -17.051811\tTA 0.929511\tHA 0.939850\t lr 0.363382\n",
      "    Update 9701\tTP -198.103209\tHP -17.844301\tTA 0.929511\tHA 0.954887\t lr 0.363019\n",
      "    Update 9801\tTP -183.867920\tHP -17.103505\tTA 0.928571\tHA 0.939850\t lr 0.362656\n",
      "    Update 9901\tTP -231.348089\tHP -19.960800\tTA 0.925752\tHA 0.954887\t lr 0.362293\n",
      "    Update 10001\tTP -200.471755\tHP -18.314354\tTA 0.929511\tHA 0.954887\t lr 0.361931\n",
      "    Update 10101\tTP -191.124627\tHP -17.657925\tTA 0.928571\tHA 0.947368\t lr 0.361570\n",
      "    Update 10201\tTP -177.166031\tHP -16.398973\tTA 0.930451\tHA 0.939850\t lr 0.361208\n",
      "    Update 10301\tTP -173.559326\tHP -16.879891\tTA 0.930451\tHA 0.939850\t lr 0.360847\n",
      "    Update 10401\tTP -155.781894\tHP -16.675835\tTA 0.932331\tHA 0.939850\t lr 0.360487\n",
      "    Update 10501\tTP -194.058982\tHP -17.788492\tTA 0.929511\tHA 0.947368\t lr 0.360126\n",
      "    Update 10601\tTP -195.260524\tHP -17.960017\tTA 0.930451\tHA 0.954887\t lr 0.359766\n",
      "Train logP=-195.260524 Test logP=-17.960017 Train Acc=0.930451 Test Acc=0.954887 TrainLoss=0.264757 TestLoss=0.194818\n",
      "----------------------------\n",
      "Epoch:11\n",
      "    Update 10701\tTP -185.866448\tHP -17.790629\tTA 0.928571\tHA 0.939850\t lr 0.359407\n",
      "    Update 10801\tTP -207.483085\tHP -18.567644\tTA 0.930451\tHA 0.954887\t lr 0.359047\n",
      "    Update 10901\tTP -197.933940\tHP -18.066818\tTA 0.929511\tHA 0.947368\t lr 0.358689\n",
      "    Update 11001\tTP -171.798330\tHP -17.303948\tTA 0.931391\tHA 0.939850\t lr 0.358330\n",
      "    Update 11101\tTP -191.803695\tHP -17.891607\tTA 0.931391\tHA 0.954887\t lr 0.357972\n",
      "    Update 11201\tTP -173.523954\tHP -16.749258\tTA 0.930451\tHA 0.939850\t lr 0.357614\n",
      "    Update 11301\tTP -181.984006\tHP -17.232681\tTA 0.930451\tHA 0.939850\t lr 0.357257\n",
      "    Update 11401\tTP -160.763156\tHP -16.676021\tTA 0.930451\tHA 0.939850\t lr 0.356900\n",
      "    Update 11501\tTP -161.568656\tHP -16.593854\tTA 0.930451\tHA 0.939850\t lr 0.356543\n",
      "    Update 11601\tTP -186.555491\tHP -17.377073\tTA 0.930451\tHA 0.947368\t lr 0.356187\n",
      "    Update 11701\tTP -158.646638\tHP -17.569366\tTA 0.931391\tHA 0.932331\t lr 0.355831\n",
      "Train logP=-158.646638 Test logP=-17.569366 Train Acc=0.931391 Test Acc=0.932331 TrainLoss=0.215112 TestLoss=0.190581\n",
      "----------------------------\n",
      "Epoch:12\n",
      "    Update 11801\tTP -192.548980\tHP -17.699578\tTA 0.930451\tHA 0.954887\t lr 0.355475\n",
      "    Update 11901\tTP -168.735842\tHP -16.466051\tTA 0.931391\tHA 0.939850\t lr 0.355120\n",
      "    Update 12001\tTP -183.458313\tHP -17.856084\tTA 0.930451\tHA 0.939850\t lr 0.354765\n",
      "    Update 12101\tTP -204.606338\tHP -18.827677\tTA 0.930451\tHA 0.954887\t lr 0.354410\n",
      "    Update 12201\tTP -184.025199\tHP -17.533705\tTA 0.932331\tHA 0.947368\t lr 0.354056\n",
      "    Update 12301\tTP -170.365621\tHP -16.319045\tTA 0.931391\tHA 0.939850\t lr 0.353702\n",
      "    Update 12401\tTP -173.925849\tHP -16.777635\tTA 0.931391\tHA 0.939850\t lr 0.353348\n",
      "    Update 12501\tTP -160.020062\tHP -16.685429\tTA 0.930451\tHA 0.939850\t lr 0.352995\n",
      "    Update 12601\tTP -184.102633\tHP -17.469262\tTA 0.931391\tHA 0.947368\t lr 0.352642\n",
      "    Update 12701\tTP -183.521361\tHP -17.492903\tTA 0.930451\tHA 0.947368\t lr 0.352290\n",
      "Train logP=-183.521361 Test logP=-17.492903 Train Acc=0.930451 Test Acc=0.947368 TrainLoss=0.248840 TestLoss=0.189751\n",
      "----------------------------\n",
      "Epoch:13\n",
      "    Update 12801\tTP -171.699675\tHP -17.349078\tTA 0.930451\tHA 0.939850\t lr 0.351938\n",
      "    Update 12901\tTP -194.976396\tHP -18.058694\tTA 0.932331\tHA 0.954887\t lr 0.351586\n",
      "    Update 13001\tTP -193.445456\tHP -17.887784\tTA 0.931391\tHA 0.954887\t lr 0.351235\n",
      "    Update 13101\tTP -212.336916\tHP -19.083901\tTA 0.930451\tHA 0.954887\t lr 0.350884\n",
      "    Update 13201\tTP -183.770926\tHP -17.533375\tTA 0.931391\tHA 0.947368\t lr 0.350533\n",
      "    Update 13301\tTP -184.580268\tHP -17.432752\tTA 0.932331\tHA 0.939850\t lr 0.350183\n",
      "    Update 13401\tTP -176.239180\tHP -16.670403\tTA 0.932331\tHA 0.939850\t lr 0.349833\n",
      "    Update 13501\tTP -156.189099\tHP -16.840961\tTA 0.929511\tHA 0.939850\t lr 0.349483\n",
      "    Update 13601\tTP -158.620246\tHP -16.683549\tTA 0.930451\tHA 0.939850\t lr 0.349134\n",
      "    Update 13701\tTP -180.588905\tHP -17.167265\tTA 0.929511\tHA 0.947368\t lr 0.348785\n",
      "    Update 13801\tTP -195.783646\tHP -18.305530\tTA 0.933271\tHA 0.954887\t lr 0.348436\n",
      "Train logP=-195.783646 Test logP=-18.305530 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.265466 TestLoss=0.198566\n",
      "----------------------------\n",
      "Epoch:14\n",
      "    Update 13901\tTP -181.875645\tHP -17.488425\tTA 0.930451\tHA 0.939850\t lr 0.348088\n",
      "    Update 14001\tTP -201.531332\tHP -18.436035\tTA 0.932331\tHA 0.954887\t lr 0.347740\n",
      "    Update 14101\tTP -200.637031\tHP -18.443458\tTA 0.932331\tHA 0.947368\t lr 0.347392\n",
      "    Update 14201\tTP -175.586183\tHP -17.450760\tTA 0.931391\tHA 0.939850\t lr 0.347045\n",
      "    Update 14301\tTP -178.608758\tHP -17.273279\tTA 0.931391\tHA 0.947368\t lr 0.346698\n",
      "    Update 14401\tTP -179.629656\tHP -17.164253\tTA 0.932331\tHA 0.939850\t lr 0.346352\n",
      "    Update 14501\tTP -192.403951\tHP -17.869420\tTA 0.932331\tHA 0.939850\t lr 0.346005\n",
      "    Update 14601\tTP -160.631034\tHP -16.860440\tTA 0.930451\tHA 0.939850\t lr 0.345660\n",
      "    Update 14701\tTP -160.534449\tHP -16.685545\tTA 0.930451\tHA 0.939850\t lr 0.345314\n",
      "    Update 14801\tTP -176.811048\tHP -17.093552\tTA 0.930451\tHA 0.939850\t lr 0.344969\n",
      "Train logP=-176.811048 Test logP=-17.093552 Train Acc=0.930451 Test Acc=0.939850 TrainLoss=0.239741 TestLoss=0.185419\n",
      "----------------------------\n",
      "Epoch:15\n",
      "    Update 14901\tTP -162.587290\tHP -17.695400\tTA 0.930451\tHA 0.932331\t lr 0.344624\n",
      "    Update 15001\tTP -194.570768\tHP -18.096005\tTA 0.931391\tHA 0.954887\t lr 0.344280\n",
      "    Update 15101\tTP -176.116219\tHP -17.040621\tTA 0.931391\tHA 0.947368\t lr 0.343936\n",
      "    Update 15201\tTP -192.736074\tHP -18.415486\tTA 0.931391\tHA 0.947368\t lr 0.343592\n",
      "    Update 15301\tTP -202.729994\tHP -18.847827\tTA 0.932331\tHA 0.954887\t lr 0.343248\n",
      "    Update 15401\tTP -181.736804\tHP -17.470664\tTA 0.932331\tHA 0.947368\t lr 0.342905\n",
      "    Update 15501\tTP -161.342109\tHP -16.137524\tTA 0.933271\tHA 0.939850\t lr 0.342563\n",
      "    Update 15601\tTP -171.794935\tHP -17.113758\tTA 0.931391\tHA 0.939850\t lr 0.342220\n",
      "    Update 15701\tTP -161.300117\tHP -16.792781\tTA 0.931391\tHA 0.939850\t lr 0.341878\n",
      "    Update 15801\tTP -182.026965\tHP -17.569900\tTA 0.932331\tHA 0.939850\t lr 0.341536\n",
      "    Update 15901\tTP -184.533615\tHP -17.711477\tTA 0.932331\tHA 0.954887\t lr 0.341195\n",
      "Train logP=-184.533615 Test logP=-17.711477 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.250212 TestLoss=0.192122\n",
      "----------------------------\n",
      "Epoch:16\n",
      "    Update 16001\tTP -187.008021\tHP -18.287702\tTA 0.931391\tHA 0.947368\t lr 0.340854\n",
      "    Update 16101\tTP -202.036210\tHP -18.730400\tTA 0.931391\tHA 0.954887\t lr 0.340513\n",
      "    Update 16201\tTP -187.470854\tHP -17.787396\tTA 0.931391\tHA 0.939850\t lr 0.340173\n",
      "    Update 16301\tTP -205.107357\tHP -18.728586\tTA 0.932331\tHA 0.954887\t lr 0.339833\n",
      "    Update 16401\tTP -185.288700\tHP -17.731521\tTA 0.931391\tHA 0.947368\t lr 0.339493\n",
      "    Update 16501\tTP -182.591220\tHP -17.450143\tTA 0.932331\tHA 0.939850\t lr 0.339154\n",
      "    Update 16601\tTP -183.539639\tHP -17.241114\tTA 0.932331\tHA 0.947368\t lr 0.338815\n",
      "    Update 16701\tTP -154.555975\tHP -17.004276\tTA 0.931391\tHA 0.947368\t lr 0.338476\n",
      "    Update 16801\tTP -157.049710\tHP -16.838041\tTA 0.930451\tHA 0.939850\t lr 0.338138\n",
      "    Update 16901\tTP -178.762556\tHP -17.327165\tTA 0.931391\tHA 0.939850\t lr 0.337800\n",
      "    Update 17001\tTP -167.332601\tHP -17.956749\tTA 0.932331\tHA 0.932331\t lr 0.337463\n",
      "Early Stop\n",
      "Train logP=-167.332601 Test logP=-17.956749 Train Acc=0.932331 Test Acc=0.932331 TrainLoss=0.226889 TestLoss=0.194783\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=100  --step=.4 --early_stop=6 --normalize=no \\\n",
    "    --log=yes --log_step=100 --plot_name='selected_features_with_normalization_with_scheduler_exp' \\\n",
    "    --save_weights_path='weights/selected_features_with_normalization_with_scheduler_exp' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy' \\\n",
    "    --learning_rate_scheduler='exp' --learning_rate_scheduler_params='{\"decay\":1e-5}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17256b85-d8fc-41ec-8a1a-3fbb1affd05d",
   "metadata": {},
   "source": [
    "![selected no normalization exp](figures/selected_features_with_normalization_with_scheduler_exp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac82e47-aad2-47f6-8f1c-b0f8dbc4f985",
   "metadata": {},
   "source": [
    "# Analyzing uingram Features of: logstic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ccde49-bd90-4c04-a2b2-05bfc6a7b7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from logreg import ExamplesDataset, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07e3e4c-328f-4a4f-9823-36d4f2fac687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   1.,   0.,   0.,   1.,   1.,   1.,   0.,   0.,   0.,\n",
       "          1.,   4.,  12.,  18.,  16.,  19.,  55.,  57., 107., 146., 180.,\n",
       "        248., 292., 356., 377., 423., 432., 397., 379., 338., 305., 259.,\n",
       "        196., 168., 121.,  86.,  59.,  31.,  25.,  11.,   7.,   4.,   2.,\n",
       "          0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([-5.82332404, -5.61263123, -5.40193843, -5.19124562, -4.98055282,\n",
       "        -4.76986001, -4.55916721, -4.34847441, -4.1377816 , -3.9270888 ,\n",
       "        -3.71639599, -3.50570319, -3.29501039, -3.08431758, -2.87362478,\n",
       "        -2.66293197, -2.45223917, -2.24154637, -2.03085356, -1.82016076,\n",
       "        -1.60946795, -1.39877515, -1.18808235, -0.97738954, -0.76669674,\n",
       "        -0.55600393, -0.34531113, -0.13461832,  0.07607448,  0.28676728,\n",
       "         0.49746009,  0.70815289,  0.9188457 ,  1.1295385 ,  1.3402313 ,\n",
       "         1.55092411,  1.76161691,  1.97230972,  2.18300252,  2.39369532,\n",
       "         2.60438813,  2.81508093,  3.02577374,  3.23646654,  3.44715934,\n",
       "         3.65785215,  3.86854495,  4.07923776,  4.28993056,  4.50062337,\n",
       "         4.71131617]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# playing with weights\n",
    "# index0-> bias\n",
    "weights = np.load('weights/all_features_no_normalization.npy')\n",
    "plt.hist(weights, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395028b5-1c88-4ff9-af48-9de0b6ceb298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Reading dataset'''\n",
    "dataset = ExamplesDataset('data/positive', 'data/negative', 'data/vocab')\n",
    "positive, negative, vocab = dataset.get_positive_negative_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f60478f3-d61e-4c55-89fa-a17997ecbca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:  (5137,)\n",
      "filter shape (7,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['BIAS_CONSTANT', 'hockey', 'next', 'runs', 'playoffs', 'biggest',\n",
       "       'pts'], dtype='<U17')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_arr = np.array(vocab)\n",
    "w = 3.5\n",
    "print('original shape: ', vocab_arr.shape)\n",
    "f = (weights<-w) | ( weights>w)\n",
    "f[0] = True\n",
    "print('filter shape', f[f==True].shape)\n",
    "vocab_arr[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a60c7e-fdcf-4566-aa01-4a8c02562542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit= 0.013825460636515907\n",
      "filter shape:  (325,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' positve words'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def most_important_features(positive ,negative ,vocab, threshold=1):\n",
    "    \"\"\"\n",
    "    :param data: list of Example objects\n",
    "    :param vocab list of words\n",
    "    :param threshold: the number at which we neglegt redudant features\n",
    "    :return: (pos_freq, neg_freq, words)\n",
    "    \"\"\"\n",
    "    # converting to numpy\n",
    "    \n",
    "    pos_arr = np.empty(shape=(len(positive), len(vocab)))\n",
    "    for i in range(len(positive)):\n",
    "        pos_arr[i] = positive[i].x\n",
    "        \n",
    "    neg_arr = np.empty(shape=(len(negative), len(vocab)))\n",
    "    for i in range(len(negative)):\n",
    "        neg_arr[i] = negative[i].x\n",
    "        \n",
    "    vocab_arr = np.array(vocab)\n",
    "    \n",
    "    return pos_arr, neg_arr, vocab_arr\n",
    "\n",
    "pos, neg, v_arr = most_important_features(positive, negative, vocab)\n",
    "\n",
    "# # playing \n",
    "# small = 1e-9\n",
    "# p_m = np.mean(pos, axis=0) + small\n",
    "# n_m = np.mean(neg, axis=0) + small\n",
    "\n",
    "# out = p_m /n_m\n",
    "# f = (out > 2.4e7) \n",
    "# print('filter shape: ', f[f==True].shape)\n",
    "# print(vocab_arr[f])\n",
    "# print(p_m[v_arr=='baseball'])\n",
    "# print(n_m[v_arr=='baseball'])\n",
    "# print(out[v_arr=='baseball'])\n",
    "# plt.hist(out, bins=50)\n",
    "\n",
    "\n",
    "# playing \n",
    "small = 0.0\n",
    "p_m = np.mean(pos, axis=0) + small\n",
    "n_m = np.mean(neg, axis=0) + small\n",
    "\n",
    "out = p_m - n_m\n",
    "\n",
    "\n",
    "counts, bins = np.histogram(out, bins=100)\n",
    "limit = bins[np.argmax(counts)+2]\n",
    "print('Limit=',limit)\n",
    "\n",
    "f = (out > limit) \n",
    "f_shape = f[f==True].shape\n",
    "print('filter shape: ', f[f==True].shape)\n",
    "# print(vocab_arr[f])\n",
    "\n",
    "plt.stairs(counts, bins)\n",
    "\n",
    "\n",
    "''' positve words'''\n",
    "# # Sortting words\n",
    "# indcies = np.argsort(out)[-f_shape[0]:]\n",
    "# chosen_indcies = []\n",
    "# for i in indcies[::-1]:\n",
    "#     print(vocab_arr[i])\n",
    "#     o = input()\n",
    "#     if o == 'y':\n",
    "#         chosen_indcies.append(i)\n",
    "    \n",
    "# np.save('data/chosen_p_indcies', np.array(chosen_indcies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "783019e5-9df5-4cdf-9ea9-700cdf03428f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baseball' 'hit' 'pitching' 'hitter' 'pitcher' 'hitting' 'pitch' 'base'\n",
      " 'pitchers' 'innings' 'bat' 'hits' 'pitched' 'batting' 'inning' 'field'\n",
      " 'cubs' 'catcher' 'tedward' 'offense' 'majors' 'mss' 'rotation' 'bases'\n",
      " 'bullpen' 'starters' 'swing' 'roger' 'rickert' 'runner' 'fielding'\n",
      " 'pinch' 'plate' 'homers' 'hitters' 'catchers' 'runners' 'fls' 'caught'\n",
      " 'relief' 'homer' 'strike' 'starter' 'scott' 'bob' 'stadium' 'pennant'\n",
      " 'catch' 'infield' 'platoon' 'stance' 'erics' 'gotten' 'edge' 'threw'\n",
      " 'demers' 'peak' 'luriem' 'fierkelab' 'vesterman' 'fielder' 'hung'\n",
      " 'admiral' 'sepinwal' 'philly' 'mjones' 'minors' 'lineup' 'pm' 'closer'\n",
      " 'batter' 'fly' 'pace' 'bats' 'nimaster' 'baseman' 'lankford' 'shortstops'\n",
      " 'outfield' 'rushed' 'prime' 'humor' 'balls' 'baserunning' 'kirsch'\n",
      " 'gspira' 'jtchern' 'cmk' 'shortstop' 'racking' 'batters' 'snichols'\n",
      " 'leagues' 'clutch' 'lame' 'jay' 'pitches' 'gajarsky' 'ted' 'rogoff'\n",
      " 'jrogoff' 'defensively' 'glove' 'liked' 'opener' 'waivers' 'davewood'\n",
      " 'rp' 'umpires' 'paula' 'uucp' 'traven' 'niguma']\n",
      "(113,)\n"
     ]
    }
   ],
   "source": [
    "chosen_p_indcies = np.load('data/chosen_p_indcies.npy')\n",
    "print(vocab_arr[chosen_p_indcies])\n",
    "print(chosen_p_indcies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf237ae-f297-4fc7-8b62-626d3517e807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit= -0.024242211055276353\n",
      "filter shape:  (235,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' negative words'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative class\n",
    "\n",
    "limit = bins[np.argmax(counts)-2]\n",
    "print('Limit=',limit)\n",
    "f = (out < limit) \n",
    "f_shape = f[f==True].shape\n",
    "print('filter shape: ', f[f==True].shape)\n",
    "\n",
    "''' negative words'''\n",
    "\n",
    "# # Sortting words\n",
    "# indcies = np.argsort(out)[:f_shape[0]]\n",
    "# print(vocab_arr[indcies])\n",
    "# chosen_indcies = []\n",
    "# for i in indcies:\n",
    "#     print(vocab_arr[i])\n",
    "#     o = input()\n",
    "#     if o == 'y':\n",
    "#         chosen_indcies.append(i)\n",
    "    \n",
    "# np.save('data/chosen_n_indcies', np.array(chosen_indcies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21457855-9a8c-44b9-a789-9989d44b1633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hockey' 'period' 'goal' 'points' 'playoffs' 'puck' 'pp' 'goals' 'ice'\n",
      " 'players' 'pts' 'playoff' 'gld' 'player' 'coach' 'penalty' 'round'\n",
      " 'draft' 'captain' 'pick' 'net' 'scoring' 'shot' 'goalie' 'shots'\n",
      " 'defenseman' 'series' 'tie' 'maynard' 'golchowy' 'penalties' 'line'\n",
      " 'mask' 'franchise' 'zone' 'gballent' 'dchhabra' 'kkeller' 'arena' 'ca'\n",
      " 'breaker' 'forwards' 'stat' 'point' 'roughing' 'sh' 'goaltender' 'slot'\n",
      " 'pool' 'forward' 'standings' 'etxonss' 'wing' 'unassisted' 'defensemen'\n",
      " 'goalies' 'conference' 'rm' 'passed' 'stick' 'farenebt' 'assists'\n",
      " 'instead' 'circle' 'match' 'club' 'nne' 'boards' 'cordially' 'tough'\n",
      " 'hell' 'hammerl' 'octopus' 'rebound' 'howl' 'goaltending' 'gargle'\n",
      " 'defensive' 'souviens' 'poll' 'fmsalvat' 'rauser' 'stop' 'ref' 'pluggers'\n",
      " 'willis' 'rights']\n",
      "(87,)\n"
     ]
    }
   ],
   "source": [
    "chosen_n_indcies = np.load('data/chosen_n_indcies.npy')\n",
    "print(vocab_arr[chosen_n_indcies])\n",
    "print(chosen_n_indcies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaa36b-9bd5-4902-8661-1e68f2286a54",
   "metadata": {},
   "source": [
    "## What words are good predictor of chosen words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a51d06-6b32-461d-8fea-e31fba11efe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9535449473858401\n",
      "0.925532031303504\n",
      "0.9507521827650778\n",
      "0.004335358001964811\n",
      "0.8536558644363252\n"
     ]
    }
   ],
   "source": [
    "def get_selected_prediction(word, val, vocab_arr, chosen_words_indcies, weights):\n",
    "    word_index = np.where(vocab_arr == word)[0]\n",
    "    new_index = np.where(chosen_words_indcies ==word_index)[0]\n",
    "    assert len(new_index)!=0, f'\"{word}\" is not in chosen words'\n",
    "    input_vector = np.zeros_like(chosen_words_indcies)\n",
    "    input_vector[new_index] = val\n",
    "    input_vector[0] = 1\n",
    "    \n",
    "    out = sigmoid(np.dot(input_vector, weights))\n",
    "    return out\n",
    "\n",
    "chosen_p_indcies = np.load('data/chosen_p_indcies.npy')\n",
    "chosen_n_indcies = np.load('data/chosen_n_indcies.npy')\n",
    "chosen_words_indcies = np.zeros(len(chosen_p_indcies) + len(chosen_n_indcies) +1, dtype=np.int32)\n",
    "chosen_words_indcies[1:len(chosen_p_indcies) +1] = chosen_p_indcies\n",
    "chosen_words_indcies[len(chosen_p_indcies) +1:] = chosen_n_indcies\n",
    "chosen_words_indcies.sort()\n",
    "chosen_words_indcies[0] = 0 # to chose bias\n",
    "# print(chosen_words_indcies)\n",
    "\n",
    "weights = np.load('weights/selected_features_no_normalization.npy')\n",
    "\n",
    "print(get_selected_prediction('baseball', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('bases', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('hit', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('hockey', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "# print(get_selected_prediction('think', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "# print(get_selected_prediction('liked', 1, vocab_arr, chosen_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b729eaaa-1e11-42bd-8a45-1e9dfe22ffda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a bias towards p\n",
      "0.8536558644363252\n"
     ]
    }
   ],
   "source": [
    "print('There is a bias towards p')\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, chosen_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "640d7810-ac87-41d9-b181-b840ab07b434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 3 2]\n"
     ]
    }
   ],
   "source": [
    "def sort_with_keys(key_arr, val_arr):\n",
    "    '''\n",
    "    sort a val_arr using kesy_arr from ascendingly\n",
    "    :param key_arr: numpy array with keys\n",
    "    :param val_arr: numpy array to be sorted\n",
    "    :return: sorted numpy array  of val_arr\n",
    "    '''\n",
    "    index = np.lexsort((val_arr, key_arr))\n",
    "    return val_arr[index]\n",
    "out=sort_with_keys(np.array([0, 1, 2, 4]), np.array([1, 5, 3, 2]))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d51d3f6-41c7-4e9a-85be-7b85d23fb047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pitching' 'inning' 'pitcher' 'pitched' 'luriem' 'assists' 'stadium'\n",
      " 'batting' 'hitters' 'tedward' 'minors' 'rotation' 'pitchers' 'bob' 'club'\n",
      " 'baseman' 'bat' 'field' 'glove' 'lame' 'ted' 'edge' 'bullpen' 'rushed'\n",
      " 'cubs' 'majors' 'kirsch' 'balls' 'homers' 'strike' 'fielding' 'jay'\n",
      " 'fierkelab' 'rogoff' 'sepinwal' 'erics' 'catcher' 'homer' 'innings' 'mss'\n",
      " 'jtchern' 'waivers' 'paula' 'pitch' 'baseball' 'philly' 'leagues'\n",
      " 'pitches' 'scott' 'rickert' 'bats' 'hit' 'prime' 'hitter' 'base' 'roger'\n",
      " 'batters' 'stance' 'uucp' 'shortstops' 'stick' 'outfield' 'starters'\n",
      " 'scoring' 'demers' 'defensive' 'slot' 'mjones' 'rp' 'closer' 'pennant'\n",
      " 'cordially' 'infield' 'bases' 'catch' 'opener' 'lineup' 'hung' 'runner'\n",
      " 'fls' 'runners' 'batter' 'cmk' 'vesterman' 'caught' 'point' 'boards' 'pm'\n",
      " 'lankford' 'hits' 'pinch' 'clutch' 'gspira' 'admiral' 'offense'\n",
      " 'shortstop' 'peak' 'platoon' 'niguma' 'davewood' 'hitting' 'swing'\n",
      " 'fielder' 'nimaster' 'traven' 'threw' 'relief' 'racking' 'shot' 'humor'\n",
      " 'jrogoff' 'BIAS_CONSTANT' 'gotten' 'draft' 'starter' 'pp' 'breaker'\n",
      " 'defensemen' 'ref' 'baserunning' 'gajarsky' 'umpires' 'players' 'series'\n",
      " 'net' 'catchers' 'fly' 'snichols' 'pace' 'rauser' 'arena' 'plate' 'line'\n",
      " 'sh' 'defensively' 'souviens' 'rebound' 'instead' 'goaltender' 'liked'\n",
      " 'rm' 'player' 'stat' 'hell' 'goalies' 'ca' 'forward' 'gargle'\n",
      " 'unassisted' 'rights' 'passed' 'goaltending' 'pluggers' 'gballent'\n",
      " 'circle' 'shots' 'pool' 'points' 'tough' 'conference' 'howl' 'octopus'\n",
      " 'gld' 'fmsalvat' 'defenseman' 'zone' 'etxonss' 'wing']\n",
      "(168,)\n",
      "\n",
      "['pts' 'hockey' 'golchowy' 'playoffs' 'playoff' 'coach' 'goals' 'goal'\n",
      " 'ice' 'stop' 'round' 'puck' 'forwards' 'willis' 'poll' 'hammerl' 'goalie'\n",
      " 'penalty' 'captain' 'period' 'franchise' 'match' 'tie' 'farenebt' 'nne'\n",
      " 'dchhabra' 'pick' 'maynard' 'penalties' 'kkeller' 'standings' 'mask'\n",
      " 'roughing']\n",
      "(33,)\n"
     ]
    }
   ],
   "source": [
    "def get_selected_prediction_arr(val, vocab_arr, chosen_words_indcies, weights):\n",
    "    # we iclude bias as a seprate input\n",
    "    input_vector = np.diag(val * np.ones(len(chosen_words_indcies)))\n",
    "    input_vector[:, 0] = 1\n",
    "    \n",
    "    out = np.matmul(input_vector, weights.reshape([-1, 1]))\n",
    "    \n",
    "    #sigmoid\n",
    "    out = np.exp(out)\n",
    "    out = out/(1+out)\n",
    "    \n",
    "    #Sorting words\n",
    "    p_words_indcies = np.where(out>=0.5)[0]\n",
    "    p_vocab_indcies = sort_with_keys(out[p_words_indcies].reshape(-1), chosen_words_indcies[p_words_indcies])[::-1]\n",
    "    \n",
    "    n_words_indcies = np.where(out<0.5)[0]\n",
    "    n_vocab_indcies = sort_with_keys(out[n_words_indcies].reshape(-1), chosen_words_indcies[n_words_indcies])\n",
    "    return p_vocab_indcies, n_vocab_indcies\n",
    "\n",
    "p_vocab_indcies, n_vocab_indcies = get_selected_prediction_arr(1, vocab_arr, chosen_words_indcies, weights)\n",
    "\n",
    "print(vocab_arr[p_vocab_indcies])\n",
    "print(vocab_arr[p_vocab_indcies].shape)\n",
    "print()\n",
    "print(vocab_arr[n_vocab_indcies])\n",
    "print(vocab_arr[n_vocab_indcies].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366aaed-148f-4069-b5e1-3d7b411ae74b",
   "metadata": {},
   "source": [
    "## Analysing words are good predictor from all words (no selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62eca5b4-3871-4374-8c14-b2964c2ce7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9881682108478935\n",
      "0.872612101821902\n",
      "0.9781151812384982\n",
      "0.012423707456081646\n",
      "0.8096406371120461\n"
     ]
    }
   ],
   "source": [
    "all_words_indcies = np.arange(len(vocab_arr))\n",
    "weights = np.load('weights/all_features_no_normalization.npy')\n",
    "\n",
    "print(get_selected_prediction('baseball', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('bases', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('hit', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('hockey', 1, vocab_arr, all_words_indcies, weights))\n",
    "# print(get_selected_prediction('think', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, all_words_indcies, weights))\n",
    "# print(get_selected_prediction('liked', 1, vocab_arr, all_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec65a56d-629d-42a4-8421-2092e41d5363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a bias towards p\n",
      "0.8096406371120461\n"
     ]
    }
   ],
   "source": [
    "print('There is a bias towards p')\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, all_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec736927-e101-475c-b46e-e0d977115e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runs' 'justification' 'coveted' 'band' 'nicely' 'irony' 'info' 'girls'\n",
      " 'baseball' 'listing' 'sepinwal' 'steven' 'traditional' 'valued' 'central'\n",
      " 'books' 'thats' 'basically' 'liking' 'nuts' 'interpreted' 'infamous'\n",
      " 'staion' 'settled' 'evaluating' 'chip' 'mitt' 'educate' 'skate' 'likely'\n",
      " 'deny' 'helping' 'prime' 'expired' 'minimum' 'pitches' 'camelot' 'gotten'\n",
      " 'performed' 'starter' 'staff' 'gem' 'equivalent' 'pitch' 'subsidiary'\n",
      " 'fist' 'remember' 'rogoff' 'backhanded' 'water' 'exceed' 'ignorance'\n",
      " 'pour' 'anyone' 'delay' 'rating' 'bar' 'hit' 'fry' 'chuck' 'inception'\n",
      " 'resting' 'equal' 'logos']\n",
      "(4739,)\n",
      "\n",
      "['hockey' 'pts' 'playoffs' 'next' 'biggest' 'finals' 'broadcast'\n",
      " 'annoying' 'need' 'playoff' 'vs' 'polite' 'lucky' 'mmb' 'personal'\n",
      " 'period' 'golchowy' 'dkl' 'arrogant' 'combinations' 'non' 'coach' 'ext'\n",
      " 'embarrasing' 'saving' 'honoured' 'beat' 'allan' 'earning' 'khettry'\n",
      " 'stiff' 'goal' 'zones' 'shorts' 'helps' 'splits' 'wangr' 'peoples'\n",
      " 'standing' 'tvartiai' 'could' 'presently' 'h' 'angles' 'dishes' 'flat'\n",
      " 'slap' 'stamber' 'leaders' 'overtime' 'counts' 'dangerous' 'traded'\n",
      " 'wish' 'retort' 'round' 'cheese' 'producers' 'drew' 'vegetarian'\n",
      " 'younger' 'shoots' 'border' 'night' 'pro' 'cyberspace' 'adjusted' 'likes'\n",
      " 'expense' 'phrase' 'screen' 'fewer' 'values' 'conducive' 'jake' 'advice'\n",
      " 'understanding' 'occasional' 'excuses' 'fault' 'join' 'write' 'flagship'\n",
      " 'grew' 'shown' 'crucial' 'locker' 'cholesterol' 'graduated' 'duties'\n",
      " 'showing' 'programs' 'insgnia' 'preceding' 'immediate' 'standings'\n",
      " 'cents' 'newfound' 'personally' 'problems' 'cool' 'swinging' 'students'\n",
      " 'guys' 'pool' 'daniell' 'credence' 'penalized' 'knew' 'blank' 'stripped'\n",
      " 'production' 'shift' 'dispersal' 'office' 'victors' 'throws' 'relieved'\n",
      " 'goes' 'kkeller' 'stat' 'treated' 'interviews' 'vision' 'fight' 'sets'\n",
      " 'recognise' 'judge' 'tervio' 'tervo' 'specifications' 'speech' 'dmoney'\n",
      " 'ca' 'competent' 'onto' 'mount' 'galvint' 'asked' 'analysis' 'penalty'\n",
      " 'decibel' 'whether' 'continent' 'studio' 'gargle' 'players' 'blotted'\n",
      " 'hair' 'wing' 'ten' 'lets' 'ages' 'represents' 'interfered' 'eb'\n",
      " 'forwards' 'armor' 'died' 'prg' 'modest' 'almost' 'stark' 'wrote'\n",
      " 'replying' 'honor' 'golden' 'increased' 'triples' 'leaves' 'loss'\n",
      " 'scalpers' 'maniacal' 'flattering' 'file' 'fame' 'tactics' 'impressive'\n",
      " 'declined' 'must' 'abotu' 'tv' 'answer' 'considerably' 'populated'\n",
      " 'hilarious' 'bust' 'letter' 'stretcher' 'clearing' 'define' 'lights'\n",
      " 'shooting' 'retard' 'g' 'adjusting' 'minnesota' 'preceded' 'content'\n",
      " 'four' 'leafs' 'points' 'champion' 'header' 'coffee' 'proof' 'uptade'\n",
      " 'played' 'fashion' 'coincidence' 'followed' 'inexpensive' 'claiming'\n",
      " 'network' 'chosen' 'horizon' 'mask' 'physical' 'quoted' 'puts' 'tee'\n",
      " 'intelligence' 'fee' 'helmets' 'disciplined' 'fewest' 'whole' 'cartlidge'\n",
      " 'revenge' 'pick' 'filed' 'punches' 'sarcasm' 'carry' 'mid' 'assists'\n",
      " 'prove' 'dollar' 'fmsalvat' 'ing' 'guns' 'steady' 'rd' 'disclose' 'sheet'\n",
      " 'normally' 'evening' 'en' 'soured' 'skills' 'complained' 'blue' 'stomach'\n",
      " 'pigpens' 'keys' 'opposite' 'indicate' 'elementary' 'test' 'sort' 'begin'\n",
      " 'instant' 'tad' 'contributors' 'yell' 'adress' 'er' 'parts' 'spelling'\n",
      " 'bratt' 'concession' 'sent' 'contending' 'youth' 'tools' 'fire' 'full'\n",
      " 'deflected' 'cleared' 'assholes' 'selection' 'everywhere' 'measure'\n",
      " 'email' 'huot' 'possession' 'ques' 'chronological' 'memorable' 'protests'\n",
      " 'equally' 'drugs' 'package' 'disappointed' 'foolish' 'honest'\n",
      " 'appreciated' 'beautiful' 'advisedly' 'aggressive' 'read' 'networks'\n",
      " 'fair' 'hzazula' 'tremendously' 'house' 'quite' 'metro' 'dinger' 'unfair'\n",
      " 'doubter' 'slacelle' 'referring' 'saddled' 'garryola' 'sometimes'\n",
      " 'anyway' 'editor' 'gjp' 'generate' 'transplanted' 'draw' 'scorer'\n",
      " 'stuppid' 'copyright' 'contains' 'doubts' 'delaying' 'boards' 'simulcast'\n",
      " 'maynard' 'puck' 'circle' 'jerseys' 'quarters' 'rubbing' 'superb' 'crazy'\n",
      " 'tommorrow' 'pops' 'watch' 'passport' 'reading' 'topic' 'translation'\n",
      " 'shots' 'pete' 'card' 'conf' 'robin' 'unfortunate' 'married' 'breaker'\n",
      " 'concluded' 'center' 'massive' 'announced' 'history' 'circumstances'\n",
      " 'currently' 'simple' 'clip' 'rocket' 'pleasant' 'sp' 'dropping' 'shall'\n",
      " 'exact' 'admittedly' 'refer' 'interpretation' 'stacks' 'offensive'\n",
      " 'disputed' 'away' 'rebound' 'strikes' 'unhappy' 'fix' 'receiving'\n",
      " 'weather' 'appeared' 'approx' 'boutch' 'shining' 'legs' 'airwaves'\n",
      " 'trading' 'viable' 'mildly' 'severed' 'franchise' 'captain' 'taped'\n",
      " 'displaying' 'scored' 'participating' 'joking']\n",
      "(398,)\n"
     ]
    }
   ],
   "source": [
    "p_vocab_indcies, n_vocab_indcies = get_selected_prediction_arr(1, vocab_arr, all_words_indcies, weights)\n",
    "\n",
    "print(vocab_arr[p_vocab_indcies][:64])\n",
    "print(vocab_arr[p_vocab_indcies].shape)\n",
    "print()\n",
    "print(vocab_arr[n_vocab_indcies])\n",
    "print(vocab_arr[n_vocab_indcies].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ca310-6992-47d2-abba-6cfde0607313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
