{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c4e367-076a-4220-b9f8-5fd0ab0aca14",
   "metadata": {},
   "source": [
    "# Binary Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c69afb-1036-4df5-8172-d39852be298b",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* adjust early stoping [DONE]()\n",
    "* ploting [done]()\n",
    "* display plot on jupyter [Done]()\n",
    "* get the most test accuracy of the model [Done]()\n",
    "* adding Dateset calss[done]()\n",
    "* adding normalization [done]()\n",
    "* analyzning features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec8d08-6009-4d3d-afd7-544a1e8f26b3",
   "metadata": {},
   "source": [
    "## Learning Rate schedules:\n",
    "* time based\n",
    "* exponenetial\n",
    "* step\n",
    "* adaptive learning rate:\n",
    "    * Momentum\n",
    "    * adagrade\n",
    "    * RMS prob\n",
    "    * Adam\n",
    "    * Deep learning learning rate (using LSTMs, [reiformcnt learning](https://arxiv.org/pdf/1909.09712.pdf)\n",
    "---------------\n",
    "| Adaptive Learning Rate Schedule | Non adaptive |\n",
    "|------------|--------------|\n",
    "|faster in convergence | slower in convergence |\n",
    "|generalizes worst in simple problems and images (data close to mean -> Guassian) and bad test and train loss [[1]](https://proceedings.neurips.cc/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf)  | generalizes better in simple problems and images (data close to mean -> Guassian) and better test and train l\n",
    "|better in heavy tailed didtribution (date is very var from mean) like: BERT[[2]](https://proceedings.neurips.cc/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf) | worst |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3016459-1a66-40ba-a109-22e9d05c2fcc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace1d6f-30e8-40fe-be28-9cfe749c5b76",
   "metadata": {},
   "source": [
    "### Training all features without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4551b246-56ea-420c-9f93-45488e109a25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-521.886868 Test logP=-139.687949 Train Acc=0.869361 Test Acc=0.744361 TrainLoss=0.701773 TestLoss=1.515146\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-217.544358 Test logP=-87.812548 Train Acc=0.936090 Test Acc=0.804511 TrainLoss=0.292573 TestLoss=0.952511\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-113.082500 Test logP=-71.322012 Train Acc=0.963346 Test Acc=0.849624 TrainLoss=0.153330 TestLoss=0.773635\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-73.666915 Test logP=-67.366276 Train Acc=0.971805 Test Acc=0.842105 TrainLoss=0.099886 TestLoss=0.730733\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-56.077619 Test logP=-64.242176 Train Acc=0.976504 Test Acc=0.864662 TrainLoss=0.076037 TestLoss=0.696842\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-45.352154 Test logP=-62.609569 Train Acc=0.980263 Test Acc=0.864662 TrainLoss=0.061494 TestLoss=0.679134\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-38.466759 Test logP=-61.194573 Train Acc=0.984962 Test Acc=0.864662 TrainLoss=0.052158 TestLoss=0.663780\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-32.956624 Test logP=-59.854989 Train Acc=0.986842 Test Acc=0.864662 TrainLoss=0.044686 TestLoss=0.649249\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-28.547823 Test logP=-58.719548 Train Acc=0.987782 Test Acc=0.864662 TrainLoss=0.038708 TestLoss=0.636932\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-24.910109 Test logP=-57.925270 Train Acc=0.989662 Test Acc=0.872180 TrainLoss=0.033776 TestLoss=0.628317\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-21.915473 Test logP=-57.307996 Train Acc=0.990602 Test Acc=0.887218 TrainLoss=0.029716 TestLoss=0.621621\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-19.422459 Test logP=-56.779832 Train Acc=0.993421 Test Acc=0.887218 TrainLoss=0.026335 TestLoss=0.615892\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Train logP=-17.309282 Test logP=-56.276953 Train Acc=0.993421 Test Acc=0.887218 TrainLoss=0.023470 TestLoss=0.610436\n",
      "----------------------------\n",
      "Epoch:14\n",
      "Train logP=-15.497302 Test logP=-55.772323 Train Acc=0.994361 Test Acc=0.887218 TrainLoss=0.021013 TestLoss=0.604962\n",
      "----------------------------\n",
      "Epoch:15\n",
      "Train logP=-13.940727 Test logP=-55.268506 Train Acc=0.996241 Test Acc=0.894737 TrainLoss=0.018902 TestLoss=0.599496\n",
      "----------------------------\n",
      "Epoch:16\n",
      "Train logP=-12.606799 Test logP=-54.783600 Train Acc=0.997180 Test Acc=0.894737 TrainLoss=0.017094 TestLoss=0.594235\n",
      "----------------------------\n",
      "Epoch:17\n",
      "Train logP=-11.464585 Test logP=-54.337627 Train Acc=0.997180 Test Acc=0.902256 TrainLoss=0.015545 TestLoss=0.589396\n",
      "----------------------------\n",
      "Epoch:18\n",
      "Train logP=-10.484285 Test logP=-53.943659 Train Acc=0.997180 Test Acc=0.902256 TrainLoss=0.014216 TestLoss=0.585122\n",
      "----------------------------\n",
      "Epoch:19\n",
      "Train logP=-9.637899 Test logP=-53.604167 Train Acc=0.997180 Test Acc=0.909774 TrainLoss=0.013068 TestLoss=0.581438\n",
      "----------------------------\n",
      "Epoch:20\n",
      "Train logP=-8.900226 Test logP=-53.313347 Train Acc=0.997180 Test Acc=0.909774 TrainLoss=0.012068 TestLoss=0.578282\n",
      "----------------------------\n",
      "Epoch:21\n",
      "Train logP=-8.250429 Test logP=-53.062683 Train Acc=0.997180 Test Acc=0.909774 TrainLoss=0.011187 TestLoss=0.575562\n",
      "----------------------------\n",
      "Epoch:22\n",
      "Train logP=-7.672043 Test logP=-52.844520 Train Acc=0.997180 Test Acc=0.909774 TrainLoss=0.010403 TestLoss=0.573194\n",
      "----------------------------\n",
      "Epoch:23\n",
      "Train logP=-7.151903 Test logP=-52.652882 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.009697 TestLoss=0.571114\n",
      "----------------------------\n",
      "Epoch:24\n",
      "Train logP=-6.679257 Test logP=-52.483157 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.009056 TestLoss=0.569272\n",
      "----------------------------\n",
      "Epoch:25\n",
      "Train logP=-6.245397 Test logP=-52.331608 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.008468 TestLoss=0.567627\n",
      "----------------------------\n",
      "Epoch:26\n",
      "Train logP=-5.843606 Test logP=-52.195037 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.007923 TestLoss=0.566144\n",
      "----------------------------\n",
      "Epoch:27\n",
      "Train logP=-5.469272 Test logP=-52.070656 Train Acc=0.998120 Test Acc=0.909774 TrainLoss=0.007416 TestLoss=0.564794\n",
      "----------------------------\n",
      "Epoch:28\n",
      "Train logP=-5.120060 Test logP=-51.956178 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.006942 TestLoss=0.563551\n",
      "----------------------------\n",
      "Epoch:29\n",
      "Train logP=-4.795934 Test logP=-51.850127 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.006503 TestLoss=0.562400\n",
      "----------------------------\n",
      "Epoch:30\n",
      "Train logP=-4.498613 Test logP=-51.752287 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.006100 TestLoss=0.561338\n",
      "----------------------------\n",
      "Epoch:31\n",
      "Train logP=-4.230108 Test logP=-51.663931 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.005736 TestLoss=0.560379\n",
      "----------------------------\n",
      "Epoch:32\n",
      "Train logP=-3.990836 Test logP=-51.587317 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.005411 TestLoss=0.559547\n",
      "----------------------------\n",
      "Epoch:33\n",
      "Train logP=-3.778701 Test logP=-51.524406 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.005124 TestLoss=0.558864\n",
      "----------------------------\n",
      "Epoch:34\n",
      "Train logP=-3.589837 Test logP=-51.475753 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.004868 TestLoss=0.558335\n",
      "----------------------------\n",
      "Epoch:35\n",
      "Train logP=-3.420049 Test logP=-51.440399 Train Acc=0.999060 Test Acc=0.909774 TrainLoss=0.004637 TestLoss=0.557951\n",
      "----------------------------\n",
      "Epoch:36\n",
      "Train logP=-3.265747 Test logP=-51.416486 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.004428 TestLoss=0.557691\n",
      "----------------------------\n",
      "Epoch:37\n",
      "Train logP=-3.124181 Test logP=-51.401897 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.004236 TestLoss=0.557532\n",
      "----------------------------\n",
      "Epoch:38\n",
      "Train logP=-2.993315 Test logP=-51.394631 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.004059 TestLoss=0.557452\n",
      "----------------------------\n",
      "Epoch:39\n",
      "Train logP=-2.871630 Test logP=-51.392940 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.003894 TestLoss=0.557433\n",
      "----------------------------\n",
      "Epoch:40\n",
      "Early Stop\n",
      "Train logP=-2.757970 Test logP=-51.395370 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.003740 TestLoss=0.557458\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=2000  --step=0.08 --early_stop=1 --normalize=no \\\n",
    "    --log=no --log_step=200 --plot_name=all_features_no_normalization \\\n",
    "    --save_weights_path=weights/all_features_no_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52930e-bca4-492c-b77a-5fabc710f41d",
   "metadata": {},
   "source": [
    "![plot](figures/all_features_no_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5eb9f-3d00-422e-88dc-ff79dbecca28",
   "metadata": {},
   "source": [
    "### Training All features with Noramlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1795b41e-246a-4d2d-84e1-2cb89730a030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-1420.191893 Test logP=-441.904951 Train Acc=0.899436 Test Acc=0.774436 TrainLoss=1.800750 TestLoss=4.513933\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-539.922492 Test logP=-310.605883 Train Acc=0.962406 Test Acc=0.819549 TrainLoss=0.688692 TestLoss=3.193685\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-147.861275 Test logP=-281.754546 Train Acc=0.987782 Test Acc=0.819549 TrainLoss=0.190379 TestLoss=2.901246\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-93.250791 Test logP=-272.909883 Train Acc=0.993421 Test Acc=0.834586 TrainLoss=0.117004 TestLoss=2.821451\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-58.002648 Test logP=-271.847600 Train Acc=0.995301 Test Acc=0.834586 TrainLoss=0.075882 TestLoss=2.791456\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-30.047144 Test logP=-273.210257 Train Acc=0.997180 Test Acc=0.842105 TrainLoss=0.039740 TestLoss=2.803731\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-0.667658 Test logP=-266.257080 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000905 TestLoss=2.742196\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-0.640799 Test logP=-257.417598 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000869 TestLoss=2.645063\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-0.039032 Test logP=-259.796024 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000053 TestLoss=2.671533\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-0.031804 Test logP=-259.479443 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000043 TestLoss=2.667849\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-0.027139 Test logP=-259.059154 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000037 TestLoss=2.663280\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-0.023789 Test logP=-258.646313 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000032 TestLoss=2.658847\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Train logP=-0.021238 Test logP=-258.259339 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000029 TestLoss=2.654709\n",
      "----------------------------\n",
      "Epoch:14\n",
      "Train logP=-0.019215 Test logP=-257.899962 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000026 TestLoss=2.650872\n",
      "----------------------------\n",
      "Epoch:15\n",
      "Train logP=-0.017565 Test logP=-257.566231 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000024 TestLoss=2.647312\n",
      "----------------------------\n",
      "Epoch:16\n",
      "Train logP=-0.016186 Test logP=-257.255489 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000022 TestLoss=2.643997\n",
      "----------------------------\n",
      "Epoch:17\n",
      "Train logP=-0.015015 Test logP=-256.965151 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000020 TestLoss=2.640901\n",
      "----------------------------\n",
      "Epoch:18\n",
      "Train logP=-0.014006 Test logP=-256.692913 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000019 TestLoss=2.637997\n",
      "----------------------------\n",
      "Epoch:19\n",
      "Train logP=-0.013127 Test logP=-256.436774 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000018 TestLoss=2.635264\n",
      "----------------------------\n",
      "Epoch:20\n",
      "Train logP=-0.012353 Test logP=-256.195018 Train Acc=1.000000 Test Acc=0.849624 TrainLoss=0.000017 TestLoss=2.632685\n",
      "----------------------------\n",
      "Epoch:21\n",
      "Train logP=-0.011666 Test logP=-255.966173 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000016 TestLoss=2.630242\n",
      "----------------------------\n",
      "Epoch:22\n",
      "Train logP=-0.011052 Test logP=-255.748979 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000015 TestLoss=2.627924\n",
      "----------------------------\n",
      "Epoch:23\n",
      "Train logP=-0.010499 Test logP=-255.542346 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000014 TestLoss=2.625719\n",
      "----------------------------\n",
      "Epoch:24\n",
      "Train logP=-0.010000 Test logP=-255.345332 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000014 TestLoss=2.623615\n",
      "----------------------------\n",
      "Epoch:25\n",
      "Train logP=-0.009545 Test logP=-255.157117 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000013 TestLoss=2.621606\n",
      "----------------------------\n",
      "Epoch:26\n",
      "Train logP=-0.009130 Test logP=-254.976983 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000012 TestLoss=2.619682\n",
      "----------------------------\n",
      "Epoch:27\n",
      "Train logP=-0.008750 Test logP=-254.804298 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000012 TestLoss=2.617838\n",
      "----------------------------\n",
      "Epoch:28\n",
      "Train logP=-0.008400 Test logP=-254.638503 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000011 TestLoss=2.616067\n",
      "----------------------------\n",
      "Epoch:29\n",
      "Train logP=-0.008077 Test logP=-254.479101 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000011 TestLoss=2.614364\n",
      "----------------------------\n",
      "Epoch:30\n",
      "Train logP=-0.007777 Test logP=-254.325649 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000011 TestLoss=2.612725\n",
      "----------------------------\n",
      "Epoch:31\n",
      "Train logP=-0.007499 Test logP=-254.177748 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000010 TestLoss=2.611145\n",
      "----------------------------\n",
      "Epoch:32\n",
      "Train logP=-0.007240 Test logP=-254.035040 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000010 TestLoss=2.609620\n",
      "----------------------------\n",
      "Epoch:33\n",
      "Train logP=-0.006999 Test logP=-253.897200 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000009 TestLoss=2.608148\n",
      "----------------------------\n",
      "Epoch:34\n",
      "Train logP=-0.006772 Test logP=-253.763936 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000009 TestLoss=2.606724\n",
      "----------------------------\n",
      "Epoch:35\n",
      "Train logP=-0.006560 Test logP=-253.634978 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000009 TestLoss=2.605346\n",
      "----------------------------\n",
      "Epoch:36\n",
      "Train logP=-0.006361 Test logP=-253.510083 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000009 TestLoss=2.604011\n",
      "----------------------------\n",
      "Epoch:37\n",
      "Train logP=-0.006173 Test logP=-253.389027 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000008 TestLoss=2.602717\n",
      "----------------------------\n",
      "Epoch:38\n",
      "Train logP=-0.005996 Test logP=-253.271603 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000008 TestLoss=2.601462\n",
      "----------------------------\n",
      "Epoch:39\n",
      "Train logP=-0.005829 Test logP=-253.157624 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000008 TestLoss=2.600244\n",
      "----------------------------\n",
      "Epoch:40\n",
      "Train logP=-0.005671 Test logP=-253.046913 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000008 TestLoss=2.599060\n",
      "----------------------------\n",
      "Epoch:41\n",
      "Train logP=-0.005521 Test logP=-252.939311 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000007 TestLoss=2.597910\n",
      "----------------------------\n",
      "Epoch:42\n",
      "Train logP=-0.005379 Test logP=-252.834667 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000007 TestLoss=2.596791\n",
      "----------------------------\n",
      "Epoch:43\n",
      "Train logP=-0.005244 Test logP=-252.732843 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000007 TestLoss=2.595702\n",
      "----------------------------\n",
      "Epoch:44\n",
      "Train logP=-0.005116 Test logP=-252.633709 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000007 TestLoss=2.594642\n",
      "----------------------------\n",
      "Epoch:45\n",
      "Train logP=-0.004993 Test logP=-252.537146 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000007 TestLoss=2.593610\n",
      "----------------------------\n",
      "Epoch:46\n",
      "Train logP=-0.004877 Test logP=-252.443042 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000007 TestLoss=2.592603\n",
      "----------------------------\n",
      "Epoch:47\n",
      "Train logP=-0.004765 Test logP=-252.351291 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000006 TestLoss=2.591622\n",
      "----------------------------\n",
      "Epoch:48\n",
      "Train logP=-0.004659 Test logP=-252.261796 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000006 TestLoss=2.590665\n",
      "----------------------------\n",
      "Epoch:49\n",
      "Train logP=-0.004557 Test logP=-252.174466 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000006 TestLoss=2.589731\n",
      "----------------------------\n",
      "Epoch:50\n",
      "Train logP=-0.004460 Test logP=-252.089213 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000006 TestLoss=2.588819\n",
      "----------------------------\n",
      "Epoch:51\n",
      "Train logP=-0.004366 Test logP=-252.005959 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000006 TestLoss=2.587928\n",
      "----------------------------\n",
      "Epoch:52\n",
      "Train logP=-0.004277 Test logP=-251.924627 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000006 TestLoss=2.587058\n",
      "----------------------------\n",
      "Epoch:53\n",
      "Train logP=-0.004191 Test logP=-251.845145 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000006 TestLoss=2.586208\n",
      "----------------------------\n",
      "Epoch:54\n",
      "Train logP=-0.004108 Test logP=-251.767447 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000006 TestLoss=2.585377\n",
      "----------------------------\n",
      "Epoch:55\n",
      "Train logP=-0.004029 Test logP=-251.691470 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.584564\n",
      "----------------------------\n",
      "Epoch:56\n",
      "Train logP=-0.003952 Test logP=-251.617153 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.583769\n",
      "----------------------------\n",
      "Epoch:57\n",
      "Train logP=-0.003879 Test logP=-251.544441 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.582991\n",
      "----------------------------\n",
      "Epoch:58\n",
      "Train logP=-0.003808 Test logP=-251.473281 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.582229\n",
      "----------------------------\n",
      "Epoch:59\n",
      "Train logP=-0.003739 Test logP=-251.403622 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.581484\n",
      "----------------------------\n",
      "Epoch:60\n",
      "Train logP=-0.003673 Test logP=-251.335416 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.580754\n",
      "----------------------------\n",
      "Epoch:61\n",
      "Train logP=-0.003610 Test logP=-251.268619 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.580039\n",
      "----------------------------\n",
      "Epoch:62\n",
      "Train logP=-0.003548 Test logP=-251.203189 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.579339\n",
      "----------------------------\n",
      "Epoch:63\n",
      "Train logP=-0.003489 Test logP=-251.139084 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.578653\n",
      "----------------------------\n",
      "Epoch:64\n",
      "Train logP=-0.003431 Test logP=-251.076267 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.577980\n",
      "----------------------------\n",
      "Epoch:65\n",
      "Train logP=-0.003376 Test logP=-251.014700 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000005 TestLoss=2.577321\n",
      "----------------------------\n",
      "Epoch:66\n",
      "Train logP=-0.003322 Test logP=-250.954351 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.576675\n",
      "----------------------------\n",
      "Epoch:67\n",
      "Train logP=-0.003270 Test logP=-250.895185 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.576042\n",
      "----------------------------\n",
      "Epoch:68\n",
      "Train logP=-0.003219 Test logP=-250.837172 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.575421\n",
      "----------------------------\n",
      "Epoch:69\n",
      "Train logP=-0.003170 Test logP=-250.780281 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.574812\n",
      "----------------------------\n",
      "Epoch:70\n",
      "Train logP=-0.003123 Test logP=-250.724486 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.574215\n",
      "----------------------------\n",
      "Epoch:71\n",
      "Train logP=-0.003077 Test logP=-250.669758 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.573629\n",
      "----------------------------\n",
      "Epoch:72\n",
      "Train logP=-0.003032 Test logP=-250.616072 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.573054\n",
      "----------------------------\n",
      "Epoch:73\n",
      "Train logP=-0.002988 Test logP=-250.563403 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.572490\n",
      "----------------------------\n",
      "Epoch:74\n",
      "Train logP=-0.002946 Test logP=-250.511728 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.571937\n",
      "----------------------------\n",
      "Epoch:75\n",
      "Train logP=-0.002905 Test logP=-250.461026 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.571394\n",
      "----------------------------\n",
      "Epoch:76\n",
      "Train logP=-0.002865 Test logP=-250.411273 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.570861\n",
      "----------------------------\n",
      "Epoch:77\n",
      "Train logP=-0.002826 Test logP=-250.362450 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.570338\n",
      "----------------------------\n",
      "Epoch:78\n",
      "Train logP=-0.002788 Test logP=-250.314538 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.569825\n",
      "----------------------------\n",
      "Epoch:79\n",
      "Train logP=-0.002751 Test logP=-250.267517 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.569322\n",
      "----------------------------\n",
      "Epoch:80\n",
      "Train logP=-0.002716 Test logP=-250.221370 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.568828\n",
      "----------------------------\n",
      "Epoch:81\n",
      "Train logP=-0.002681 Test logP=-250.176079 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.568343\n",
      "----------------------------\n",
      "Epoch:82\n",
      "Train logP=-0.002647 Test logP=-250.131628 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.567867\n",
      "----------------------------\n",
      "Epoch:83\n",
      "Train logP=-0.002613 Test logP=-250.088000 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000004 TestLoss=2.567400\n",
      "----------------------------\n",
      "Epoch:84\n",
      "Train logP=-0.002581 Test logP=-250.045180 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.566941\n",
      "----------------------------\n",
      "Epoch:85\n",
      "Train logP=-0.002550 Test logP=-250.003154 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.566491\n",
      "----------------------------\n",
      "Epoch:86\n",
      "Train logP=-0.002519 Test logP=-249.961907 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.566049\n",
      "----------------------------\n",
      "Epoch:87\n",
      "Train logP=-0.002489 Test logP=-249.921425 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.565616\n",
      "----------------------------\n",
      "Epoch:88\n",
      "Train logP=-0.002459 Test logP=-249.881695 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.565190\n",
      "----------------------------\n",
      "Epoch:89\n",
      "Train logP=-0.002431 Test logP=-249.842704 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.564773\n",
      "----------------------------\n",
      "Epoch:90\n",
      "Train logP=-0.002403 Test logP=-249.804440 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.564363\n",
      "----------------------------\n",
      "Epoch:91\n",
      "Train logP=-0.002375 Test logP=-249.766890 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.563961\n",
      "----------------------------\n",
      "Epoch:92\n",
      "Train logP=-0.002349 Test logP=-249.730043 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.563566\n",
      "----------------------------\n",
      "Epoch:93\n",
      "Train logP=-0.002322 Test logP=-249.693887 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.563179\n",
      "----------------------------\n",
      "Epoch:94\n",
      "Train logP=-0.002297 Test logP=-249.658411 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.562799\n",
      "----------------------------\n",
      "Epoch:95\n",
      "Train logP=-0.002272 Test logP=-249.623604 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.562427\n",
      "----------------------------\n",
      "Epoch:96\n",
      "Train logP=-0.002248 Test logP=-249.589456 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.562061\n",
      "----------------------------\n",
      "Epoch:97\n",
      "Train logP=-0.002224 Test logP=-249.555956 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.561702\n",
      "----------------------------\n",
      "Epoch:98\n",
      "Train logP=-0.002200 Test logP=-249.523094 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.561350\n",
      "----------------------------\n",
      "Epoch:99\n",
      "Train logP=-0.002177 Test logP=-249.490861 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.561005\n",
      "----------------------------\n",
      "Epoch:100\n",
      "Train logP=-0.002155 Test logP=-249.459246 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.560667\n",
      "----------------------------\n",
      "Train logP=-0.002155 Test logP=-249.459246 Train Acc=1.000000 Test Acc=0.857143 TrainLoss=0.000003 TestLoss=2.560667\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=100  --step=0.08 --early_stop=3 --normalize=yes \\\n",
    "    --log=no --log_step=200 --plot_name=all_features_with_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d4306-094d-4022-97bd-6438ef923a5b",
   "metadata": {},
   "source": [
    "![all_normalized](figures/all_features_with_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74b8f0-cd6e-4884-b465-4087a682c6b0",
   "metadata": {},
   "source": [
    "## Traing selected features without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888d5e74-fa5a-42c5-b205-16e373915c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-376.803548 Test logP=-33.523141 Train Acc=0.824248 Test Acc=0.894737 TrainLoss=0.510914 TestLoss=0.363637\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-290.504943 Test logP=-21.533256 Train Acc=0.870301 Test Acc=0.924812 TrainLoss=0.393900 TestLoss=0.233578\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-255.191170 Test logP=-17.482726 Train Acc=0.893797 Test Acc=0.932331 TrainLoss=0.346018 TestLoss=0.189641\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-236.524933 Test logP=-15.759704 Train Acc=0.903195 Test Acc=0.939850 TrainLoss=0.320708 TestLoss=0.170951\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-224.787657 Test logP=-14.919939 Train Acc=0.909774 Test Acc=0.939850 TrainLoss=0.304793 TestLoss=0.161842\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-216.936924 Test logP=-14.518586 Train Acc=0.911654 Test Acc=0.947368 TrainLoss=0.294148 TestLoss=0.157488\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-211.158322 Test logP=-14.323780 Train Acc=0.912594 Test Acc=0.939850 TrainLoss=0.286313 TestLoss=0.155375\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-206.532488 Test logP=-14.219030 Train Acc=0.912594 Test Acc=0.939850 TrainLoss=0.280041 TestLoss=0.154239\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-202.668893 Test logP=-14.153692 Train Acc=0.915414 Test Acc=0.939850 TrainLoss=0.274802 TestLoss=0.153530\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-199.392291 Test logP=-14.112696 Train Acc=0.917293 Test Acc=0.939850 TrainLoss=0.270359 TestLoss=0.153085\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-196.597720 Test logP=-14.092754 Train Acc=0.918233 Test Acc=0.939850 TrainLoss=0.266570 TestLoss=0.152869\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-194.200616 Test logP=-14.090976 Train Acc=0.921053 Test Acc=0.947368 TrainLoss=0.263320 TestLoss=0.152849\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Early Stop\n",
      "Train logP=-192.130880 Test logP=-14.103659 Train Acc=0.921992 Test Acc=0.947368 TrainLoss=0.260513 TestLoss=0.152987\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=200  --step=.08 --early_stop=1 --normalize=no \\\n",
    "    --log=no --log_step=50 --plot_name=selected_features_no_normalization \\\n",
    "    --save_weights_path='weights/selected_features_no_normalization' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e58cd8-945a-4bd7-b7d8-8eb2d61e5fee",
   "metadata": {},
   "source": [
    "![selected_no_normalization](figures/selected_features_no_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0638cbc-3da5-4d6d-a101-4a99cd6e82ad",
   "metadata": {},
   "source": [
    "## Trainign Selected Features with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "456b1814-b331-4359-a504-da6b4458c3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "    Update 1\tTP -4868.335080\tHP -467.869872\tTA 0.471805\tHA 0.533835\n",
      "    Update 201\tTP -1381.331320\tHP -129.813547\tTA 0.767857\tHA 0.812030\n",
      "    Update 401\tTP -791.024044\tHP -59.193294\tTA 0.828947\tHA 0.909774\n",
      "    Update 601\tTP -385.155447\tHP -19.255807\tTA 0.880639\tHA 0.954887\n",
      "    Update 801\tTP -265.245576\tHP -15.788990\tTA 0.906015\tHA 0.947368\n",
      "    Update 1001\tTP -242.604030\tHP -13.608165\tTA 0.910714\tHA 0.954887\n",
      "Train logP=-242.604030 Test logP=-13.608165 Train Acc=0.910714 Test Acc=0.954887 TrainLoss=0.327237 TestLoss=0.147612\n",
      "----------------------------\n",
      "Epoch:2\n",
      "    Update 1\tTP -224.693351\tHP -14.389476\tTA 0.914474\tHA 0.962406\n",
      "    Update 201\tTP -228.161541\tHP -12.853893\tTA 0.911654\tHA 0.954887\n",
      "    Update 401\tTP -221.368567\tHP -15.196707\tTA 0.913534\tHA 0.962406\n",
      "    Update 601\tTP -195.800804\tHP -12.198843\tTA 0.916353\tHA 0.969925\n",
      "    Update 801\tTP -178.368708\tHP -12.117319\tTA 0.919173\tHA 0.969925\n",
      "    Update 1001\tTP -195.460572\tHP -13.803910\tTA 0.917293\tHA 0.962406\n",
      "Early Stop\n",
      "Train logP=-195.460572 Test logP=-13.803910 Train Acc=0.917293 Test Acc=0.962406 TrainLoss=0.265028 TestLoss=0.149736\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=200  --step=.08 --early_stop=1 --normalize=yes \\\n",
    "    --log=yes --log_step=200 --plot_name='selected_features_with_normalization' \\\n",
    "    --save_weights_path='weights/selected_features_with_normalization' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a0ac7-f352-4b41-ac65-718cfe30fe9f",
   "metadata": {},
   "source": [
    "![slected with normalization](figures/selected_features_with_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac82e47-aad2-47f6-8f1c-b0f8dbc4f985",
   "metadata": {},
   "source": [
    "# Analyzing uingram Features of: logstic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ccde49-bd90-4c04-a2b2-05bfc6a7b7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from logreg import ExamplesDataset, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07e3e4c-328f-4a4f-9823-36d4f2fac687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   0.,   1.,   0.,   1.,   1.,   1.,   0.,   0.,   0.,\n",
       "          2.,   4.,  10.,  16.,  17.,  17.,  55.,  54., 105., 149., 175.,\n",
       "        248., 297., 359., 375., 424., 436., 404., 375., 341., 316., 249.,\n",
       "        202., 168., 117.,  82.,  58.,  29.,  25.,  11.,   5.,   4.,   2.,\n",
       "          0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([-5.90130406, -5.68775304, -5.47420203, -5.26065101, -5.0471    ,\n",
       "        -4.83354899, -4.61999797, -4.40644696, -4.19289594, -3.97934493,\n",
       "        -3.76579392, -3.5522429 , -3.33869189, -3.12514088, -2.91158986,\n",
       "        -2.69803885, -2.48448783, -2.27093682, -2.05738581, -1.84383479,\n",
       "        -1.63028378, -1.41673276, -1.20318175, -0.98963074, -0.77607972,\n",
       "        -0.56252871, -0.34897769, -0.13542668,  0.07812433,  0.29167535,\n",
       "         0.50522636,  0.71877737,  0.93232839,  1.1458794 ,  1.35943042,\n",
       "         1.57298143,  1.78653244,  2.00008346,  2.21363447,  2.42718549,\n",
       "         2.6407365 ,  2.85428751,  3.06783853,  3.28138954,  3.49494056,\n",
       "         3.70849157,  3.92204258,  4.1355936 ,  4.34914461,  4.56269562,\n",
       "         4.77624664]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# playing with weights\n",
    "# index0-> bias\n",
    "weights = np.load('weights/all_features_no_normalization.npy')\n",
    "plt.hist(weights, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395028b5-1c88-4ff9-af48-9de0b6ceb298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Reading dataset'''\n",
    "dataset = ExamplesDataset('data/positive', 'data/negative', 'data/vocab')\n",
    "positive, negative, vocab = dataset.get_positive_negative_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60478f3-d61e-4c55-89fa-a17997ecbca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:  (5137,)\n",
      "filter shape (8,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['BIAS_CONSTANT', 'hockey', 'next', 'runs', 'playoffs', 'finals',\n",
       "       'biggest', 'pts'], dtype='<U17')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_arr = np.array(vocab)\n",
    "w = 3.5\n",
    "print('original shape: ', vocab_arr.shape)\n",
    "f = (weights<-w) | ( weights>w)\n",
    "f[0] = True\n",
    "print('filter shape', f[f==True].shape)\n",
    "vocab_arr[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a60c7e-fdcf-4566-aa01-4a8c02562542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit= 0.013825460636515907\n",
      "filter shape:  (325,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' positve words'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def most_important_features(positive ,negative ,vocab, threshold=1):\n",
    "    \"\"\"\n",
    "    :param data: list of Example objects\n",
    "    :param vocab list of words\n",
    "    :param threshold: the number at which we neglegt redudant features\n",
    "    :return: (pos_freq, neg_freq, words)\n",
    "    \"\"\"\n",
    "    # converting to numpy\n",
    "    \n",
    "    pos_arr = np.empty(shape=(len(positive), len(vocab)))\n",
    "    for i in range(len(positive)):\n",
    "        pos_arr[i] = positive[i].x\n",
    "        \n",
    "    neg_arr = np.empty(shape=(len(negative), len(vocab)))\n",
    "    for i in range(len(negative)):\n",
    "        neg_arr[i] = negative[i].x\n",
    "        \n",
    "    vocab_arr = np.array(vocab)\n",
    "    \n",
    "    return pos_arr, neg_arr, vocab_arr\n",
    "\n",
    "pos, neg, v_arr = most_important_features(positive, negative, vocab)\n",
    "\n",
    "# # playing \n",
    "# small = 1e-9\n",
    "# p_m = np.mean(pos, axis=0) + small\n",
    "# n_m = np.mean(neg, axis=0) + small\n",
    "\n",
    "# out = p_m /n_m\n",
    "# f = (out > 2.4e7) \n",
    "# print('filter shape: ', f[f==True].shape)\n",
    "# print(vocab_arr[f])\n",
    "# print(p_m[v_arr=='baseball'])\n",
    "# print(n_m[v_arr=='baseball'])\n",
    "# print(out[v_arr=='baseball'])\n",
    "# plt.hist(out, bins=50)\n",
    "\n",
    "\n",
    "# playing \n",
    "small = 0.0\n",
    "p_m = np.mean(pos, axis=0) + small\n",
    "n_m = np.mean(neg, axis=0) + small\n",
    "\n",
    "out = p_m - n_m\n",
    "\n",
    "\n",
    "counts, bins = np.histogram(out, bins=100)\n",
    "limit = bins[np.argmax(counts)+2]\n",
    "print('Limit=',limit)\n",
    "\n",
    "f = (out > limit) \n",
    "f_shape = f[f==True].shape\n",
    "print('filter shape: ', f[f==True].shape)\n",
    "# print(vocab_arr[f])\n",
    "\n",
    "plt.stairs(counts, bins)\n",
    "\n",
    "\n",
    "''' positve words'''\n",
    "# # Sortting words\n",
    "# indcies = np.argsort(out)[-f_shape[0]:]\n",
    "# chosen_indcies = []\n",
    "# for i in indcies[::-1]:\n",
    "#     print(vocab_arr[i])\n",
    "#     o = input()\n",
    "#     if o == 'y':\n",
    "#         chosen_indcies.append(i)\n",
    "    \n",
    "# np.save('data/chosen_p_indcies', np.array(chosen_indcies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "783019e5-9df5-4cdf-9ea9-700cdf03428f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baseball' 'hit' 'pitching' 'hitter' 'pitcher' 'hitting' 'pitch' 'base'\n",
      " 'pitchers' 'innings' 'bat' 'hits' 'pitched' 'batting' 'inning' 'field'\n",
      " 'cubs' 'catcher' 'tedward' 'offense' 'majors' 'mss' 'rotation' 'bases'\n",
      " 'bullpen' 'starters' 'swing' 'roger' 'rickert' 'runner' 'fielding'\n",
      " 'pinch' 'plate' 'homers' 'hitters' 'catchers' 'runners' 'fls' 'caught'\n",
      " 'relief' 'homer' 'strike' 'starter' 'scott' 'bob' 'stadium' 'pennant'\n",
      " 'catch' 'infield' 'platoon' 'stance' 'erics' 'gotten' 'edge' 'threw'\n",
      " 'demers' 'peak' 'luriem' 'fierkelab' 'vesterman' 'fielder' 'hung'\n",
      " 'admiral' 'sepinwal' 'philly' 'mjones' 'minors' 'lineup' 'pm' 'closer'\n",
      " 'batter' 'fly' 'pace' 'bats' 'nimaster' 'baseman' 'lankford' 'shortstops'\n",
      " 'outfield' 'rushed' 'prime' 'humor' 'balls' 'baserunning' 'kirsch'\n",
      " 'gspira' 'jtchern' 'cmk' 'shortstop' 'racking' 'batters' 'snichols'\n",
      " 'leagues' 'clutch' 'lame' 'jay' 'pitches' 'gajarsky' 'ted' 'rogoff'\n",
      " 'jrogoff' 'defensively' 'glove' 'liked' 'opener' 'waivers' 'davewood'\n",
      " 'rp' 'umpires' 'paula' 'uucp' 'traven' 'niguma']\n",
      "(113,)\n"
     ]
    }
   ],
   "source": [
    "chosen_p_indcies = np.load('data/chosen_p_indcies.npy')\n",
    "print(vocab_arr[chosen_p_indcies])\n",
    "print(chosen_p_indcies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf237ae-f297-4fc7-8b62-626d3517e807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit= -0.024242211055276353\n",
      "filter shape:  (235,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' negative words'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative class\n",
    "\n",
    "limit = bins[np.argmax(counts)-2]\n",
    "print('Limit=',limit)\n",
    "f = (out < limit) \n",
    "f_shape = f[f==True].shape\n",
    "print('filter shape: ', f[f==True].shape)\n",
    "\n",
    "''' negative words'''\n",
    "\n",
    "# # Sortting words\n",
    "# indcies = np.argsort(out)[:f_shape[0]]\n",
    "# print(vocab_arr[indcies])\n",
    "# chosen_indcies = []\n",
    "# for i in indcies:\n",
    "#     print(vocab_arr[i])\n",
    "#     o = input()\n",
    "#     if o == 'y':\n",
    "#         chosen_indcies.append(i)\n",
    "    \n",
    "# np.save('data/chosen_n_indcies', np.array(chosen_indcies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21457855-9a8c-44b9-a789-9989d44b1633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hockey' 'period' 'goal' 'points' 'playoffs' 'puck' 'pp' 'goals' 'ice'\n",
      " 'players' 'pts' 'playoff' 'gld' 'player' 'coach' 'penalty' 'round'\n",
      " 'draft' 'captain' 'pick' 'net' 'scoring' 'shot' 'goalie' 'shots'\n",
      " 'defenseman' 'series' 'tie' 'maynard' 'golchowy' 'penalties' 'line'\n",
      " 'mask' 'franchise' 'zone' 'gballent' 'dchhabra' 'kkeller' 'arena' 'ca'\n",
      " 'breaker' 'forwards' 'stat' 'point' 'roughing' 'sh' 'goaltender' 'slot'\n",
      " 'pool' 'forward' 'standings' 'etxonss' 'wing' 'unassisted' 'defensemen'\n",
      " 'goalies' 'conference' 'rm' 'passed' 'stick' 'farenebt' 'assists'\n",
      " 'instead' 'circle' 'match' 'club' 'nne' 'boards' 'cordially' 'tough'\n",
      " 'hell' 'hammerl' 'octopus' 'rebound' 'howl' 'goaltending' 'gargle'\n",
      " 'defensive' 'souviens' 'poll' 'fmsalvat' 'rauser' 'stop' 'ref' 'pluggers'\n",
      " 'willis' 'rights']\n",
      "(87,)\n"
     ]
    }
   ],
   "source": [
    "chosen_n_indcies = np.load('data/chosen_n_indcies.npy')\n",
    "print(vocab_arr[chosen_n_indcies])\n",
    "print(chosen_n_indcies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaa36b-9bd5-4902-8661-1e68f2286a54",
   "metadata": {},
   "source": [
    "## What words are good predictor of chosen words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2a51d06-6b32-461d-8fea-e31fba11efe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9528861025854305\n",
      "0.9288154073372098\n",
      "0.9494211882807919\n",
      "0.003896945563074537\n",
      "0.8540851151032064\n"
     ]
    }
   ],
   "source": [
    "def get_selected_prediction(word, val, vocab_arr, chosen_words_indcies, weights):\n",
    "    word_index = np.where(vocab_arr == word)[0]\n",
    "    new_index = np.where(chosen_words_indcies ==word_index)[0]\n",
    "    assert len(new_index)!=0, f'\"{word}\" is not in chosen words'\n",
    "    input_vector = np.zeros_like(chosen_words_indcies)\n",
    "    input_vector[new_index] = val\n",
    "    input_vector[0] = 1\n",
    "    \n",
    "    out = sigmoid(np.dot(input_vector, weights))\n",
    "    return out\n",
    "\n",
    "chosen_p_indcies = np.load('data/chosen_p_indcies.npy')\n",
    "chosen_n_indcies = np.load('data/chosen_n_indcies.npy')\n",
    "chosen_words_indcies = np.zeros(len(chosen_p_indcies) + len(chosen_n_indcies) +1, dtype=np.int32)\n",
    "chosen_words_indcies[1:len(chosen_p_indcies) +1] = chosen_p_indcies\n",
    "chosen_words_indcies[len(chosen_p_indcies) +1:] = chosen_n_indcies\n",
    "chosen_words_indcies.sort()\n",
    "chosen_words_indcies[0] = 0 # to chose bias\n",
    "# print(chosen_words_indcies)\n",
    "\n",
    "weights = np.load('weights/selected_features_no_normalization.npy')\n",
    "\n",
    "print(get_selected_prediction('baseball', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('bases', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('hit', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('hockey', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "# print(get_selected_prediction('think', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "# print(get_selected_prediction('liked', 1, vocab_arr, chosen_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b729eaaa-1e11-42bd-8a45-1e9dfe22ffda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a bias towards p\n",
      "0.8540851151032064\n"
     ]
    }
   ],
   "source": [
    "print('There is a bias towards p')\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, chosen_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "640d7810-ac87-41d9-b181-b840ab07b434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 3 2]\n"
     ]
    }
   ],
   "source": [
    "def sort_with_keys(key_arr, val_arr):\n",
    "    '''\n",
    "    sort a val_arr using kesy_arr from ascendingly\n",
    "    :param key_arr: numpy array with keys\n",
    "    :param val_arr: numpy array to be sorted\n",
    "    :return: sorted numpy array  of val_arr\n",
    "    '''\n",
    "    index = np.lexsort((val_arr, key_arr))\n",
    "    return val_arr[index]\n",
    "out=sort_with_keys(np.array([0, 1, 2, 4]), np.array([1, 5, 3, 2]))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d51d3f6-41c7-4e9a-85be-7b85d23fb047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pitching' 'inning' 'pitcher' 'pitched' 'luriem' 'stadium' 'assists'\n",
      " 'batting' 'tedward' 'hitters' 'minors' 'bob' 'pitchers' 'rotation' 'club'\n",
      " 'baseman' 'field' 'bat' 'lame' 'glove' 'ted' 'edge' 'bullpen' 'rushed'\n",
      " 'cubs' 'sepinwal' 'majors' 'kirsch' 'strike' 'balls' 'homers' 'fielding'\n",
      " 'fierkelab' 'homer' 'jay' 'rogoff' 'innings' 'catcher' 'erics' 'mss'\n",
      " 'jtchern' 'waivers' 'philly' 'paula' 'pitch' 'leagues' 'prime' 'baseball'\n",
      " 'bats' 'scott' 'rickert' 'pitches' 'hitter' 'base' 'hit' 'roger' 'uucp'\n",
      " 'batters' 'stance' 'stick' 'shortstops' 'outfield' 'starters' 'defensive'\n",
      " 'scoring' 'demers' 'slot' 'mjones' 'pennant' 'rp' 'closer' 'infield'\n",
      " 'catch' 'bases' 'cordially' 'opener' 'hung' 'runner' 'lineup' 'fls' 'cmk'\n",
      " 'runners' 'vesterman' 'batter' 'caught' 'point' 'pm' 'lankford' 'gspira'\n",
      " 'hits' 'pinch' 'clutch' 'boards' 'admiral' 'offense' 'platoon'\n",
      " 'shortstop' 'peak' 'niguma' 'hitting' 'davewood' 'fielder' 'swing'\n",
      " 'threw' 'nimaster' 'traven' 'racking' 'gotten' 'relief' 'humor' 'shot'\n",
      " 'jrogoff' 'draft' 'starter' 'BIAS_CONSTANT' 'pp' 'breaker' 'ref'\n",
      " 'gajarsky' 'baserunning' 'defensemen' 'umpires' 'players' 'series'\n",
      " 'catchers' 'net' 'fly' 'pace' 'rauser' 'snichols' 'arena' 'plate' 'line'\n",
      " 'sh' 'defensively' 'souviens' 'instead' 'rebound' 'liked' 'rm' 'player'\n",
      " 'goaltender' 'stat' 'hell' 'goalies' 'forward' 'gargle' 'ca' 'unassisted'\n",
      " 'passed' 'rights' 'pluggers' 'goaltending' 'circle' 'gballent' 'points'\n",
      " 'tough' 'shots' 'pool' 'conference' 'howl' 'gld' 'octopus' 'fmsalvat'\n",
      " 'defenseman']\n",
      "(165,)\n",
      "\n",
      "['pts' 'hockey' 'golchowy' 'playoffs' 'playoff' 'coach' 'goals' 'goal'\n",
      " 'ice' 'stop' 'round' 'puck' 'forwards' 'willis' 'poll' 'hammerl' 'goalie'\n",
      " 'penalty' 'captain' 'period' 'franchise' 'tie' 'match' 'dchhabra' 'nne'\n",
      " 'farenebt' 'penalties' 'pick' 'maynard' 'kkeller' 'standings' 'mask'\n",
      " 'roughing' 'etxonss' 'wing' 'zone']\n",
      "(36,)\n"
     ]
    }
   ],
   "source": [
    "def get_selected_prediction_arr(val, vocab_arr, chosen_words_indcies, weights):\n",
    "    # we iclude bias as a seprate input\n",
    "    input_vector = np.diag(val * np.ones(len(chosen_words_indcies)))\n",
    "    input_vector[:, 0] = 1\n",
    "    \n",
    "    out = np.matmul(input_vector, weights.reshape([-1, 1]))\n",
    "    \n",
    "    #sigmoid\n",
    "    out = np.exp(out)\n",
    "    out = out/(1+out)\n",
    "    \n",
    "    #Sorting words\n",
    "    p_words_indcies = np.where(out>=0.5)[0]\n",
    "    p_vocab_indcies = sort_with_keys(out[p_words_indcies].reshape(-1), chosen_words_indcies[p_words_indcies])[::-1]\n",
    "    \n",
    "    n_words_indcies = np.where(out<0.5)[0]\n",
    "    n_vocab_indcies = sort_with_keys(out[n_words_indcies].reshape(-1), chosen_words_indcies[n_words_indcies])\n",
    "    return p_vocab_indcies, n_vocab_indcies\n",
    "\n",
    "p_vocab_indcies, n_vocab_indcies = get_selected_prediction_arr(1, vocab_arr, chosen_words_indcies, weights)\n",
    "\n",
    "print(vocab_arr[p_vocab_indcies])\n",
    "print(vocab_arr[p_vocab_indcies].shape)\n",
    "print()\n",
    "print(vocab_arr[n_vocab_indcies])\n",
    "print(vocab_arr[n_vocab_indcies].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366aaed-148f-4069-b5e1-3d7b411ae74b",
   "metadata": {},
   "source": [
    "## Analysing words are good predictor from all words (no selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62eca5b4-3871-4374-8c14-b2964c2ce7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9890365588519922\n",
      "0.8876874058118711\n",
      "0.9802580969303366\n",
      "0.012482282385483602\n",
      "0.8220677345416729\n"
     ]
    }
   ],
   "source": [
    "all_words_indcies = np.arange(len(vocab_arr))\n",
    "weights = np.load('weights/all_features_no_normalization.npy')\n",
    "\n",
    "print(get_selected_prediction('baseball', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('bases', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('hit', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('hockey', 1, vocab_arr, all_words_indcies, weights))\n",
    "# print(get_selected_prediction('think', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, all_words_indcies, weights))\n",
    "# print(get_selected_prediction('liked', 1, vocab_arr, all_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec65a56d-629d-42a4-8421-2092e41d5363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a bias towards p\n",
      "0.8220677345416729\n"
     ]
    }
   ],
   "source": [
    "print('There is a bias towards p')\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, all_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec736927-e101-475c-b46e-e0d977115e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runs' 'justification' 'coveted' 'band' 'nicely' 'info' 'irony' 'girls'\n",
      " 'listing' 'baseball' 'sepinwal' 'steven' 'traditional' 'valued' 'central'\n",
      " 'books' 'thats' 'basically' 'liking' 'nuts' 'interpreted' 'infamous'\n",
      " 'staion' 'evaluating' 'chip' 'settled' 'mitt' 'educate' 'likely' 'skate'\n",
      " 'deny' 'prime' 'camelot' 'helping' 'pitches' 'minimum' 'expired' 'gotten'\n",
      " 'anyone' 'performed' 'starter' 'staff' 'gem' 'subsidiary' 'pitch'\n",
      " 'equivalent' 'chuck' 'remember' 'rogoff' 'water' 'fist' 'backhanded'\n",
      " 'exceed' 'pour' 'rating' 'ignorance' 'hit' 'bar' 'delay' 'fry' 'logos'\n",
      " 'inception' 'resting' 'copy']\n",
      "(4785,)\n",
      "\n",
      "['hockey' 'pts' 'playoffs' 'next' 'biggest' 'finals' 'need' 'mmb'\n",
      " 'annoying' 'playoff' 'khettry' 'broadcast' 'vs' 'polite' 'personal'\n",
      " 'golchowy' 'lucky' 'period' 'beat' 'arrogant' 'combinations' 'dkl' 'non'\n",
      " 'coach' 'ext' 'embarrasing' 'saving' 'honoured' 'allan' 'goal' 'earning'\n",
      " 'stiff' 'wangr' 'zones' 'shorts' 'helps' 'splits' 'peoples' 'standing'\n",
      " 'could' 'tvartiai' 'leaders' 'h' 'presently' 'stamber' 'angles' 'dishes'\n",
      " 'traded' 'flat' 'slap' 'overtime' 'counts' 'dangerous' 'round' 'wish'\n",
      " 'retort' 'cheese' 'producers' 'drew' 'shown' 'likes' 'vegetarian' 'night'\n",
      " 'shoots' 'younger' 'border' 'pro' 'jake' 'fewer' 'cyberspace' 'adjusted'\n",
      " 'expense' 'screen' 'values' 'conducive' 'fault' 'phrase' 'advice'\n",
      " 'understanding' 'occasional' 'excuses' 'join' 'grew' 'write' 'flagship'\n",
      " 'crucial' 'locker' 'showing' 'cholesterol' 'immediate' 'preceding'\n",
      " 'graduated' 'duties' 'standings' 'programs' 'insgnia' 'knew' 'stripped'\n",
      " 'students' 'cents' 'pool' 'interviews' 'newfound' 'problems' 'daniell'\n",
      " 'personally' 'cool' 'swinging' 'treated' 'guys' 'credence' 'penalized'\n",
      " 'kkeller' 'blank' 'stat' 'production' 'office' 'tervo' 'shift'\n",
      " 'dispersal' 'victors' 'ca' 'fight' 'throws' 'goes' 'relieved' 'vision'\n",
      " 'sets' 'recognise' 'judge' 'tervio' 'whether' 'specifications' 'dmoney'\n",
      " 'continent' 'asked' 'speech' 'onto' 'galvint' 'mount' 'competent'\n",
      " 'penalty' 'analysis' 'blotted' 'decibel' 'represents' 'studio' 'gargle'\n",
      " 'hair' 'players' 'wing' 'lets' 'forwards' 'eb' 'ages' 'interfered' 'ten'\n",
      " 'wrote' 'died' 'modest' 'armor' 'prg' 'almost' 'stark' 'replying' 'honor'\n",
      " 'golden' 'leaves' 'increased' 'loss' 'triples' 'must' 'scalpers'\n",
      " 'maniacal' 'impressive' 'flattering' 'file' 'declined' 'fame' 'tactics'\n",
      " 'answer' 'abotu' 'tv' 'considerably' 'populated' 'coincidence'\n",
      " 'hilarious' 'bust' 'retard' 'four' 'letter' 'shooting' 'header'\n",
      " 'stretcher' 'clearing' 'define' 'points' 'lights' 'adjusting' 'uptade'\n",
      " 'minnesota' 'g' 'mask' 'preceded' 'leafs' 'champion' 'coffee' 'content'\n",
      " 'proof' 'played' 'followed' 'network' 'fashion' 'chosen' 'inexpensive'\n",
      " 'claiming' 'whole' 'pick' 'horizon' 'fmsalvat' 'physical' 'quoted'\n",
      " 'revenge' 'puts' 'appreciated' 'tee' 'intelligence' 'email' 'fee' 'test'\n",
      " 'disciplined' 'helmets' 'fewest' 'sarcasm' 'adress' 'filed' 'punches'\n",
      " 'rd' 'cartlidge' 'assists' 'carry' 'mid' 'dollar' 'guns' 'prove' 'ing'\n",
      " 'steady' 'sheet' 'disclose' 'normally' 'opposite' 'spelling' 'evening'\n",
      " 'skills' 'en' 'soured' 'fire' 'complained' 'sort' 'stomach' 'blue'\n",
      " 'pigpens' 'keys' 'indicate' 'er' 'huot' 'elementary' 'instant' 'tad'\n",
      " 'contributors' 'begin' 'parts' 'yell' 'concession' 'contending' 'full'\n",
      " 'youth' 'tools' 'assholes' 'sent' 'bratt' 'deflected' 'cleared'\n",
      " 'selection' 'everywhere' 'ques' 'possession' 'measure' 'chronological'\n",
      " 'drugs' 'equally' 'memorable' 'disappointed' 'protests' 'anyway'\n",
      " 'package' 'fair' 'honest' 'foolish' 'read' 'beautiful' 'advisedly'\n",
      " 'aggressive' 'shall' 'hzazula' 'house' 'referring' 'networks'\n",
      " 'tremendously' 'metro' 'maynard' 'dinger' 'unfair' 'sometimes' 'weather'\n",
      " 'doubter' 'slacelle' 'quite' 'gjp' 'boards' 'saddled' 'garryola' 'scorer'\n",
      " 'rocket' 'editor' 'transplanted' 'generate' 'reading' 'draw' 'delaying'\n",
      " 'stuppid' 'puck' 'copyright' 'contains' 'simulcast' 'doubts' 'watch'\n",
      " 'circle' 'passport' 'quarters' 'rubbing' 'superb' 'jerseys' 'announced'\n",
      " 'crazy' 'tommorrow' 'pops' 'shots' 'breaker' 'pete' 'translation' 'topic']\n",
      "(352,)\n"
     ]
    }
   ],
   "source": [
    "p_vocab_indcies, n_vocab_indcies = get_selected_prediction_arr(1, vocab_arr, all_words_indcies, weights)\n",
    "\n",
    "print(vocab_arr[p_vocab_indcies][:64])\n",
    "print(vocab_arr[p_vocab_indcies].shape)\n",
    "print()\n",
    "print(vocab_arr[n_vocab_indcies])\n",
    "print(vocab_arr[n_vocab_indcies].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ca310-6992-47d2-abba-6cfde0607313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
