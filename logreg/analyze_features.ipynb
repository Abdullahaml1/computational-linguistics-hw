{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c4e367-076a-4220-b9f8-5fd0ab0aca14",
   "metadata": {},
   "source": [
    "# Binary Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c69afb-1036-4df5-8172-d39852be298b",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* adjust early stoping [DONE]()\n",
    "* ploting [done]()\n",
    "* display plot on jupyter [Done]()\n",
    "* get the most test accuracy of the model [Done]()\n",
    "* adding Dateset calss[done]()\n",
    "* adding normalization [done]()\n",
    "* analyzning features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3016459-1a66-40ba-a109-22e9d05c2fcc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace1d6f-30e8-40fe-be28-9cfe749c5b76",
   "metadata": {},
   "source": [
    "### Training all features without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4551b246-56ea-420c-9f93-45488e109a25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-510.815015 Test logP=-130.574824 Train Acc=0.865602 Test Acc=0.804511 TrainLoss=0.688432 TestLoss=1.414564\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-225.509969 Test logP=-94.424239 Train Acc=0.928571 Test Acc=0.864662 TrainLoss=0.302947 TestLoss=1.023002\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-138.736219 Test logP=-82.106650 Train Acc=0.959586 Test Acc=0.879699 TrainLoss=0.187660 TestLoss=0.888054\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-79.359601 Test logP=-73.060031 Train Acc=0.972744 Test Acc=0.879699 TrainLoss=0.107605 TestLoss=0.788752\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-55.126707 Test logP=-71.334874 Train Acc=0.981203 Test Acc=0.902256 TrainLoss=0.074747 TestLoss=0.768637\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-41.297784 Test logP=-68.951384 Train Acc=0.982143 Test Acc=0.917293 TrainLoss=0.055996 TestLoss=0.736710\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-33.695668 Test logP=-67.345991 Train Acc=0.984023 Test Acc=0.909774 TrainLoss=0.045688 TestLoss=0.714542\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-28.067654 Test logP=-66.618783 Train Acc=0.986842 Test Acc=0.909774 TrainLoss=0.038057 TestLoss=0.704236\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-23.809455 Test logP=-65.859784 Train Acc=0.988722 Test Acc=0.917293 TrainLoss=0.032284 TestLoss=0.695238\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-20.441323 Test logP=-65.108232 Train Acc=0.991541 Test Acc=0.917293 TrainLoss=0.027717 TestLoss=0.687086\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-17.725018 Test logP=-64.359396 Train Acc=0.992481 Test Acc=0.917293 TrainLoss=0.024034 TestLoss=0.678963\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-15.513692 Test logP=-63.560602 Train Acc=0.996241 Test Acc=0.917293 TrainLoss=0.021035 TestLoss=0.670298\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Train logP=-13.687896 Test logP=-62.760977 Train Acc=0.996241 Test Acc=0.917293 TrainLoss=0.018560 TestLoss=0.661625\n",
      "----------------------------\n",
      "Epoch:14\n",
      "Train logP=-12.177670 Test logP=-62.057413 Train Acc=0.996241 Test Acc=0.917293 TrainLoss=0.016512 TestLoss=0.653993\n",
      "----------------------------\n",
      "Epoch:15\n",
      "Train logP=-10.939723 Test logP=-61.481514 Train Acc=0.996241 Test Acc=0.917293 TrainLoss=0.014833 TestLoss=0.647746\n",
      "----------------------------\n",
      "Epoch:16\n",
      "Train logP=-9.922611 Test logP=-61.019996 Train Acc=0.996241 Test Acc=0.917293 TrainLoss=0.013454 TestLoss=0.642740\n",
      "----------------------------\n",
      "Epoch:17\n",
      "Train logP=-9.071239 Test logP=-60.646178 Train Acc=0.997180 Test Acc=0.917293 TrainLoss=0.012300 TestLoss=0.638685\n",
      "----------------------------\n",
      "Epoch:18\n",
      "Train logP=-8.341658 Test logP=-60.335333 Train Acc=0.998120 Test Acc=0.917293 TrainLoss=0.011311 TestLoss=0.635313\n",
      "----------------------------\n",
      "Epoch:19\n",
      "Train logP=-7.703638 Test logP=-60.069465 Train Acc=0.998120 Test Acc=0.917293 TrainLoss=0.010445 TestLoss=0.632429\n",
      "----------------------------\n",
      "Epoch:20\n",
      "Train logP=-7.137426 Test logP=-59.836127 Train Acc=0.998120 Test Acc=0.917293 TrainLoss=0.009678 TestLoss=0.629898\n",
      "----------------------------\n",
      "Epoch:21\n",
      "Train logP=-6.630351 Test logP=-59.626857 Train Acc=0.998120 Test Acc=0.917293 TrainLoss=0.008990 TestLoss=0.627628\n",
      "----------------------------\n",
      "Epoch:22\n",
      "Train logP=-6.174112 Test logP=-59.436009 Train Acc=0.998120 Test Acc=0.917293 TrainLoss=0.008372 TestLoss=0.625558\n",
      "----------------------------\n",
      "Epoch:23\n",
      "Train logP=-5.762732 Test logP=-59.259822 Train Acc=0.998120 Test Acc=0.917293 TrainLoss=0.007814 TestLoss=0.623646\n",
      "----------------------------\n",
      "Epoch:24\n",
      "Train logP=-5.391204 Test logP=-59.095681 Train Acc=0.998120 Test Acc=0.917293 TrainLoss=0.007310 TestLoss=0.621866\n",
      "----------------------------\n",
      "Epoch:25\n",
      "Train logP=-5.054839 Test logP=-58.941597 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.006854 TestLoss=0.620195\n",
      "----------------------------\n",
      "Epoch:26\n",
      "Train logP=-4.749165 Test logP=-58.795897 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.006439 TestLoss=0.618614\n",
      "----------------------------\n",
      "Epoch:27\n",
      "Train logP=-4.470107 Test logP=-58.657109 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.006061 TestLoss=0.617109\n",
      "----------------------------\n",
      "Epoch:28\n",
      "Train logP=-4.214165 Test logP=-58.523936 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.005714 TestLoss=0.615664\n",
      "----------------------------\n",
      "Epoch:29\n",
      "Train logP=-3.978481 Test logP=-58.395265 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.005394 TestLoss=0.614268\n",
      "----------------------------\n",
      "Epoch:30\n",
      "Train logP=-3.760778 Test logP=-58.270156 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.005099 TestLoss=0.612911\n",
      "----------------------------\n",
      "Epoch:31\n",
      "Train logP=-3.559250 Test logP=-58.147837 Train Acc=0.999060 Test Acc=0.917293 TrainLoss=0.004826 TestLoss=0.611584\n",
      "----------------------------\n",
      "Epoch:32\n",
      "Train logP=-3.372431 Test logP=-58.027688 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.004573 TestLoss=0.610281\n",
      "----------------------------\n",
      "Epoch:33\n",
      "Train logP=-3.199100 Test logP=-57.909243 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.004338 TestLoss=0.608996\n",
      "----------------------------\n",
      "Epoch:34\n",
      "Train logP=-3.038206 Test logP=-57.792179 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.004120 TestLoss=0.607726\n",
      "----------------------------\n",
      "Epoch:35\n",
      "Train logP=-2.888827 Test logP=-57.676320 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.003917 TestLoss=0.606470\n",
      "----------------------------\n",
      "Epoch:36\n",
      "Train logP=-2.750144 Test logP=-57.561632 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.003729 TestLoss=0.605226\n",
      "----------------------------\n",
      "Epoch:37\n",
      "Train logP=-2.621427 Test logP=-57.448218 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.003554 TestLoss=0.603995\n",
      "----------------------------\n",
      "Epoch:38\n",
      "Train logP=-2.502017 Test logP=-57.336294 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.003393 TestLoss=0.602781\n",
      "----------------------------\n",
      "Epoch:39\n",
      "Train logP=-2.391310 Test logP=-57.226177 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.003242 TestLoss=0.601587\n",
      "----------------------------\n",
      "Epoch:40\n",
      "Train logP=-2.288742 Test logP=-57.118245 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.003103 TestLoss=0.600416\n",
      "----------------------------\n",
      "Epoch:41\n",
      "Train logP=-2.193772 Test logP=-57.012905 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002975 TestLoss=0.599273\n",
      "----------------------------\n",
      "Epoch:42\n",
      "Train logP=-2.105873 Test logP=-56.910558 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002855 TestLoss=0.598163\n",
      "----------------------------\n",
      "Epoch:43\n",
      "Train logP=-2.024521 Test logP=-56.811567 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002745 TestLoss=0.597089\n",
      "----------------------------\n",
      "Epoch:44\n",
      "Train logP=-1.949201 Test logP=-56.716231 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002643 TestLoss=0.596055\n",
      "----------------------------\n",
      "Epoch:45\n",
      "Train logP=-1.879411 Test logP=-56.624772 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002548 TestLoss=0.595063\n",
      "----------------------------\n",
      "Epoch:46\n",
      "Train logP=-1.814664 Test logP=-56.537331 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002461 TestLoss=0.594115\n",
      "----------------------------\n",
      "Epoch:47\n",
      "Train logP=-1.754500 Test logP=-56.453971 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002379 TestLoss=0.593210\n",
      "----------------------------\n",
      "Epoch:48\n",
      "Train logP=-1.698490 Test logP=-56.374685 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002303 TestLoss=0.592350\n",
      "----------------------------\n",
      "Epoch:49\n",
      "Train logP=-1.646237 Test logP=-56.299409 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002232 TestLoss=0.591534\n",
      "----------------------------\n",
      "Epoch:50\n",
      "Train logP=-1.597383 Test logP=-56.228037 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002166 TestLoss=0.590760\n",
      "----------------------------\n",
      "Epoch:51\n",
      "Train logP=-1.551602 Test logP=-56.160428 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002104 TestLoss=0.590026\n",
      "----------------------------\n",
      "Epoch:52\n",
      "Train logP=-1.508605 Test logP=-56.096421 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.002046 TestLoss=0.589332\n",
      "----------------------------\n",
      "Epoch:53\n",
      "Train logP=-1.468135 Test logP=-56.035843 Train Acc=1.000000 Test Acc=0.909774 TrainLoss=0.001991 TestLoss=0.588675\n",
      "----------------------------\n",
      "Epoch:54\n",
      "Train logP=-1.429961 Test logP=-55.978513 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001939 TestLoss=0.588053\n",
      "----------------------------\n",
      "Epoch:55\n",
      "Train logP=-1.393881 Test logP=-55.924250 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001890 TestLoss=0.587464\n",
      "----------------------------\n",
      "Epoch:56\n",
      "Train logP=-1.359713 Test logP=-55.872877 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001844 TestLoss=0.586907\n",
      "----------------------------\n",
      "Epoch:57\n",
      "Train logP=-1.327299 Test logP=-55.824222 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001800 TestLoss=0.586379\n",
      "----------------------------\n",
      "Epoch:58\n",
      "Train logP=-1.296496 Test logP=-55.778120 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001758 TestLoss=0.585879\n",
      "----------------------------\n",
      "Epoch:59\n",
      "Train logP=-1.267177 Test logP=-55.734414 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001718 TestLoss=0.585405\n",
      "----------------------------\n",
      "Epoch:60\n",
      "Train logP=-1.239229 Test logP=-55.692957 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001680 TestLoss=0.584955\n",
      "----------------------------\n",
      "Epoch:61\n",
      "Train logP=-1.212550 Test logP=-55.653610 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001644 TestLoss=0.584529\n",
      "----------------------------\n",
      "Epoch:62\n",
      "Train logP=-1.187050 Test logP=-55.616243 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001610 TestLoss=0.584123\n",
      "----------------------------\n",
      "Epoch:63\n",
      "Train logP=-1.162645 Test logP=-55.580737 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001576 TestLoss=0.583738\n",
      "----------------------------\n",
      "Epoch:64\n",
      "Train logP=-1.139263 Test logP=-55.546978 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001545 TestLoss=0.583372\n",
      "----------------------------\n",
      "Epoch:65\n",
      "Train logP=-1.116836 Test logP=-55.514862 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001514 TestLoss=0.583024\n",
      "----------------------------\n",
      "Epoch:66\n",
      "Train logP=-1.095302 Test logP=-55.484293 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001485 TestLoss=0.582692\n",
      "----------------------------\n",
      "Epoch:67\n",
      "Train logP=-1.074606 Test logP=-55.455179 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001457 TestLoss=0.582376\n",
      "----------------------------\n",
      "Epoch:68\n",
      "Train logP=-1.054697 Test logP=-55.427439 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001430 TestLoss=0.582075\n",
      "----------------------------\n",
      "Epoch:69\n",
      "Train logP=-1.035528 Test logP=-55.400994 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001404 TestLoss=0.581788\n",
      "----------------------------\n",
      "Epoch:70\n",
      "Train logP=-1.017057 Test logP=-55.375774 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001379 TestLoss=0.581515\n",
      "----------------------------\n",
      "Epoch:71\n",
      "Train logP=-0.999245 Test logP=-55.351710 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001355 TestLoss=0.581254\n",
      "----------------------------\n",
      "Epoch:72\n",
      "Train logP=-0.982054 Test logP=-55.328742 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001332 TestLoss=0.581005\n",
      "----------------------------\n",
      "Epoch:73\n",
      "Train logP=-0.965452 Test logP=-55.306812 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001309 TestLoss=0.580767\n",
      "----------------------------\n",
      "Epoch:74\n",
      "Train logP=-0.949408 Test logP=-55.285866 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001287 TestLoss=0.580540\n",
      "----------------------------\n",
      "Epoch:75\n",
      "Train logP=-0.933892 Test logP=-55.265854 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001266 TestLoss=0.580323\n",
      "----------------------------\n",
      "Epoch:76\n",
      "Train logP=-0.918878 Test logP=-55.246730 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001246 TestLoss=0.580115\n",
      "----------------------------\n",
      "Epoch:77\n",
      "Train logP=-0.904341 Test logP=-55.228451 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001226 TestLoss=0.579917\n",
      "----------------------------\n",
      "Epoch:78\n",
      "Train logP=-0.890257 Test logP=-55.210977 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001207 TestLoss=0.579727\n",
      "----------------------------\n",
      "Epoch:79\n",
      "Train logP=-0.876606 Test logP=-55.194268 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001189 TestLoss=0.579546\n",
      "----------------------------\n",
      "Epoch:80\n",
      "Train logP=-0.863367 Test logP=-55.178291 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001171 TestLoss=0.579373\n",
      "----------------------------\n",
      "Epoch:81\n",
      "Train logP=-0.850520 Test logP=-55.163011 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001153 TestLoss=0.579207\n",
      "----------------------------\n",
      "Epoch:82\n",
      "Train logP=-0.838049 Test logP=-55.148398 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001136 TestLoss=0.579048\n",
      "----------------------------\n",
      "Epoch:83\n",
      "Train logP=-0.825936 Test logP=-55.134422 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001120 TestLoss=0.578897\n",
      "----------------------------\n",
      "Epoch:84\n",
      "Train logP=-0.814166 Test logP=-55.121056 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001104 TestLoss=0.578752\n",
      "----------------------------\n",
      "Epoch:85\n",
      "Train logP=-0.802724 Test logP=-55.108274 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001088 TestLoss=0.578613\n",
      "----------------------------\n",
      "Epoch:86\n",
      "Train logP=-0.791597 Test logP=-55.096052 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001073 TestLoss=0.578481\n",
      "----------------------------\n",
      "Epoch:87\n",
      "Train logP=-0.780770 Test logP=-55.084366 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001059 TestLoss=0.578354\n",
      "----------------------------\n",
      "Epoch:88\n",
      "Train logP=-0.770233 Test logP=-55.073195 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001044 TestLoss=0.578233\n",
      "----------------------------\n",
      "Epoch:89\n",
      "Train logP=-0.759972 Test logP=-55.062519 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001030 TestLoss=0.578117\n",
      "----------------------------\n",
      "Epoch:90\n",
      "Train logP=-0.749978 Test logP=-55.052318 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001017 TestLoss=0.578006\n",
      "----------------------------\n",
      "Epoch:91\n",
      "Train logP=-0.740239 Test logP=-55.042574 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.001004 TestLoss=0.577901\n",
      "----------------------------\n",
      "Epoch:92\n",
      "Train logP=-0.730746 Test logP=-55.033269 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.000991 TestLoss=0.577800\n",
      "----------------------------\n",
      "Epoch:93\n",
      "Train logP=-0.721490 Test logP=-55.024387 Train Acc=1.000000 Test Acc=0.917293 TrainLoss=0.000978 TestLoss=0.577703\n",
      "----------------------------\n",
      "Epoch:94\n",
      "Train logP=-0.712461 Test logP=-55.015912 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000966 TestLoss=0.577611\n",
      "----------------------------\n",
      "Epoch:95\n",
      "Train logP=-0.703651 Test logP=-55.007830 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000954 TestLoss=0.577524\n",
      "----------------------------\n",
      "Epoch:96\n",
      "Train logP=-0.695052 Test logP=-55.000126 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000942 TestLoss=0.577440\n",
      "----------------------------\n",
      "Epoch:97\n",
      "Train logP=-0.686657 Test logP=-54.992786 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000931 TestLoss=0.577360\n",
      "----------------------------\n",
      "Epoch:98\n",
      "Train logP=-0.678458 Test logP=-54.985799 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000920 TestLoss=0.577285\n",
      "----------------------------\n",
      "Epoch:99\n",
      "Train logP=-0.670449 Test logP=-54.979152 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000909 TestLoss=0.577213\n",
      "----------------------------\n",
      "Epoch:100\n",
      "Train logP=-0.662622 Test logP=-54.972833 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000898 TestLoss=0.577144\n",
      "----------------------------\n",
      "Epoch:101\n",
      "Train logP=-0.654972 Test logP=-54.966831 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000888 TestLoss=0.577079\n",
      "----------------------------\n",
      "Epoch:102\n",
      "Train logP=-0.647493 Test logP=-54.961135 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000878 TestLoss=0.577017\n",
      "----------------------------\n",
      "Epoch:103\n",
      "Train logP=-0.640178 Test logP=-54.955736 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000868 TestLoss=0.576959\n",
      "----------------------------\n",
      "Epoch:104\n",
      "Train logP=-0.633022 Test logP=-54.950624 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000858 TestLoss=0.576903\n",
      "----------------------------\n",
      "Epoch:105\n",
      "Train logP=-0.626021 Test logP=-54.945789 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000849 TestLoss=0.576851\n",
      "----------------------------\n",
      "Epoch:106\n",
      "Train logP=-0.619169 Test logP=-54.941222 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000840 TestLoss=0.576801\n",
      "----------------------------\n",
      "Epoch:107\n",
      "Train logP=-0.612462 Test logP=-54.936916 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000830 TestLoss=0.576754\n",
      "----------------------------\n",
      "Epoch:108\n",
      "Train logP=-0.605894 Test logP=-54.932862 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000822 TestLoss=0.576710\n",
      "----------------------------\n",
      "Epoch:109\n",
      "Train logP=-0.599462 Test logP=-54.929052 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000813 TestLoss=0.576669\n",
      "----------------------------\n",
      "Epoch:110\n",
      "Train logP=-0.593161 Test logP=-54.925479 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000804 TestLoss=0.576630\n",
      "----------------------------\n",
      "Epoch:111\n",
      "Train logP=-0.586987 Test logP=-54.922135 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000796 TestLoss=0.576594\n",
      "----------------------------\n",
      "Epoch:112\n",
      "Train logP=-0.580937 Test logP=-54.919014 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000788 TestLoss=0.576560\n",
      "----------------------------\n",
      "Epoch:113\n",
      "Train logP=-0.575007 Test logP=-54.916108 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000780 TestLoss=0.576529\n",
      "----------------------------\n",
      "Epoch:114\n",
      "Train logP=-0.569193 Test logP=-54.913413 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000772 TestLoss=0.576499\n",
      "----------------------------\n",
      "Epoch:115\n",
      "Train logP=-0.563492 Test logP=-54.910921 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000764 TestLoss=0.576472\n",
      "----------------------------\n",
      "Epoch:116\n",
      "Train logP=-0.557900 Test logP=-54.908626 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000756 TestLoss=0.576447\n",
      "----------------------------\n",
      "Epoch:117\n",
      "Train logP=-0.552415 Test logP=-54.906524 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000749 TestLoss=0.576425\n",
      "----------------------------\n",
      "Epoch:118\n",
      "Train logP=-0.547033 Test logP=-54.904608 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000742 TestLoss=0.576404\n",
      "----------------------------\n",
      "Epoch:119\n",
      "Train logP=-0.541752 Test logP=-54.902873 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000735 TestLoss=0.576385\n",
      "----------------------------\n",
      "Epoch:120\n",
      "Train logP=-0.536568 Test logP=-54.901315 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000728 TestLoss=0.576368\n",
      "----------------------------\n",
      "Epoch:121\n",
      "Train logP=-0.531479 Test logP=-54.899927 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000721 TestLoss=0.576353\n",
      "----------------------------\n",
      "Epoch:122\n",
      "Train logP=-0.526483 Test logP=-54.898707 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000714 TestLoss=0.576340\n",
      "----------------------------\n",
      "Epoch:123\n",
      "Train logP=-0.521576 Test logP=-54.897648 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000707 TestLoss=0.576328\n",
      "----------------------------\n",
      "Epoch:124\n",
      "Train logP=-0.516757 Test logP=-54.896747 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000701 TestLoss=0.576319\n",
      "----------------------------\n",
      "Epoch:125\n",
      "Train logP=-0.512024 Test logP=-54.896000 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000694 TestLoss=0.576311\n",
      "----------------------------\n",
      "Epoch:126\n",
      "Train logP=-0.507373 Test logP=-54.895402 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000688 TestLoss=0.576304\n",
      "----------------------------\n",
      "Epoch:127\n",
      "Train logP=-0.502802 Test logP=-54.894949 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000682 TestLoss=0.576299\n",
      "----------------------------\n",
      "Epoch:128\n",
      "Train logP=-0.498311 Test logP=-54.894638 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000676 TestLoss=0.576296\n",
      "----------------------------\n",
      "Epoch:129\n",
      "Train logP=-0.493896 Test logP=-54.894465 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000670 TestLoss=0.576294\n",
      "----------------------------\n",
      "Epoch:130\n",
      "Train logP=-0.489556 Test logP=-54.894426 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000664 TestLoss=0.576293\n",
      "----------------------------\n",
      "Epoch:131\n",
      "Early Stop\n",
      "Train logP=-0.485288 Test logP=-54.894518 Train Acc=1.000000 Test Acc=0.932331 TrainLoss=0.000658 TestLoss=0.576294\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=2000  --step=0.08 --early_stop=1 --normalize=no \\\n",
    "    --log=no --log_step=200 --plot_name=all_features_no_normalization \\\n",
    "    --save_weights_path=weights/all_features_no_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52930e-bca4-492c-b77a-5fabc710f41d",
   "metadata": {},
   "source": [
    "![plot](figures/all_features_no_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5eb9f-3d00-422e-88dc-ff79dbecca28",
   "metadata": {},
   "source": [
    "### Training All features with Noramlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1795b41e-246a-4d2d-84e1-2cb89730a030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-1707.100097 Test logP=-367.528166 Train Acc=0.890977 Test Acc=0.774436 TrainLoss=2.162091 TestLoss=3.738816\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-600.374470 Test logP=-281.271077 Train Acc=0.955827 Test Acc=0.834586 TrainLoss=0.766395 TestLoss=2.840102\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-302.900195 Test logP=-322.050256 Train Acc=0.983083 Test Acc=0.842105 TrainLoss=0.381888 TestLoss=3.289216\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-166.698603 Test logP=-325.055732 Train Acc=0.989662 Test Acc=0.849624 TrainLoss=0.211634 TestLoss=3.302329\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Early Stop\n",
      "Train logP=-33.699494 Test logP=-334.076131 Train Acc=0.996241 Test Acc=0.849624 TrainLoss=0.043298 TestLoss=3.394144\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=100  --step=0.08 --early_stop=3 --normalize=yes \\\n",
    "    --log=no --log_step=200 --plot_name=all_features_with_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d4306-094d-4022-97bd-6438ef923a5b",
   "metadata": {},
   "source": [
    "![all_normalized](figures/all_features_with_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74b8f0-cd6e-4884-b465-4087a682c6b0",
   "metadata": {},
   "source": [
    "## Traing selected features without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888d5e74-fa5a-42c5-b205-16e373915c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-313.491317 Test logP=-35.737710 Train Acc=0.852444 Test Acc=0.879699 TrainLoss=0.425068 TestLoss=0.387659\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-259.900902 Test logP=-25.323137 Train Acc=0.890038 Test Acc=0.902256 TrainLoss=0.352404 TestLoss=0.274688\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-237.708311 Test logP=-21.383661 Train Acc=0.900376 Test Acc=0.909774 TrainLoss=0.322313 TestLoss=0.231956\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-223.107328 Test logP=-19.355256 Train Acc=0.907895 Test Acc=0.924812 TrainLoss=0.302515 TestLoss=0.209953\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-212.860730 Test logP=-18.189471 Train Acc=0.911654 Test Acc=0.932331 TrainLoss=0.288621 TestLoss=0.197307\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-206.514877 Test logP=-17.594005 Train Acc=0.912594 Test Acc=0.939850 TrainLoss=0.280017 TestLoss=0.190848\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-202.384490 Test logP=-17.302063 Train Acc=0.913534 Test Acc=0.939850 TrainLoss=0.274416 TestLoss=0.187681\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-199.207291 Test logP=-17.136491 Train Acc=0.918233 Test Acc=0.954887 TrainLoss=0.270108 TestLoss=0.185885\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-196.548713 Test logP=-17.024807 Train Acc=0.919173 Test Acc=0.954887 TrainLoss=0.266504 TestLoss=0.184674\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-194.271064 Test logP=-16.944644 Train Acc=0.919173 Test Acc=0.954887 TrainLoss=0.263415 TestLoss=0.183804\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-192.295282 Test logP=-16.888565 Train Acc=0.920113 Test Acc=0.954887 TrainLoss=0.260736 TestLoss=0.183196\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-190.553918 Test logP=-16.851684 Train Acc=0.920113 Test Acc=0.954887 TrainLoss=0.258375 TestLoss=0.182796\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Train logP=-188.995904 Test logP=-16.830128 Train Acc=0.922932 Test Acc=0.954887 TrainLoss=0.256263 TestLoss=0.182562\n",
      "----------------------------\n",
      "Epoch:14\n",
      "Train logP=-187.585303 Test logP=-16.820810 Train Acc=0.922932 Test Acc=0.954887 TrainLoss=0.254350 TestLoss=0.182461\n",
      "----------------------------\n",
      "Epoch:15\n",
      "Train logP=-186.297079 Test logP=-16.821222 Train Acc=0.921992 Test Acc=0.954887 TrainLoss=0.252603 TestLoss=0.182465\n",
      "----------------------------\n",
      "Epoch:16\n",
      "Early Stop\n",
      "Train logP=-185.113502 Test logP=-16.829278 Train Acc=0.922932 Test Acc=0.954887 TrainLoss=0.250998 TestLoss=0.182553\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=200  --step=.08 --early_stop=2 --normalize=no \\\n",
    "    --log=no --log_step=50 --plot_name=selected_features_no_normalization \\\n",
    "    --save_weights_path='weights/selected_features_no_normalization' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e58cd8-945a-4bd7-b7d8-8eb2d61e5fee",
   "metadata": {},
   "source": [
    "![selected_no_normalization](figures/selected_features_no_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0638cbc-3da5-4d6d-a101-4a99cd6e82ad",
   "metadata": {},
   "source": [
    "## Trainign Selected Features with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "456b1814-b331-4359-a504-da6b4458c3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-239.282094 Test logP=-39.177141 Train Acc=0.899436 Test Acc=0.894737 TrainLoss=0.322045 TestLoss=0.424967\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-202.802205 Test logP=-35.120921 Train Acc=0.916353 Test Acc=0.917293 TrainLoss=0.274982 TestLoss=0.380968\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-190.126872 Test logP=-35.962471 Train Acc=0.923872 Test Acc=0.924812 TrainLoss=0.257796 TestLoss=0.390097\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-183.990358 Test logP=-36.126000 Train Acc=0.924812 Test Acc=0.924812 TrainLoss=0.249476 TestLoss=0.391871\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-182.039998 Test logP=-35.351964 Train Acc=0.924812 Test Acc=0.924812 TrainLoss=0.246831 TestLoss=0.383474\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-179.163315 Test logP=-36.372584 Train Acc=0.926692 Test Acc=0.924812 TrainLoss=0.242930 TestLoss=0.394545\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-177.652030 Test logP=-37.417812 Train Acc=0.928571 Test Acc=0.924812 TrainLoss=0.240881 TestLoss=0.405883\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-176.869039 Test logP=-36.695280 Train Acc=0.928571 Test Acc=0.924812 TrainLoss=0.239820 TestLoss=0.398045\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-176.539118 Test logP=-36.460410 Train Acc=0.928571 Test Acc=0.932331 TrainLoss=0.239372 TestLoss=0.395498\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-176.137511 Test logP=-36.866098 Train Acc=0.928571 Test Acc=0.939850 TrainLoss=0.238828 TestLoss=0.399898\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-176.836805 Test logP=-36.627458 Train Acc=0.928571 Test Acc=0.939850 TrainLoss=0.239776 TestLoss=0.397309\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-176.357342 Test logP=-36.752960 Train Acc=0.930451 Test Acc=0.939850 TrainLoss=0.239126 TestLoss=0.398671\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Train logP=-176.801171 Test logP=-36.796952 Train Acc=0.930451 Test Acc=0.939850 TrainLoss=0.239728 TestLoss=0.399148\n",
      "----------------------------\n",
      "Epoch:14\n",
      "Train logP=-177.446308 Test logP=-36.320462 Train Acc=0.932331 Test Acc=0.939850 TrainLoss=0.240602 TestLoss=0.393979\n",
      "----------------------------\n",
      "Epoch:15\n",
      "Train logP=-177.000934 Test logP=-36.527969 Train Acc=0.932331 Test Acc=0.939850 TrainLoss=0.239998 TestLoss=0.396230\n",
      "----------------------------\n",
      "Epoch:16\n",
      "Train logP=-177.381978 Test logP=-36.650018 Train Acc=0.932331 Test Acc=0.939850 TrainLoss=0.240515 TestLoss=0.397554\n",
      "----------------------------\n",
      "Epoch:17\n",
      "Train logP=-177.776676 Test logP=-36.399327 Train Acc=0.932331 Test Acc=0.939850 TrainLoss=0.241050 TestLoss=0.394834\n",
      "----------------------------\n",
      "Epoch:18\n",
      "Early Stop\n",
      "Train logP=-177.732818 Test logP=-36.584476 Train Acc=0.932331 Test Acc=0.939850 TrainLoss=0.240991 TestLoss=0.396842\n",
      "Figure(1000x400)\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=200  --step=.08 --early_stop=10 --normalize=yes \\\n",
    "    --log=no --log_step=200 --plot_name='selected_features_with_normalization' \\\n",
    "    --save_weights_path='weights/selected_features_with_normalization' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a0ac7-f352-4b41-ac65-718cfe30fe9f",
   "metadata": {},
   "source": [
    "![slected with normalization](figures/selected_features_with_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac82e47-aad2-47f6-8f1c-b0f8dbc4f985",
   "metadata": {},
   "source": [
    "## Analyzing uingram Features of: logstic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6ccde49-bd90-4c04-a2b2-05bfc6a7b7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from logreg import ExamplesDataset, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b07e3e4c-328f-4a4f-9823-36d4f2fac687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   0.,   0.,   1.,   1.,   0.,   0.,   1.,   0.,   1.,\n",
       "          1.,   0.,   5.,   4.,   1.,   6.,  16.,  21.,  21.,  43.,  74.,\n",
       "        100., 129., 166., 232., 284., 350., 422., 411., 434., 439., 427.,\n",
       "        375., 326., 237., 174., 146., 107.,  81.,  43.,  28.,   7.,  10.,\n",
       "          5.,   3.,   1.,   0.,   2.,   1.]),\n",
       " array([-6.97955785, -6.75132384, -6.52308983, -6.29485582, -6.06662182,\n",
       "        -5.83838781, -5.6101538 , -5.38191979, -5.15368579, -4.92545178,\n",
       "        -4.69721777, -4.46898376, -4.24074976, -4.01251575, -3.78428174,\n",
       "        -3.55604774, -3.32781373, -3.09957972, -2.87134571, -2.64311171,\n",
       "        -2.4148777 , -2.18664369, -1.95840968, -1.73017568, -1.50194167,\n",
       "        -1.27370766, -1.04547365, -0.81723965, -0.58900564, -0.36077163,\n",
       "        -0.13253762,  0.09569638,  0.32393039,  0.5521644 ,  0.78039841,\n",
       "         1.00863241,  1.23686642,  1.46510043,  1.69333444,  1.92156844,\n",
       "         2.14980245,  2.37803646,  2.60627047,  2.83450447,  3.06273848,\n",
       "         3.29097249,  3.5192065 ,  3.7474405 ,  3.97567451,  4.20390852,\n",
       "         4.43214252]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# playing with weights\n",
    "# index0-> bias\n",
    "weights = np.load('weights/all_features_no_normalization.npy')\n",
    "plt.hist(weights, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "395028b5-1c88-4ff9-af48-9de0b6ceb298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Reading dataset'''\n",
    "dataset = ExamplesDataset('data/positive', 'data/negative', 'data/vocab')\n",
    "positive, negative, vocab = dataset.get_positive_negative_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f60478f3-d61e-4c55-89fa-a17997ecbca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:  (5137,)\n",
      "filter shape (21,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['BIAS_CONSTANT', 'hockey', 'next', 'runs', 'playoffs', 'need',\n",
       "       'beat', 'pitching', 'playoff', 'ice', 'coach', 'golchowy',\n",
       "       'finals', 'kkeller', 'leaders', 'woofing', 'wangr', 'bag', 'mmb',\n",
       "       'acquisitions', 'khettry'], dtype='<U17')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_arr = np.array(vocab)\n",
    "w = 3.5\n",
    "print('original shape: ', vocab_arr.shape)\n",
    "f = (weights<-w) | ( weights>w)\n",
    "f[0] = True\n",
    "print('filter shape', f[f==True].shape)\n",
    "vocab_arr[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "53a60c7e-fdcf-4566-aa01-4a8c02562542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit= 0.013825460636515907\n",
      "filter shape:  (325,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' positve words'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def most_important_features(positive ,negative ,vocab, threshold=1):\n",
    "    \"\"\"\n",
    "    :param data: list of Example objects\n",
    "    :param vocab list of words\n",
    "    :param threshold: the number at which we neglegt redudant features\n",
    "    :return: (pos_freq, neg_freq, words)\n",
    "    \"\"\"\n",
    "    # converting to numpy\n",
    "    \n",
    "    pos_arr = np.empty(shape=(len(positive), len(vocab)))\n",
    "    for i in range(len(positive)):\n",
    "        pos_arr[i] = positive[i].x\n",
    "        \n",
    "    neg_arr = np.empty(shape=(len(negative), len(vocab)))\n",
    "    for i in range(len(negative)):\n",
    "        neg_arr[i] = negative[i].x\n",
    "        \n",
    "    vocab_arr = np.array(vocab)\n",
    "    \n",
    "    return pos_arr, neg_arr, vocab_arr\n",
    "\n",
    "pos, neg, v_arr = most_important_features(positive, negative, vocab)\n",
    "\n",
    "# # playing \n",
    "# small = 1e-9\n",
    "# p_m = np.mean(pos, axis=0) + small\n",
    "# n_m = np.mean(neg, axis=0) + small\n",
    "\n",
    "# out = p_m /n_m\n",
    "# f = (out > 2.4e7) \n",
    "# print('filter shape: ', f[f==True].shape)\n",
    "# print(vocab_arr[f])\n",
    "# print(p_m[v_arr=='baseball'])\n",
    "# print(n_m[v_arr=='baseball'])\n",
    "# print(out[v_arr=='baseball'])\n",
    "# plt.hist(out, bins=50)\n",
    "\n",
    "\n",
    "# playing \n",
    "small = 0.0\n",
    "p_m = np.mean(pos, axis=0) + small\n",
    "n_m = np.mean(neg, axis=0) + small\n",
    "\n",
    "out = p_m - n_m\n",
    "\n",
    "\n",
    "counts, bins = np.histogram(out, bins=100)\n",
    "limit = bins[np.argmax(counts)+2]\n",
    "print('Limit=',limit)\n",
    "\n",
    "f = (out > limit) \n",
    "f_shape = f[f==True].shape\n",
    "print('filter shape: ', f[f==True].shape)\n",
    "# print(vocab_arr[f])\n",
    "\n",
    "plt.stairs(counts, bins)\n",
    "\n",
    "\n",
    "''' positve words'''\n",
    "# # Sortting words\n",
    "# indcies = np.argsort(out)[-f_shape[0]:]\n",
    "# chosen_indcies = []\n",
    "# for i in indcies[::-1]:\n",
    "#     print(vocab_arr[i])\n",
    "#     o = input()\n",
    "#     if o == 'y':\n",
    "#         chosen_indcies.append(i)\n",
    "    \n",
    "# np.save('data/chosen_p_indcies', np.array(chosen_indcies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "783019e5-9df5-4cdf-9ea9-700cdf03428f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  29   76  116  268  278  260  298  269  344  476  381  276  423  347\n",
      "  450  350 1379  640  367  205  515  542  411  467  535  612  808  799\n",
      " 1452  800 1085  794  658  563  650 1067  688 2206  586  762  784  691\n",
      "  732 1458  637  806  625  702  786 1200 3556 1693  406  836  693 1873\n",
      " 1581  848 1270  982 1395 1177 1487  917  966 2271  479  453 1990  705\n",
      " 1060 1171 1298 1059  962 1143 1560 1115 1297 1778  626  897 1058 2116\n",
      " 1731 1091 1554 1070 1114 1026 1242  690  901 1153 1095 1093 1300 1172\n",
      " 1222 1212 1188  560  842  754 1195 1480 2159 4779 2391 1985 1477 2067\n",
      " 1574]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['baseball', 'hit', 'pitching', 'hitter', 'pitcher', 'hitting',\n",
       "       'pitch', 'base', 'pitchers', 'innings', 'bat', 'hits', 'pitched',\n",
       "       'batting', 'inning', 'field', 'cubs', 'catcher', 'tedward',\n",
       "       'offense', 'majors', 'mss', 'rotation', 'bases', 'bullpen',\n",
       "       'starters', 'swing', 'roger', 'rickert', 'runner', 'fielding',\n",
       "       'pinch', 'plate', 'homers', 'hitters', 'catchers', 'runners',\n",
       "       'fls', 'caught', 'relief', 'homer', 'strike', 'starter', 'scott',\n",
       "       'bob', 'stadium', 'pennant', 'catch', 'infield', 'platoon',\n",
       "       'stance', 'erics', 'gotten', 'edge', 'threw', 'demers', 'peak',\n",
       "       'luriem', 'fierkelab', 'vesterman', 'fielder', 'hung', 'admiral',\n",
       "       'sepinwal', 'philly', 'mjones', 'minors', 'lineup', 'pm', 'closer',\n",
       "       'batter', 'fly', 'pace', 'bats', 'nimaster', 'baseman', 'lankford',\n",
       "       'shortstops', 'outfield', 'rushed', 'prime', 'humor', 'balls',\n",
       "       'baserunning', 'kirsch', 'gspira', 'jtchern', 'cmk', 'shortstop',\n",
       "       'racking', 'batters', 'snichols', 'leagues', 'clutch', 'lame',\n",
       "       'jay', 'pitches', 'gajarsky', 'ted', 'rogoff', 'jrogoff',\n",
       "       'defensively', 'glove', 'liked', 'opener', 'waivers', 'davewood',\n",
       "       'rp', 'umpires', 'paula', 'uucp', 'traven', 'niguma'], dtype='<U17')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_p_indcies = np.load('data/chosen_p_indcies.npy')\n",
    "print(chosen_p_indcies)\n",
    "print()\n",
    "vocab_arr[chosen_p_indcies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cdf237ae-f297-4fc7-8b62-626d3517e807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit= -0.024242211055276353\n",
      "filter shape:  (235,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' negative words'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative class\n",
    "\n",
    "limit = bins[np.argmax(counts)-2]\n",
    "print('Limit=',limit)\n",
    "f = (out < limit) \n",
    "f_shape = f[f==True].shape\n",
    "print('filter shape: ', f[f==True].shape)\n",
    "\n",
    "''' negative words'''\n",
    "\n",
    "# # Sortting words\n",
    "# indcies = np.argsort(out)[:f_shape[0]]\n",
    "# print(vocab_arr[indcies])\n",
    "# chosen_indcies = []\n",
    "# for i in indcies:\n",
    "#     print(vocab_arr[i])\n",
    "#     o = input()\n",
    "#     if o == 'y':\n",
    "#         chosen_indcies.append(i)\n",
    "    \n",
    "# np.save('data/chosen_n_indcies', np.array(chosen_indcies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "21457855-9a8c-44b9-a789-9989d44b1633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hockey', 'period', 'goal', 'points', 'playoffs', 'puck', 'pp',\n",
       "       'goals', 'ice', 'players', 'pts', 'playoff', 'gld', 'player',\n",
       "       'coach', 'penalty', 'round', 'draft', 'captain', 'pick', 'net',\n",
       "       'scoring', 'shot', 'goalie', 'shots', 'defenseman', 'series',\n",
       "       'tie', 'maynard', 'golchowy', 'penalties', 'line', 'mask',\n",
       "       'franchise', 'zone', 'gballent', 'dchhabra', 'kkeller', 'arena',\n",
       "       'ca', 'breaker', 'forwards', 'stat', 'point', 'roughing', 'sh',\n",
       "       'goaltender', 'slot', 'pool', 'forward', 'standings', 'etxonss',\n",
       "       'wing', 'unassisted', 'defensemen', 'goalies', 'conference', 'rm',\n",
       "       'passed', 'stick', 'farenebt', 'assists', 'instead', 'circle',\n",
       "       'match', 'club', 'nne', 'boards', 'cordially', 'tough', 'hell',\n",
       "       'hammerl', 'octopus', 'rebound', 'howl', 'goaltending', 'gargle',\n",
       "       'defensive', 'souviens', 'poll', 'fmsalvat', 'rauser', 'stop',\n",
       "       'ref', 'pluggers', 'willis', 'rights'], dtype='<U17')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_n_indcies = np.load('data/chosen_n_indcies.npy')\n",
    "vocab_arr[chosen_n_indcies]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaa36b-9bd5-4902-8661-1e68f2286a54",
   "metadata": {},
   "source": [
    "## What words are good predictor of chosen words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c2a51d06-6b32-461d-8fea-e31fba11efe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9497710196650777\n",
      "0.9503553226993848\n",
      "0.9560067301658831\n",
      "0.003268102474681003\n",
      "0.8491074625725104\n",
      "0.8856054732683343\n"
     ]
    }
   ],
   "source": [
    "def get_selected_prediction(word, val, vocab_arr, chosen_words_indcies, weights):\n",
    "    word_index = np.where(vocab_arr == word)[0]\n",
    "    new_index = np.where(chosen_words_indcies ==word_index)[0]\n",
    "    assert len(new_index)!=0, f'\"{word}\" is not in chosen words'\n",
    "    input_vector = np.zeros_like(chosen_words_indcies)\n",
    "    input_vector[new_index] = val\n",
    "    input_vector[0] = 1\n",
    "    \n",
    "    out = sigmoid(np.dot(input_vector, weights))\n",
    "    return out\n",
    "\n",
    "chosen_p_indcies = np.load('data/chosen_p_indcies.npy')\n",
    "chosen_n_indcies = np.load('data/chosen_n_indcies.npy')\n",
    "chosen_words_indcies = np.zeros(len(chosen_p_indcies) + len(chosen_n_indcies) +1, dtype=np.int32)\n",
    "chosen_words_indcies[1:len(chosen_p_indcies) +1] = chosen_p_indcies\n",
    "chosen_words_indcies[len(chosen_p_indcies) +1:] = chosen_n_indcies\n",
    "chosen_words_indcies.sort()\n",
    "chosen_words_indcies[0] = 0 # to chose bias\n",
    "# print(chosen_words_indcies)\n",
    "\n",
    "weights = np.load('weights/selected_features_no_normalization.npy')\n",
    "print(get_selected_prediction('baseball', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('bases', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('hit', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('hockey', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "# print(get_selected_prediction('think', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('liked', 1, vocab_arr, chosen_words_indcies, weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b729eaaa-1e11-42bd-8a45-1e9dfe22ffda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a bias towards p\n",
      "0.8491074625725104\n"
     ]
    }
   ],
   "source": [
    "print('There is a bias towards p')\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, chosen_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "640d7810-ac87-41d9-b181-b840ab07b434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 3 2]\n"
     ]
    }
   ],
   "source": [
    "def sort_with_keys(key_arr, val_arr):\n",
    "    '''\n",
    "    sort a val_arr using kesy_arr from ascendingly\n",
    "    :param key_arr: numpy array with keys\n",
    "    :param val_arr: numpy array to be sorted\n",
    "    :return: sorted numpy array  of val_arr\n",
    "    '''\n",
    "    index = np.lexsort((val_arr, key_arr))\n",
    "    return val_arr[index]\n",
    "out=sort_with_keys(np.array([0, 1, 2, 4]), np.array([1, 5, 3, 2]))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8d51d3f6-41c7-4e9a-85be-7b85d23fb047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pitching' 'luriem' 'stadium' 'pitcher' 'sepinwal' 'rogoff' 'bullpen'\n",
      " 'innings' 'fielder' 'bob' 'catcher' 'pluggers' 'gotten' 'caught' 'philly'\n",
      " 'tedward' 'jtchern' 'edge' 'pitchers' 'batting' 'niguma' 'base' 'mss'\n",
      " 'cubs' 'majors' 'strike' 'field' 'minors' 'inning' 'ted' 'bat' 'pitched'\n",
      " 'balls' 'homer' 'cmk' 'ref' 'hitting' 'batter' 'pennant' 'club'\n",
      " 'outfield' 'uucp' 'rushed' 'defensive' 'roger' 'infield' 'hit' 'hung'\n",
      " 'homers' 'threw' 'bases' 'pitch' 'baseball' 'opener' 'waivers'\n",
      " 'fierkelab' 'hitter' 'platoon' 'erics' 'kirsch' 'shortstop' 'rotation'\n",
      " 'demers' 'gajarsky' 'lame' 'catch' 'passed' 'rp' 'jay' 'mjones' 'rickert'\n",
      " 'clutch' 'relief' 'humor' 'batters' 'racking' 'bats' 'starters' 'scott'\n",
      " 'paula' 'prime' 'scoring' 'fielding' 'fls' 'admiral' 'closer' 'hits'\n",
      " 'slot' 'peak' 'unassisted' 'glove' 'vesterman' 'pitches' 'leagues'\n",
      " 'point' 'stick' 'runners' 'defensively' 'swing' 'lineup' 'pm' 'gspira'\n",
      " 'baseman' 'baserunning' 'goaltender' 'liked' 'souviens' 'stance'\n",
      " 'lankford' 'davewood' 'fly' 'pace' 'gargle' 'jrogoff' 'starter'\n",
      " 'BIAS_CONSTANT' 'shot' 'umpires' 'draft' 'traven' 'runner' 'offense'\n",
      " 'assists' 'series' 'pinch' 'plate' 'hitters' 'catchers' 'circle' 'tough'\n",
      " 'rauser' 'players' 'net' 'pp' 'snichols' 'maynard' 'nimaster' 'goalies'\n",
      " 'willis' 'instead' 'shortstops' 'goaltending' 'player' 'rm' 'match'\n",
      " 'line' 'roughing' 'stat' 'sh' 'points' 'howl' 'forwards' 'etxonss'\n",
      " 'hammerl' 'conference' 'rebound' 'hell' 'forward' 'ca' 'gballent' 'pts'\n",
      " 'breaker' 'pool']\n",
      "(163,)\n",
      "\n",
      "['hockey' 'golchowy' 'playoffs' 'playoff' 'goal' 'goals' 'round' 'coach'\n",
      " 'ice' 'dchhabra' 'puck' 'stop' 'farenebt' 'defensemen' 'penalties'\n",
      " 'captain' 'cordially' 'poll' 'pick' 'mask' 'franchise' 'standings'\n",
      " 'kkeller' 'nne' 'defenseman' 'boards' 'rights' 'octopus' 'fmsalvat' 'gld'\n",
      " 'wing' 'goalie' 'period' 'shots' 'tie' 'penalty' 'arena' 'zone']\n",
      "(38,)\n"
     ]
    }
   ],
   "source": [
    "def get_selected_prediction_arr(val, vocab_arr, chosen_words_indcies, weights):\n",
    "    # we iclude bias as a seprate input\n",
    "    input_vector = np.diag(val * np.ones(len(chosen_words_indcies)))\n",
    "    input_vector[:, 0] = 1\n",
    "    \n",
    "    out = np.matmul(input_vector, weights.reshape([-1, 1]))\n",
    "    \n",
    "    #sigmoid\n",
    "    out = np.exp(out)\n",
    "    out = out/(1+out)\n",
    "    \n",
    "    #Sorting words\n",
    "    p_words_indcies = np.where(out>=0.5)[0]\n",
    "    p_vocab_indcies = sort_with_keys(out[p_words_indcies].reshape(-1), chosen_words_indcies[p_words_indcies])[::-1]\n",
    "    \n",
    "    n_words_indcies = np.where(out<0.5)[0]\n",
    "    n_vocab_indcies = sort_with_keys(out[n_words_indcies].reshape(-1), chosen_words_indcies[n_words_indcies])\n",
    "    return p_vocab_indcies, n_vocab_indcies\n",
    "\n",
    "p_vocab_indcies, n_vocab_indcies = get_selected_prediction_arr(1, vocab_arr, chosen_words_indcies, weights)\n",
    "\n",
    "print(vocab_arr[p_vocab_indcies])\n",
    "print(vocab_arr[p_vocab_indcies].shape)\n",
    "print()\n",
    "print(vocab_arr[n_vocab_indcies])\n",
    "print(vocab_arr[n_vocab_indcies].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e622db-5ff5-4cfa-b8e0-48076a4abb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
