{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c4e367-076a-4220-b9f8-5fd0ab0aca14",
   "metadata": {},
   "source": [
    "# Binary Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c69afb-1036-4df5-8172-d39852be298b",
   "metadata": {},
   "source": [
    "# Least Test Loss \n",
    "**Selected Features with normalization no any thing else**\n",
    "```\n",
    "Saveing weights for epoch=2, Least Test Loss=0.109632\n",
    "Train logP=-177.792395 Test logP=-14.581043 Train Acc=0.927632 Test Acc=0.962406 TrainLoss=0.167098 TestLoss=0.109632\n",
    "\n",
    "```\n",
    "\n",
    "# Top Test Accuracy\n",
    "**Selected Features with normalization with exp decay lr=0.9, decay_rate=1e-5**\n",
    "```\n",
    "Epoch:81\n",
    "    Update 85251\tTP -128.839587\tHP -22.282172\tTA 0.937030\tHA 0.962406\t lr 0.383709\n",
    "    Update 85501\tTP -129.724659\tHP -22.480176\tTA 0.934211\tHA 0.977444\t lr 0.382751\n",
    "    Update 85751\tTP -130.095325\tHP -21.560638\tTA 0.936090\tHA 0.969925\t lr 0.381795\n",
    "    Update 86001\tTP -130.016436\tHP -21.867915\tTA 0.935150\tHA 0.977444\t lr 0.380842\n",
    "Train logP=-130.016436 Test logP=-21.867915 Train Acc=0.935150 Test Acc=0.977444 TrainLoss=0.122196 TestLoss=0.164420\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec8d08-6009-4d3d-afd7-544a1e8f26b3",
   "metadata": {},
   "source": [
    "## Learning Rate schedules:\n",
    "\n",
    "* time based\n",
    "* exponenetial\n",
    "* step\n",
    "* adaptive learning rate:\n",
    "    * Momentum\n",
    "    * adagrade\n",
    "    * RMS prob\n",
    "    * Adam\n",
    "    * Deep learning learning rate (using LSTMs, [reiformcnt learning](https://arxiv.org/pdf/1909.09712.pdf)\n",
    "---------------\n",
    "| Adaptive Learning Rate Schedule | Non adaptive |\n",
    "|------------|--------------|\n",
    "|faster in convergence | slower in convergence |\n",
    "|generalizes worst in simple problems and images (data close to mean -> Guassian) and bad test and train loss [[1]](https://proceedings.neurips.cc/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf)  | generalizes better in simple problems and images (data close to mean -> Guassian) and better test and train l\n",
    "|better in heavy tailed didtribution (date is very var from mean) like: BERT[[2]](https://proceedings.neurips.cc/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf) | worst |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3016459-1a66-40ba-a109-22e9d05c2fcc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf32e4-c419-4071-8f25-a5863c7d50e5",
   "metadata": {},
   "source": [
    "## All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca29b4-edda-4557-9971-4d24232b07f9",
   "metadata": {},
   "source": [
    "### Training all features without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4551b246-56ea-420c-9f93-45488e109a25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-149.149401 Test logP=-26.754949 Train Acc=0.964286 Test Acc=0.924812 TrainLoss=0.140178 TestLoss=0.201165\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-84.401002 Test logP=-21.527312 Train Acc=0.986842 Test Acc=0.939850 TrainLoss=0.079324 TestLoss=0.161859\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-65.502195 Test logP=-20.412968 Train Acc=0.996241 Test Acc=0.947368 TrainLoss=0.061562 TestLoss=0.153481\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-53.069850 Test logP=-20.612826 Train Acc=0.996241 Test Acc=0.947368 TrainLoss=0.049878 TestLoss=0.154984\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-46.047651 Test logP=-20.111051 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.043278 TestLoss=0.151211\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-40.675099 Test logP=-19.653638 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.038228 TestLoss=0.147772\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-36.130418 Test logP=-20.100699 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.033957 TestLoss=0.151133\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-32.845221 Test logP=-19.557510 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.030870 TestLoss=0.147049\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-30.222429 Test logP=-19.394910 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.028405 TestLoss=0.145826\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-27.733150 Test logP=-19.753764 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.026065 TestLoss=0.148525\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-25.909535 Test logP=-19.725736 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.024351 TestLoss=0.148314\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-24.279060 Test logP=-19.436022 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.022819 TestLoss=0.146136\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Train logP=-22.688054 Test logP=-19.738439 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.021323 TestLoss=0.148409\n",
      "----------------------------\n",
      "Epoch:14\n",
      "Train logP=-21.466356 Test logP=-19.697014 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.020175 TestLoss=0.148098\n",
      "----------------------------\n",
      "Epoch:15\n",
      "Train logP=-20.385334 Test logP=-19.599153 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.019159 TestLoss=0.147362\n",
      "----------------------------\n",
      "Epoch:16\n",
      "Train logP=-19.233071 Test logP=-19.623229 Train Acc=1.000000 Test Acc=0.939850 TrainLoss=0.018076 TestLoss=0.147543\n",
      "----------------------------\n",
      "Epoch:17\n",
      "Train logP=-18.394759 Test logP=-19.743704 Train Acc=1.000000 Test Acc=0.939850 TrainLoss=0.017288 TestLoss=0.148449\n",
      "----------------------------\n",
      "Epoch:18\n",
      "Train logP=-17.613860 Test logP=-19.699945 Train Acc=1.000000 Test Acc=0.939850 TrainLoss=0.016554 TestLoss=0.148120\n",
      "----------------------------\n",
      "Epoch:19\n",
      "Train logP=-16.763340 Test logP=-19.768688 Train Acc=1.000000 Test Acc=0.939850 TrainLoss=0.015755 TestLoss=0.148637\n",
      "----------------------------\n",
      "Epoch:20\n",
      "Train logP=-16.126564 Test logP=-19.842739 Train Acc=1.000000 Test Acc=0.939850 TrainLoss=0.015157 TestLoss=0.149194\n",
      "----------------------------\n",
      "Epoch:21\n",
      "Train logP=-15.535032 Test logP=-19.850626 Train Acc=1.000000 Test Acc=0.939850 TrainLoss=0.014601 TestLoss=0.149253\n",
      "----------------------------\n",
      "Epoch:22\n",
      "Train logP=-14.865469 Test logP=-19.816899 Train Acc=1.000000 Test Acc=0.939850 TrainLoss=0.013971 TestLoss=0.148999\n",
      "----------------------------\n",
      "Epoch:23\n",
      "Early Stop\n",
      "Train logP=-14.375029 Test logP=-19.956505 Train Acc=1.000000 Test Acc=0.939850 TrainLoss=0.013510 TestLoss=0.150049\n",
      "Figure(1000x400)\n",
      "Saveing weights for epoch=9, Least Test Loss=0.145826\n",
      "Train logP=-30.222429 Test logP=-19.394910 Train Acc=0.999060 Test Acc=0.939850 TrainLoss=0.028405 TestLoss=0.145826\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=2000  --step=0.08 --early_stop=10 --normalize=no \\\n",
    "    --log=no --log_step=200 --plot_name=all_features_no_normalization \\\n",
    "    --save_weights_path=weights/all_features_no_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52930e-bca4-492c-b77a-5fabc710f41d",
   "metadata": {},
   "source": [
    "![plot](figures/all_features_no_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5eb9f-3d00-422e-88dc-ff79dbecca28",
   "metadata": {},
   "source": [
    "### Training All features with Noramlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1795b41e-246a-4d2d-84e1-2cb89730a030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-617.326649 Test logP=-108.078413 Train Acc=0.947368 Test Acc=0.917293 TrainLoss=0.580194 TestLoss=0.812620\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-628.007954 Test logP=-113.450335 Train Acc=0.952068 Test Acc=0.932331 TrainLoss=0.590233 TestLoss=0.853010\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-17.231142 Test logP=-87.355211 Train Acc=0.996241 Test Acc=0.932331 TrainLoss=0.016195 TestLoss=0.656806\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-0.473470 Test logP=-97.253513 Train Acc=1.000000 Test Acc=0.924812 TrainLoss=0.000445 TestLoss=0.731229\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-0.347900 Test logP=-98.867284 Train Acc=1.000000 Test Acc=0.924812 TrainLoss=0.000327 TestLoss=0.743363\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-0.271777 Test logP=-99.469886 Train Acc=1.000000 Test Acc=0.924812 TrainLoss=0.000255 TestLoss=0.747894\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-0.225678 Test logP=-99.896939 Train Acc=1.000000 Test Acc=0.924812 TrainLoss=0.000212 TestLoss=0.751105\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-0.194921 Test logP=-100.271733 Train Acc=1.000000 Test Acc=0.924812 TrainLoss=0.000183 TestLoss=0.753923\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-0.172543 Test logP=-100.592803 Train Acc=1.000000 Test Acc=0.924812 TrainLoss=0.000162 TestLoss=0.756337\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Early Stop\n",
      "Train logP=-0.155432 Test logP=-100.874220 Train Acc=1.000000 Test Acc=0.924812 TrainLoss=0.000146 TestLoss=0.758453\n",
      "Figure(1000x400)\n",
      "Saveing weights for epoch=3, Least Test Loss=0.656806\n",
      "Train logP=-17.231142 Test logP=-87.355211 Train Acc=0.996241 Test Acc=0.932331 TrainLoss=0.016195 TestLoss=0.656806\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=100  --step=0.08 --early_stop=9 --normalize=yes \\\n",
    "    --log=no --log_step=200 --plot_name=all_features_with_normalization \\\n",
    "    --save_weights_path=weights/all_features_with_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d4306-094d-4022-97bd-6438ef923a5b",
   "metadata": {},
   "source": [
    "![all_normalized](figures/all_features_with_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4ce1c-050f-4372-be08-9813e62a3f63",
   "metadata": {},
   "source": [
    "## Selected Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74b8f0-cd6e-4884-b465-4087a682c6b0",
   "metadata": {},
   "source": [
    "### Traing selected features without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "888d5e74-fa5a-42c5-b205-16e373915c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-293.135474 Test logP=-30.573377 Train Acc=0.914474 Test Acc=0.939850 TrainLoss=0.275503 TestLoss=0.229875\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-247.746595 Test logP=-24.573902 Train Acc=0.918233 Test Acc=0.939850 TrainLoss=0.232845 TestLoss=0.184766\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-226.362215 Test logP=-21.807285 Train Acc=0.921053 Test Acc=0.947368 TrainLoss=0.212746 TestLoss=0.163965\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-215.242675 Test logP=-20.963799 Train Acc=0.927632 Test Acc=0.947368 TrainLoss=0.202296 TestLoss=0.157623\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-204.341468 Test logP=-19.747946 Train Acc=0.929511 Test Acc=0.947368 TrainLoss=0.192050 TestLoss=0.148481\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-196.511864 Test logP=-18.682876 Train Acc=0.927632 Test Acc=0.954887 TrainLoss=0.184692 TestLoss=0.140473\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-191.041470 Test logP=-18.179328 Train Acc=0.927632 Test Acc=0.954887 TrainLoss=0.179550 TestLoss=0.136687\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-187.830075 Test logP=-18.497257 Train Acc=0.931391 Test Acc=0.947368 TrainLoss=0.176532 TestLoss=0.139077\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-182.816112 Test logP=-17.954522 Train Acc=0.931391 Test Acc=0.954887 TrainLoss=0.171820 TestLoss=0.134996\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-179.000570 Test logP=-17.336483 Train Acc=0.930451 Test Acc=0.954887 TrainLoss=0.168234 TestLoss=0.130349\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-178.010127 Test logP=-17.890909 Train Acc=0.931391 Test Acc=0.954887 TrainLoss=0.167303 TestLoss=0.134518\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-174.604834 Test logP=-17.559185 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.164102 TestLoss=0.132024\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Train logP=-171.384366 Test logP=-16.856816 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.161076 TestLoss=0.126743\n",
      "----------------------------\n",
      "Epoch:14\n",
      "Train logP=-169.453833 Test logP=-16.740612 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.159261 TestLoss=0.125869\n",
      "----------------------------\n",
      "Epoch:15\n",
      "Train logP=-169.535426 Test logP=-17.475894 Train Acc=0.933271 Test Acc=0.962406 TrainLoss=0.159338 TestLoss=0.131398\n",
      "----------------------------\n",
      "Epoch:16\n",
      "Train logP=-166.509131 Test logP=-17.099034 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.156494 TestLoss=0.128564\n",
      "----------------------------\n",
      "Epoch:17\n",
      "Train logP=-164.378364 Test logP=-16.533444 Train Acc=0.934211 Test Acc=0.954887 TrainLoss=0.154491 TestLoss=0.124312\n",
      "----------------------------\n",
      "Epoch:18\n",
      "Train logP=-164.598452 Test logP=-17.224137 Train Acc=0.934211 Test Acc=0.962406 TrainLoss=0.154698 TestLoss=0.129505\n",
      "----------------------------\n",
      "Epoch:19\n",
      "Train logP=-162.501856 Test logP=-16.992135 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.152727 TestLoss=0.127760\n",
      "----------------------------\n",
      "Epoch:20\n",
      "Train logP=-160.623927 Test logP=-16.683980 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.150962 TestLoss=0.125443\n",
      "----------------------------\n",
      "Epoch:21\n",
      "Train logP=-159.369950 Test logP=-16.398754 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.149784 TestLoss=0.123299\n",
      "----------------------------\n",
      "Epoch:22\n",
      "Train logP=-159.976951 Test logP=-17.081094 Train Acc=0.934211 Test Acc=0.962406 TrainLoss=0.150354 TestLoss=0.128429\n",
      "----------------------------\n",
      "Epoch:23\n",
      "Train logP=-157.878497 Test logP=-16.770397 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.148382 TestLoss=0.126093\n",
      "----------------------------\n",
      "Epoch:24\n",
      "Train logP=-156.644979 Test logP=-16.281621 Train Acc=0.934211 Test Acc=0.954887 TrainLoss=0.147223 TestLoss=0.122418\n",
      "----------------------------\n",
      "Epoch:25\n",
      "Train logP=-155.917646 Test logP=-16.248158 Train Acc=0.934211 Test Acc=0.954887 TrainLoss=0.146539 TestLoss=0.122167\n",
      "----------------------------\n",
      "Epoch:26\n",
      "Train logP=-155.668130 Test logP=-16.819793 Train Acc=0.933271 Test Acc=0.947368 TrainLoss=0.146305 TestLoss=0.126465\n",
      "----------------------------\n",
      "Epoch:27\n",
      "Train logP=-154.288293 Test logP=-16.527623 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.145008 TestLoss=0.124268\n",
      "----------------------------\n",
      "Epoch:28\n",
      "Train logP=-153.489496 Test logP=-16.258345 Train Acc=0.934211 Test Acc=0.954887 TrainLoss=0.144257 TestLoss=0.122243\n",
      "----------------------------\n",
      "Epoch:29\n",
      "Train logP=-154.226943 Test logP=-16.959593 Train Acc=0.934211 Test Acc=0.962406 TrainLoss=0.144950 TestLoss=0.127516\n",
      "----------------------------\n",
      "Epoch:30\n",
      "Train logP=-152.497539 Test logP=-16.654478 Train Acc=0.933271 Test Acc=0.947368 TrainLoss=0.143325 TestLoss=0.125222\n",
      "----------------------------\n",
      "Epoch:31\n",
      "Train logP=-151.797882 Test logP=-16.198751 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.142667 TestLoss=0.121795\n",
      "----------------------------\n",
      "Epoch:32\n",
      "Train logP=-151.282915 Test logP=-16.189929 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.142183 TestLoss=0.121729\n",
      "----------------------------\n",
      "Epoch:33\n",
      "Train logP=-151.397924 Test logP=-16.838432 Train Acc=0.934211 Test Acc=0.962406 TrainLoss=0.142291 TestLoss=0.126605\n",
      "----------------------------\n",
      "Epoch:34\n",
      "Train logP=-150.125527 Test logP=-16.577721 Train Acc=0.934211 Test Acc=0.947368 TrainLoss=0.141095 TestLoss=0.124645\n",
      "----------------------------\n",
      "Epoch:35\n",
      "Train logP=-149.650748 Test logP=-16.204032 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.140649 TestLoss=0.121835\n",
      "----------------------------\n",
      "Epoch:36\n",
      "Train logP=-150.313379 Test logP=-16.942010 Train Acc=0.934211 Test Acc=0.962406 TrainLoss=0.141272 TestLoss=0.127384\n",
      "----------------------------\n",
      "Epoch:37\n",
      "Train logP=-149.117712 Test logP=-16.754032 Train Acc=0.935150 Test Acc=0.962406 TrainLoss=0.140148 TestLoss=0.125970\n",
      "----------------------------\n",
      "Epoch:38\n",
      "Train logP=-148.368024 Test logP=-16.203406 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.139444 TestLoss=0.121830\n",
      "----------------------------\n",
      "Epoch:39\n",
      "Early Stop\n",
      "Train logP=-147.992718 Test logP=-16.203803 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.139091 TestLoss=0.121833\n",
      "Figure(1000x400)\n",
      "Saveing weights for epoch=32, Least Test Loss=0.121729\n",
      "Train logP=-151.282915 Test logP=-16.189929 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.142183 TestLoss=0.121729\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=200  --step=.08 --early_stop=10 --normalize=no \\\n",
    "    --log=no --log_step=50 --plot_name=selected_features_no_normalization \\\n",
    "    --save_weights_path='weights/selected_features_no_normalization' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e58cd8-945a-4bd7-b7d8-8eb2d61e5fee",
   "metadata": {},
   "source": [
    "![selected_no_normalization](figures/selected_features_no_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0638cbc-3da5-4d6d-a101-4a99cd6e82ad",
   "metadata": {},
   "source": [
    "### Trainign Selected Features with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "456b1814-b331-4359-a504-da6b4458c3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "Train logP=-195.493812 Test logP=-16.568682 Train Acc=0.924812 Test Acc=0.947368 TrainLoss=0.183735 TestLoss=0.124577\n",
      "----------------------------\n",
      "Epoch:2\n",
      "Train logP=-177.792395 Test logP=-14.581043 Train Acc=0.927632 Test Acc=0.962406 TrainLoss=0.167098 TestLoss=0.109632\n",
      "----------------------------\n",
      "Epoch:3\n",
      "Train logP=-156.392502 Test logP=-14.778595 Train Acc=0.929511 Test Acc=0.962406 TrainLoss=0.146985 TestLoss=0.111117\n",
      "----------------------------\n",
      "Epoch:4\n",
      "Train logP=-154.351257 Test logP=-17.348090 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.145067 TestLoss=0.130437\n",
      "----------------------------\n",
      "Epoch:5\n",
      "Train logP=-150.205824 Test logP=-16.027217 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.141171 TestLoss=0.120505\n",
      "----------------------------\n",
      "Epoch:6\n",
      "Train logP=-145.281425 Test logP=-17.272224 Train Acc=0.933271 Test Acc=0.947368 TrainLoss=0.136543 TestLoss=0.129866\n",
      "----------------------------\n",
      "Epoch:7\n",
      "Train logP=-147.546922 Test logP=-17.862052 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.138672 TestLoss=0.134301\n",
      "----------------------------\n",
      "Epoch:8\n",
      "Train logP=-146.490758 Test logP=-15.629441 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.137679 TestLoss=0.117515\n",
      "----------------------------\n",
      "Epoch:9\n",
      "Train logP=-141.498891 Test logP=-15.392958 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.132988 TestLoss=0.115737\n",
      "----------------------------\n",
      "Epoch:10\n",
      "Train logP=-142.932644 Test logP=-17.751345 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.134335 TestLoss=0.133469\n",
      "----------------------------\n",
      "Epoch:11\n",
      "Train logP=-143.346417 Test logP=-16.920787 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.134724 TestLoss=0.127224\n",
      "----------------------------\n",
      "Epoch:12\n",
      "Train logP=-142.328146 Test logP=-16.333013 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.133767 TestLoss=0.122805\n",
      "----------------------------\n",
      "Epoch:13\n",
      "Train logP=-141.692420 Test logP=-18.128852 Train Acc=0.936090 Test Acc=0.947368 TrainLoss=0.133170 TestLoss=0.136307\n",
      "----------------------------\n",
      "Epoch:14\n",
      "Train logP=-141.212260 Test logP=-16.855285 Train Acc=0.936090 Test Acc=0.954887 TrainLoss=0.132718 TestLoss=0.126731\n",
      "----------------------------\n",
      "Epoch:15\n",
      "Train logP=-140.966589 Test logP=-16.682582 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.132487 TestLoss=0.125433\n",
      "----------------------------\n",
      "Epoch:16\n",
      "Train logP=-136.732186 Test logP=-16.943026 Train Acc=0.937030 Test Acc=0.954887 TrainLoss=0.128508 TestLoss=0.127391\n",
      "----------------------------\n",
      "Epoch:17\n",
      "Train logP=-140.515035 Test logP=-17.407886 Train Acc=0.936090 Test Acc=0.954887 TrainLoss=0.132063 TestLoss=0.130886\n",
      "----------------------------\n",
      "Epoch:18\n",
      "Train logP=-139.993828 Test logP=-16.871359 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.131573 TestLoss=0.126852\n",
      "----------------------------\n",
      "Epoch:19\n",
      "Train logP=-136.703043 Test logP=-16.751444 Train Acc=0.934211 Test Acc=0.954887 TrainLoss=0.128480 TestLoss=0.125951\n",
      "----------------------------\n",
      "Epoch:20\n",
      "Train logP=-139.295724 Test logP=-17.515512 Train Acc=0.936090 Test Acc=0.954887 TrainLoss=0.130917 TestLoss=0.131696\n",
      "----------------------------\n",
      "Epoch:21\n",
      "Train logP=-139.390570 Test logP=-17.398932 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.131006 TestLoss=0.130819\n",
      "----------------------------\n",
      "Epoch:22\n",
      "Train logP=-139.307564 Test logP=-16.430725 Train Acc=0.906015 Test Acc=0.954887 TrainLoss=0.130928 TestLoss=0.123539\n",
      "----------------------------\n",
      "Epoch:23\n",
      "Early Stop\n",
      "Train logP=-134.517860 Test logP=-17.193293 Train Acc=0.938910 Test Acc=0.954887 TrainLoss=0.126427 TestLoss=0.129273\n",
      "Figure(1000x400)\n",
      "Saveing weights for epoch=2, Least Test Loss=0.109632\n",
      "Train logP=-177.792395 Test logP=-14.581043 Train Acc=0.927632 Test Acc=0.962406 TrainLoss=0.167098 TestLoss=0.109632\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=100  --step=.08 --early_stop=10 --normalize=yes \\\n",
    "    --log=no --log_step=200 --plot_name='selected_features_with_normalization' \\\n",
    "    --save_weights_path='weights/selected_features_with_normalization' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a0ac7-f352-4b41-ac65-718cfe30fe9f",
   "metadata": {},
   "source": [
    "![slected with normalization](figures/selected_features_with_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d10977-03eb-40ed-ab3c-30b3f96b1bc7",
   "metadata": {},
   "source": [
    "## Selected Features with Exp Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59358fa-ab62-4ece-99a4-cff7d47d3e05",
   "metadata": {},
   "source": [
    "### Training Selected Features without normalization with Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42c152af-d4d4-40f4-9d3f-d73e194bf116",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "    Update 1\tTP -727.962508\tHP -89.946812\tTA 0.497180\tHA 0.533835\t lr 0.099992\n",
      "    Update 201\tTP -403.750931\tHP -45.037074\tTA 0.881579\tHA 0.924812\t lr 0.098405\n",
      "    Update 401\tTP -364.899568\tHP -40.957187\tTA 0.843985\tHA 0.902256\t lr 0.096843\n",
      "    Update 601\tTP -322.633859\tHP -33.573849\tTA 0.900376\tHA 0.932331\t lr 0.095306\n",
      "    Update 801\tTP -295.189530\tHP -30.396073\tTA 0.905075\tHA 0.932331\t lr 0.093793\n",
      "    Update 1001\tTP -280.894037\tHP -28.697907\tTA 0.911654\tHA 0.932331\t lr 0.092304\n",
      "Train logP=-280.894037 Test logP=-28.697907 Train Acc=0.911654 Test Acc=0.932331 TrainLoss=0.263998 TestLoss=0.215774\n",
      "----------------------------\n",
      "Epoch:2\n",
      "    Update 1201\tTP -269.766972\tHP -27.247359\tTA 0.908835\tHA 0.954887\t lr 0.090839\n",
      "    Update 1401\tTP -264.244422\tHP -26.347554\tTA 0.900376\tHA 0.939850\t lr 0.089397\n",
      "    Update 1601\tTP -256.080149\tHP -25.433419\tTA 0.909774\tHA 0.947368\t lr 0.087978\n",
      "    Update 1801\tTP -249.717531\tHP -24.682599\tTA 0.918233\tHA 0.939850\t lr 0.086582\n",
      "    Update 2001\tTP -242.221256\tHP -23.757345\tTA 0.916353\tHA 0.947368\t lr 0.085208\n",
      "Train logP=-242.221256 Test logP=-23.757345 Train Acc=0.916353 Test Acc=0.947368 TrainLoss=0.227652 TestLoss=0.178627\n",
      "----------------------------\n",
      "Epoch:3\n",
      "    Update 2201\tTP -237.259504\tHP -23.089763\tTA 0.919173\tHA 0.947368\t lr 0.083855\n",
      "    Update 2401\tTP -234.008260\tHP -22.570600\tTA 0.915414\tHA 0.954887\t lr 0.082524\n",
      "    Update 2601\tTP -230.510159\tHP -22.373148\tTA 0.916353\tHA 0.954887\t lr 0.081214\n",
      "    Update 2801\tTP -226.711801\tHP -21.598104\tTA 0.920113\tHA 0.954887\t lr 0.079925\n",
      "    Update 3001\tTP -224.548154\tHP -21.626162\tTA 0.923872\tHA 0.939850\t lr 0.078656\n",
      "Train logP=-224.548154 Test logP=-21.626162 Train Acc=0.923872 Test Acc=0.939850 TrainLoss=0.211041 TestLoss=0.162603\n",
      "----------------------------\n",
      "Epoch:4\n",
      "    Update 3201\tTP -223.252310\tHP -21.758058\tTA 0.926692\tHA 0.939850\t lr 0.077408\n",
      "    Update 3401\tTP -218.579594\tHP -20.944064\tTA 0.921053\tHA 0.954887\t lr 0.076179\n",
      "    Update 3601\tTP -219.134182\tHP -21.010593\tTA 0.917293\tHA 0.947368\t lr 0.074970\n",
      "    Update 3801\tTP -214.938148\tHP -20.477909\tTA 0.924812\tHA 0.947368\t lr 0.073780\n",
      "    Update 4001\tTP -212.974234\tHP -20.400909\tTA 0.925752\tHA 0.939850\t lr 0.072609\n",
      "    Update 4201\tTP -210.512984\tHP -20.014479\tTA 0.922932\tHA 0.954887\t lr 0.071457\n",
      "Train logP=-210.512984 Test logP=-20.014479 Train Acc=0.922932 Test Acc=0.954887 TrainLoss=0.197851 TestLoss=0.150485\n",
      "----------------------------\n",
      "Epoch:5\n",
      "    Update 4401\tTP -209.208276\tHP -19.886775\tTA 0.922932\tHA 0.954887\t lr 0.070322\n",
      "    Update 4601\tTP -208.244718\tHP -19.717882\tTA 0.921053\tHA 0.954887\t lr 0.069206\n",
      "    Update 4801\tTP -206.553505\tHP -19.591876\tTA 0.921992\tHA 0.954887\t lr 0.068108\n",
      "    Update 5001\tTP -206.273765\tHP -19.931184\tTA 0.923872\tHA 0.947368\t lr 0.067027\n",
      "    Update 5201\tTP -203.276555\tHP -19.398621\tTA 0.928571\tHA 0.954887\t lr 0.065963\n",
      "Train logP=-203.276555 Test logP=-19.398621 Train Acc=0.928571 Test Acc=0.954887 TrainLoss=0.191049 TestLoss=0.145854\n",
      "----------------------------\n",
      "Epoch:6\n",
      "    Update 5401\tTP -202.108377\tHP -19.244819\tTA 0.927632\tHA 0.954887\t lr 0.064916\n",
      "    Update 5601\tTP -201.503714\tHP -19.049110\tTA 0.921992\tHA 0.954887\t lr 0.063885\n",
      "    Update 5801\tTP -200.191261\tHP -19.064378\tTA 0.924812\tHA 0.954887\t lr 0.062871\n",
      "    Update 6001\tTP -198.767740\tHP -18.817638\tTA 0.927632\tHA 0.954887\t lr 0.061873\n",
      "    Update 6201\tTP -198.162494\tHP -18.933225\tTA 0.927632\tHA 0.954887\t lr 0.060891\n",
      "Train logP=-198.162494 Test logP=-18.933225 Train Acc=0.927632 Test Acc=0.954887 TrainLoss=0.186243 TestLoss=0.142355\n",
      "----------------------------\n",
      "Epoch:7\n",
      "    Update 6401\tTP -198.100247\tHP -19.187796\tTA 0.928571\tHA 0.947368\t lr 0.059925\n",
      "    Update 6601\tTP -196.018745\tHP -18.681397\tTA 0.928571\tHA 0.954887\t lr 0.058974\n",
      "    Update 6801\tTP -196.001600\tHP -18.668242\tTA 0.924812\tHA 0.954887\t lr 0.058038\n",
      "    Update 7001\tTP -194.678797\tHP -18.559623\tTA 0.928571\tHA 0.954887\t lr 0.057116\n",
      "    Update 7201\tTP -194.751474\tHP -18.772095\tTA 0.929511\tHA 0.947368\t lr 0.056210\n",
      "    Update 7401\tTP -192.888859\tHP -18.376515\tTA 0.927632\tHA 0.954887\t lr 0.055318\n",
      "Train logP=-192.888859 Test logP=-18.376515 Train Acc=0.927632 Test Acc=0.954887 TrainLoss=0.181287 TestLoss=0.138169\n",
      "----------------------------\n",
      "Epoch:8\n",
      "    Update 7601\tTP -192.586223\tHP -18.281956\tTA 0.925752\tHA 0.954887\t lr 0.054440\n",
      "    Update 7801\tTP -192.246233\tHP -18.305730\tTA 0.925752\tHA 0.954887\t lr 0.053575\n",
      "    Update 8001\tTP -191.062455\tHP -18.147921\tTA 0.930451\tHA 0.954887\t lr 0.052725\n",
      "    Update 8201\tTP -190.606073\tHP -18.406261\tTA 0.928571\tHA 0.954887\t lr 0.051888\n",
      "    Update 8401\tTP -190.176610\tHP -18.353232\tTA 0.929511\tHA 0.954887\t lr 0.051065\n",
      "Train logP=-190.176610 Test logP=-18.353232 Train Acc=0.929511 Test Acc=0.954887 TrainLoss=0.178737 TestLoss=0.137994\n",
      "----------------------------\n",
      "Epoch:9\n",
      "    Update 8601\tTP -189.174571\tHP -18.191698\tTA 0.930451\tHA 0.954887\t lr 0.050254\n",
      "    Update 8801\tTP -188.906972\tHP -17.953507\tTA 0.927632\tHA 0.954887\t lr 0.049456\n",
      "    Update 9001\tTP -188.446741\tHP -17.984713\tTA 0.926692\tHA 0.954887\t lr 0.048671\n",
      "    Update 9201\tTP -187.511190\tHP -17.837282\tTA 0.929511\tHA 0.954887\t lr 0.047899\n",
      "    Update 9401\tTP -187.506110\tHP -18.107486\tTA 0.928571\tHA 0.954887\t lr 0.047139\n",
      "Train logP=-187.506110 Test logP=-18.107486 Train Acc=0.928571 Test Acc=0.954887 TrainLoss=0.176228 TestLoss=0.136147\n",
      "----------------------------\n",
      "Epoch:10\n",
      "    Update 9601\tTP -186.886319\tHP -18.141390\tTA 0.930451\tHA 0.954887\t lr 0.046390\n",
      "    Update 9801\tTP -186.009713\tHP -17.848363\tTA 0.929511\tHA 0.954887\t lr 0.045654\n",
      "    Update 10001\tTP -186.525468\tHP -17.800348\tTA 0.926692\tHA 0.954887\t lr 0.044929\n",
      "    Update 10201\tTP -185.125538\tHP -17.702964\tTA 0.931391\tHA 0.954887\t lr 0.044216\n",
      "    Update 10401\tTP -186.030149\tHP -18.121961\tTA 0.930451\tHA 0.947368\t lr 0.043514\n",
      "    Update 10601\tTP -184.338177\tHP -17.753482\tTA 0.929511\tHA 0.954887\t lr 0.042824\n",
      "Train logP=-184.338177 Test logP=-17.753482 Train Acc=0.929511 Test Acc=0.954887 TrainLoss=0.173250 TestLoss=0.133485\n",
      "----------------------------\n",
      "Epoch:11\n",
      "    Update 10801\tTP -184.190221\tHP -17.632635\tTA 0.929511\tHA 0.954887\t lr 0.042144\n",
      "    Update 11001\tTP -183.702138\tHP -17.711187\tTA 0.930451\tHA 0.954887\t lr 0.041475\n",
      "    Update 11201\tTP -183.320099\tHP -17.559861\tTA 0.930451\tHA 0.954887\t lr 0.040817\n",
      "    Update 11401\tTP -183.233539\tHP -17.829812\tTA 0.930451\tHA 0.954887\t lr 0.040169\n",
      "    Update 11601\tTP -182.580322\tHP -17.669815\tTA 0.931391\tHA 0.954887\t lr 0.039531\n",
      "Train logP=-182.580322 Test logP=-17.669815 Train Acc=0.931391 Test Acc=0.954887 TrainLoss=0.171598 TestLoss=0.132856\n",
      "----------------------------\n",
      "Epoch:12\n",
      "    Update 11801\tTP -182.211548\tHP -17.617523\tTA 0.930451\tHA 0.954887\t lr 0.038904\n",
      "    Update 12001\tTP -181.954504\tHP -17.560871\tTA 0.930451\tHA 0.954887\t lr 0.038286\n",
      "    Update 12201\tTP -181.960408\tHP -17.479665\tTA 0.928571\tHA 0.954887\t lr 0.037679\n",
      "    Update 12401\tTP -181.199797\tHP -17.454397\tTA 0.930451\tHA 0.954887\t lr 0.037080\n",
      "    Update 12601\tTP -181.141814\tHP -17.588595\tTA 0.931391\tHA 0.954887\t lr 0.036492\n",
      "Train logP=-181.141814 Test logP=-17.588595 Train Acc=0.931391 Test Acc=0.954887 TrainLoss=0.170246 TestLoss=0.132245\n",
      "----------------------------\n",
      "Epoch:13\n",
      "    Update 12801\tTP -181.031193\tHP -17.701200\tTA 0.931391\tHA 0.954887\t lr 0.035913\n",
      "    Update 13001\tTP -180.415165\tHP -17.400660\tTA 0.931391\tHA 0.954887\t lr 0.035343\n",
      "    Update 13201\tTP -180.486547\tHP -17.389519\tTA 0.930451\tHA 0.954887\t lr 0.034782\n",
      "    Update 13401\tTP -179.824663\tHP -17.317225\tTA 0.931391\tHA 0.954887\t lr 0.034230\n",
      "    Update 13601\tTP -180.112249\tHP -17.599162\tTA 0.930451\tHA 0.954887\t lr 0.033686\n",
      "    Update 13801\tTP -179.341882\tHP -17.396738\tTA 0.931391\tHA 0.954887\t lr 0.033152\n",
      "Train logP=-179.341882 Test logP=-17.396738 Train Acc=0.931391 Test Acc=0.954887 TrainLoss=0.168554 TestLoss=0.130803\n",
      "----------------------------\n",
      "Epoch:14\n",
      "    Update 14001\tTP -179.173393\tHP -17.304868\tTA 0.931391\tHA 0.954887\t lr 0.032625\n",
      "    Update 14201\tTP -178.990046\tHP -17.344603\tTA 0.931391\tHA 0.954887\t lr 0.032108\n",
      "    Update 14401\tTP -178.827039\tHP -17.226777\tTA 0.931391\tHA 0.954887\t lr 0.031598\n",
      "    Update 14601\tTP -178.581357\tHP -17.456903\tTA 0.931391\tHA 0.954887\t lr 0.031096\n",
      "    Update 14801\tTP -178.314930\tHP -17.398579\tTA 0.931391\tHA 0.954887\t lr 0.030603\n",
      "Train logP=-178.314930 Test logP=-17.398579 Train Acc=0.931391 Test Acc=0.954887 TrainLoss=0.167589 TestLoss=0.130816\n",
      "----------------------------\n",
      "Epoch:15\n",
      "    Update 15001\tTP -177.994123\tHP -17.325008\tTA 0.932331\tHA 0.954887\t lr 0.030117\n",
      "    Update 15201\tTP -177.881583\tHP -17.254043\tTA 0.931391\tHA 0.954887\t lr 0.029639\n",
      "    Update 15401\tTP -177.901118\tHP -17.194827\tTA 0.930451\tHA 0.954887\t lr 0.029169\n",
      "    Update 15601\tTP -177.343765\tHP -17.227527\tTA 0.932331\tHA 0.954887\t lr 0.028706\n",
      "    Update 15801\tTP -177.260133\tHP -17.313890\tTA 0.931391\tHA 0.954887\t lr 0.028250\n",
      "Train logP=-177.260133 Test logP=-17.313890 Train Acc=0.931391 Test Acc=0.954887 TrainLoss=0.166598 TestLoss=0.130180\n",
      "----------------------------\n",
      "Epoch:16\n",
      "    Update 16001\tTP -177.073856\tHP -17.349831\tTA 0.932331\tHA 0.954887\t lr 0.027802\n",
      "    Update 16201\tTP -176.802945\tHP -17.200425\tTA 0.932331\tHA 0.954887\t lr 0.027360\n",
      "    Update 16401\tTP -176.908034\tHP -17.149434\tTA 0.931391\tHA 0.954887\t lr 0.026926\n",
      "    Update 16601\tTP -176.467829\tHP -17.096605\tTA 0.932331\tHA 0.954887\t lr 0.026499\n",
      "    Update 16801\tTP -176.614477\tHP -17.338837\tTA 0.932331\tHA 0.954887\t lr 0.026078\n",
      "    Update 17001\tTP -176.309724\tHP -17.342152\tTA 0.931391\tHA 0.954887\t lr 0.025664\n",
      "Train logP=-176.309724 Test logP=-17.342152 Train Acc=0.931391 Test Acc=0.954887 TrainLoss=0.165705 TestLoss=0.130392\n",
      "----------------------------\n",
      "Epoch:17\n",
      "    Update 17201\tTP -176.005092\tHP -17.106344\tTA 0.932331\tHA 0.954887\t lr 0.025257\n",
      "    Update 17401\tTP -175.900241\tHP -17.119036\tTA 0.932331\tHA 0.954887\t lr 0.024856\n",
      "    Update 17601\tTP -175.655810\tHP -17.076285\tTA 0.932331\tHA 0.954887\t lr 0.024461\n",
      "    Update 17801\tTP -175.633606\tHP -17.253285\tTA 0.932331\tHA 0.954887\t lr 0.024073\n",
      "    Update 18001\tTP -175.415787\tHP -17.192803\tTA 0.932331\tHA 0.954887\t lr 0.023691\n",
      "Train logP=-175.415787 Test logP=-17.192803 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.164864 TestLoss=0.129269\n",
      "----------------------------\n",
      "Epoch:18\n",
      "    Update 18201\tTP -175.197588\tHP -17.097400\tTA 0.932331\tHA 0.954887\t lr 0.023315\n",
      "    Update 18401\tTP -175.109234\tHP -17.092660\tTA 0.932331\tHA 0.954887\t lr 0.022945\n",
      "    Update 18601\tTP -175.105590\tHP -17.037365\tTA 0.932331\tHA 0.954887\t lr 0.022581\n",
      "    Update 18801\tTP -174.771544\tHP -17.071967\tTA 0.932331\tHA 0.954887\t lr 0.022222\n",
      "    Update 19001\tTP -174.706145\tHP -17.138765\tTA 0.932331\tHA 0.954887\t lr 0.021869\n",
      "Train logP=-174.706145 Test logP=-17.138765 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.164198 TestLoss=0.128863\n",
      "----------------------------\n",
      "Epoch:19\n",
      "    Update 19201\tTP -174.591908\tHP -17.176666\tTA 0.932331\tHA 0.954887\t lr 0.021522\n",
      "    Update 19401\tTP -174.400961\tHP -17.047068\tTA 0.932331\tHA 0.954887\t lr 0.021181\n",
      "    Update 19601\tTP -174.431876\tHP -17.011243\tTA 0.932331\tHA 0.954887\t lr 0.020845\n",
      "    Update 19801\tTP -174.148765\tHP -17.045328\tTA 0.932331\tHA 0.954887\t lr 0.020514\n",
      "    Update 20001\tTP -174.249566\tHP -17.166094\tTA 0.931391\tHA 0.954887\t lr 0.020188\n",
      "    Update 20201\tTP -174.064498\tHP -17.179598\tTA 0.932331\tHA 0.954887\t lr 0.019868\n",
      "Train logP=-174.064498 Test logP=-17.179598 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.163594 TestLoss=0.129170\n",
      "----------------------------\n",
      "Epoch:20\n",
      "    Update 20401\tTP -173.799200\tHP -17.023338\tTA 0.932331\tHA 0.954887\t lr 0.019552\n",
      "    Update 20601\tTP -173.793106\tHP -16.986310\tTA 0.932331\tHA 0.954887\t lr 0.019242\n",
      "    Update 20801\tTP -173.603520\tHP -16.958696\tTA 0.932331\tHA 0.954887\t lr 0.018936\n",
      "    Update 21001\tTP -173.522282\tHP -17.077317\tTA 0.932331\tHA 0.954887\t lr 0.018636\n",
      "    Update 21201\tTP -173.427049\tHP -17.076259\tTA 0.932331\tHA 0.954887\t lr 0.018340\n",
      "Train logP=-173.427049 Test logP=-17.076259 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.162995 TestLoss=0.128393\n",
      "----------------------------\n",
      "Epoch:21\n",
      "    Update 21401\tTP -173.277367\tHP -17.001082\tTA 0.932331\tHA 0.954887\t lr 0.018049\n",
      "    Update 21601\tTP -173.248046\tHP -16.969036\tTA 0.932331\tHA 0.954887\t lr 0.017763\n",
      "    Update 21801\tTP -173.207744\tHP -16.932071\tTA 0.932331\tHA 0.954887\t lr 0.017481\n",
      "    Update 22001\tTP -172.977213\tHP -16.975801\tTA 0.932331\tHA 0.954887\t lr 0.017203\n",
      "    Update 22201\tTP -172.921705\tHP -17.030161\tTA 0.932331\tHA 0.954887\t lr 0.016930\n",
      "Train logP=-172.921705 Test logP=-17.030161 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.162520 TestLoss=0.128046\n",
      "----------------------------\n",
      "Epoch:22\n",
      "    Update 22401\tTP -172.830288\tHP -17.043959\tTA 0.932331\tHA 0.954887\t lr 0.016661\n",
      "    Update 22601\tTP -172.719538\tHP -16.944667\tTA 0.932331\tHA 0.954887\t lr 0.016397\n",
      "    Update 22801\tTP -172.739047\tHP -16.915367\tTA 0.932331\tHA 0.954887\t lr 0.016137\n",
      "    Update 23001\tTP -172.537434\tHP -16.935958\tTA 0.932331\tHA 0.954887\t lr 0.015880\n",
      "    Update 23201\tTP -172.540811\tHP -17.025154\tTA 0.932331\tHA 0.954887\t lr 0.015628\n",
      "    Update 23401\tTP -172.513206\tHP -17.084469\tTA 0.932331\tHA 0.954887\t lr 0.015380\n",
      "Train logP=-172.513206 Test logP=-17.084469 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.162136 TestLoss=0.128455\n",
      "----------------------------\n",
      "Epoch:23\n",
      "    Update 23601\tTP -172.285896\tHP -16.948194\tTA 0.932331\tHA 0.954887\t lr 0.015136\n",
      "    Update 23801\tTP -172.285717\tHP -16.904127\tTA 0.932331\tHA 0.954887\t lr 0.014896\n",
      "    Update 24001\tTP -172.148382\tHP -16.885737\tTA 0.932331\tHA 0.954887\t lr 0.014660\n",
      "    Update 24201\tTP -172.104076\tHP -16.992630\tTA 0.932331\tHA 0.954887\t lr 0.014427\n",
      "    Update 24401\tTP -172.015708\tHP -16.978058\tTA 0.932331\tHA 0.954887\t lr 0.014198\n",
      "Train logP=-172.015708 Test logP=-16.978058 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.161669 TestLoss=0.127655\n",
      "----------------------------\n",
      "Epoch:24\n",
      "    Update 24601\tTP -171.908329\tHP -16.921227\tTA 0.932331\tHA 0.954887\t lr 0.013972\n",
      "    Update 24801\tTP -171.902479\tHP -16.877610\tTA 0.932331\tHA 0.954887\t lr 0.013751\n",
      "    Update 25001\tTP -171.852281\tHP -16.856561\tTA 0.932331\tHA 0.954887\t lr 0.013532\n",
      "    Update 25201\tTP -171.694214\tHP -16.897451\tTA 0.932331\tHA 0.954887\t lr 0.013318\n",
      "    Update 25401\tTP -171.645231\tHP -16.937108\tTA 0.932331\tHA 0.954887\t lr 0.013106\n",
      "Train logP=-171.645231 Test logP=-16.937108 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.161321 TestLoss=0.127347\n",
      "----------------------------\n",
      "Epoch:25\n",
      "    Update 25601\tTP -171.583312\tHP -16.961455\tTA 0.932331\tHA 0.954887\t lr 0.012898\n",
      "    Update 25801\tTP -171.505007\tHP -16.874671\tTA 0.932331\tHA 0.954887\t lr 0.012694\n",
      "    Update 26001\tTP -171.500282\tHP -16.859372\tTA 0.932331\tHA 0.954887\t lr 0.012492\n",
      "    Update 26201\tTP -171.387240\tHP -16.851923\tTA 0.932331\tHA 0.954887\t lr 0.012294\n",
      "    Update 26401\tTP -171.380358\tHP -16.954054\tTA 0.932331\tHA 0.954887\t lr 0.012099\n",
      "Train logP=-171.380358 Test logP=-16.954054 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.161072 TestLoss=0.127474\n",
      "----------------------------\n",
      "Epoch:26\n",
      "    Update 26601\tTP -171.321308\tHP -16.981158\tTA 0.932331\tHA 0.954887\t lr 0.011907\n",
      "    Update 26801\tTP -171.190134\tHP -16.888165\tTA 0.932331\tHA 0.954887\t lr 0.011718\n",
      "    Update 27001\tTP -171.219507\tHP -16.835998\tTA 0.932331\tHA 0.954887\t lr 0.011532\n",
      "    Update 27201\tTP -171.081679\tHP -16.848687\tTA 0.932331\tHA 0.954887\t lr 0.011349\n",
      "    Update 27401\tTP -171.033032\tHP -16.903974\tTA 0.932331\tHA 0.954887\t lr 0.011168\n",
      "    Update 27601\tTP -170.982039\tHP -16.907107\tTA 0.932331\tHA 0.954887\t lr 0.010991\n",
      "Train logP=-170.982039 Test logP=-16.907107 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.160697 TestLoss=0.127121\n",
      "----------------------------\n",
      "Epoch:27\n",
      "    Update 27801\tTP -170.912139\tHP -16.862075\tTA 0.932331\tHA 0.954887\t lr 0.010817\n",
      "    Update 28001\tTP -170.921881\tHP -16.823955\tTA 0.932331\tHA 0.954887\t lr 0.010645\n",
      "    Update 28201\tTP -170.859745\tHP -16.814135\tTA 0.932331\tHA 0.954887\t lr 0.010476\n",
      "    Update 28401\tTP -170.766525\tHP -16.890980\tTA 0.932331\tHA 0.954887\t lr 0.010310\n",
      "    Update 28601\tTP -170.720405\tHP -16.884810\tTA 0.932331\tHA 0.954887\t lr 0.010146\n",
      "Train logP=-170.720405 Test logP=-16.884810 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.160452 TestLoss=0.126953\n",
      "----------------------------\n",
      "Epoch:28\n",
      "    Update 28801\tTP -170.673178\tHP -16.892557\tTA 0.932331\tHA 0.954887\t lr 0.009985\n",
      "    Update 29001\tTP -170.619244\tHP -16.828153\tTA 0.932331\tHA 0.954887\t lr 0.009827\n",
      "    Update 29201\tTP -170.586558\tHP -16.825060\tTA 0.932331\tHA 0.954887\t lr 0.009671\n",
      "    Update 29401\tTP -170.534828\tHP -16.807727\tTA 0.932331\tHA 0.954887\t lr 0.009517\n",
      "    Update 29601\tTP -170.496915\tHP -16.879835\tTA 0.932331\tHA 0.954887\t lr 0.009366\n",
      "Train logP=-170.496915 Test logP=-16.879835 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.160241 TestLoss=0.126916\n",
      "----------------------------\n",
      "Epoch:29\n",
      "    Update 29801\tTP -170.482302\tHP -16.923441\tTA 0.932331\tHA 0.954887\t lr 0.009217\n",
      "    Update 30001\tTP -170.381860\tHP -16.847122\tTA 0.932331\tHA 0.954887\t lr 0.009071\n",
      "    Update 30201\tTP -170.376521\tHP -16.809475\tTA 0.932331\tHA 0.954887\t lr 0.008927\n",
      "    Update 30401\tTP -170.300814\tHP -16.809997\tTA 0.932331\tHA 0.954887\t lr 0.008785\n",
      "    Update 30601\tTP -170.263627\tHP -16.853408\tTA 0.932331\tHA 0.954887\t lr 0.008646\n",
      "    Update 30801\tTP -170.218290\tHP -16.841938\tTA 0.932331\tHA 0.954887\t lr 0.008509\n",
      "Train logP=-170.218290 Test logP=-16.841938 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.159980 TestLoss=0.126631\n",
      "----------------------------\n",
      "Epoch:30\n",
      "    Update 31001\tTP -170.174364\tHP -16.836041\tTA 0.932331\tHA 0.954887\t lr 0.008374\n",
      "    Update 31201\tTP -170.156014\tHP -16.804875\tTA 0.932331\tHA 0.954887\t lr 0.008241\n",
      "    Update 31401\tTP -170.120951\tHP -16.789970\tTA 0.932331\tHA 0.954887\t lr 0.008110\n",
      "    Update 31601\tTP -170.065108\tHP -16.847862\tTA 0.932331\tHA 0.954887\t lr 0.007981\n",
      "    Update 31801\tTP -170.029240\tHP -16.841622\tTA 0.932331\tHA 0.954887\t lr 0.007855\n",
      "Train logP=-170.029240 Test logP=-16.841622 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.159802 TestLoss=0.126629\n",
      "----------------------------\n",
      "Epoch:31\n",
      "    Update 32001\tTP -169.990467\tHP -16.839979\tTA 0.932331\tHA 0.954887\t lr 0.007730\n",
      "    Update 32201\tTP -169.954417\tHP -16.801044\tTA 0.932331\tHA 0.954887\t lr 0.007607\n",
      "    Update 32401\tTP -169.928962\tHP -16.796790\tTA 0.932331\tHA 0.954887\t lr 0.007486\n",
      "    Update 32601\tTP -169.885790\tHP -16.788245\tTA 0.932331\tHA 0.954887\t lr 0.007368\n",
      "    Update 32801\tTP -169.853745\tHP -16.830013\tTA 0.932331\tHA 0.954887\t lr 0.007251\n",
      "Train logP=-169.853745 Test logP=-16.830013 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.159637 TestLoss=0.126541\n",
      "----------------------------\n",
      "Epoch:32\n",
      "    Update 33001\tTP -169.841823\tHP -16.867098\tTA 0.932331\tHA 0.954887\t lr 0.007136\n",
      "    Update 33201\tTP -169.777547\tHP -16.812266\tTA 0.932331\tHA 0.954887\t lr 0.007022\n",
      "    Update 33401\tTP -169.759884\tHP -16.792097\tTA 0.932331\tHA 0.954887\t lr 0.006911\n",
      "    Update 33601\tTP -169.715886\tHP -16.788191\tTA 0.932331\tHA 0.954887\t lr 0.006801\n",
      "    Update 33801\tTP -169.694726\tHP -16.826959\tTA 0.932331\tHA 0.954887\t lr 0.006693\n",
      "    Update 34001\tTP -169.654760\tHP -16.808066\tTA 0.932331\tHA 0.954887\t lr 0.006587\n",
      "Train logP=-169.654760 Test logP=-16.808066 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.159450 TestLoss=0.126376\n",
      "----------------------------\n",
      "Epoch:33\n",
      "    Update 34201\tTP -169.623849\tHP -16.799172\tTA 0.932331\tHA 0.954887\t lr 0.006482\n",
      "    Update 34401\tTP -169.607387\tHP -16.788628\tTA 0.932331\tHA 0.954887\t lr 0.006380\n",
      "    Update 34601\tTP -169.575855\tHP -16.771773\tTA 0.932331\tHA 0.954887\t lr 0.006278\n",
      "    Update 34801\tTP -169.538608\tHP -16.807969\tTA 0.932331\tHA 0.954887\t lr 0.006179\n",
      "    Update 35001\tTP -169.521003\tHP -16.821513\tTA 0.932331\tHA 0.954887\t lr 0.006081\n",
      "Train logP=-169.521003 Test logP=-16.821513 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.159324 TestLoss=0.126478\n",
      "----------------------------\n",
      "Epoch:34\n",
      "    Update 35201\tTP -169.488924\tHP -16.816691\tTA 0.932331\tHA 0.954887\t lr 0.005984\n",
      "    Update 35401\tTP -169.457413\tHP -16.779336\tTA 0.932331\tHA 0.954887\t lr 0.005889\n",
      "    Update 35601\tTP -169.438351\tHP -16.772078\tTA 0.932331\tHA 0.954887\t lr 0.005796\n",
      "    Update 35801\tTP -169.405597\tHP -16.765601\tTA 0.932331\tHA 0.954887\t lr 0.005704\n",
      "    Update 36001\tTP -169.384403\tHP -16.807318\tTA 0.932331\tHA 0.954887\t lr 0.005613\n",
      "Train logP=-169.384403 Test logP=-16.807318 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.159196 TestLoss=0.126371\n",
      "----------------------------\n",
      "Epoch:35\n",
      "    Update 36201\tTP -169.361816\tHP -16.822089\tTA 0.932331\tHA 0.954887\t lr 0.005524\n",
      "    Update 36401\tTP -169.324721\tHP -16.789417\tTA 0.932331\tHA 0.954887\t lr 0.005436\n",
      "    Update 36601\tTP -169.314889\tHP -16.765767\tTA 0.932331\tHA 0.954887\t lr 0.005350\n",
      "    Update 36801\tTP -169.278696\tHP -16.763828\tTA 0.932331\tHA 0.954887\t lr 0.005265\n",
      "    Update 37001\tTP -169.267157\tHP -16.809589\tTA 0.932331\tHA 0.954887\t lr 0.005181\n",
      "    Update 37201\tTP -169.231507\tHP -16.787562\tTA 0.932331\tHA 0.954887\t lr 0.005099\n",
      "Train logP=-169.231507 Test logP=-16.787562 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.159052 TestLoss=0.126222\n",
      "----------------------------\n",
      "Epoch:36\n",
      "    Update 37401\tTP -169.208749\tHP -16.776821\tTA 0.932331\tHA 0.954887\t lr 0.005018\n",
      "    Update 37601\tTP -169.189874\tHP -16.781339\tTA 0.932331\tHA 0.954887\t lr 0.004939\n",
      "    Update 37801\tTP -169.168945\tHP -16.758988\tTA 0.932331\tHA 0.954887\t lr 0.004860\n",
      "    Update 38001\tTP -169.143623\tHP -16.790733\tTA 0.932331\tHA 0.954887\t lr 0.004783\n",
      "    Update 38201\tTP -169.124623\tHP -16.786715\tTA 0.932331\tHA 0.954887\t lr 0.004707\n",
      "Train logP=-169.124623 Test logP=-16.786715 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.158952 TestLoss=0.126216\n",
      "----------------------------\n",
      "Epoch:37\n",
      "    Update 38401\tTP -169.103294\tHP -16.787069\tTA 0.932331\tHA 0.954887\t lr 0.004632\n",
      "    Update 38601\tTP -169.080669\tHP -16.777512\tTA 0.932331\tHA 0.954887\t lr 0.004559\n",
      "    Update 38801\tTP -169.067281\tHP -16.756265\tTA 0.932331\tHA 0.954887\t lr 0.004487\n",
      "    Update 39001\tTP -169.038856\tHP -16.759742\tTA 0.932331\tHA 0.954887\t lr 0.004415\n",
      "    Update 39201\tTP -169.023820\tHP -16.781782\tTA 0.932331\tHA 0.954887\t lr 0.004345\n",
      "Train logP=-169.023820 Test logP=-16.781782 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.158857 TestLoss=0.126179\n",
      "----------------------------\n",
      "Epoch:38\n",
      "    Update 39401\tTP -169.008291\tHP -16.798494\tTA 0.932331\tHA 0.954887\t lr 0.004276\n",
      "    Update 39601\tTP -168.981669\tHP -16.766582\tTA 0.932331\tHA 0.954887\t lr 0.004208\n",
      "    Update 39801\tTP -168.969716\tHP -16.756923\tTA 0.932331\tHA 0.954887\t lr 0.004142\n",
      "    Update 40001\tTP -168.945971\tHP -16.750839\tTA 0.932331\tHA 0.954887\t lr 0.004076\n",
      "    Update 40201\tTP -168.933298\tHP -16.783516\tTA 0.932331\tHA 0.954887\t lr 0.004011\n",
      "    Update 40401\tTP -168.910631\tHP -16.769411\tTA 0.932331\tHA 0.954887\t lr 0.003948\n",
      "Train logP=-168.910631 Test logP=-16.769411 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.158751 TestLoss=0.126086\n",
      "----------------------------\n",
      "Epoch:39\n",
      "    Update 40601\tTP -168.892979\tHP -16.761768\tTA 0.932331\tHA 0.954887\t lr 0.003885\n",
      "    Update 40801\tTP -168.879103\tHP -16.762778\tTA 0.932331\tHA 0.954887\t lr 0.003823\n",
      "    Update 41001\tTP -168.864020\tHP -16.744762\tTA 0.932331\tHA 0.954887\t lr 0.003763\n",
      "    Update 41201\tTP -168.843548\tHP -16.772708\tTA 0.932331\tHA 0.954887\t lr 0.003703\n",
      "    Update 41401\tTP -168.830109\tHP -16.773104\tTA 0.932331\tHA 0.954887\t lr 0.003644\n",
      "Train logP=-168.830109 Test logP=-16.773104 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.158675 TestLoss=0.126114\n",
      "----------------------------\n",
      "Epoch:40\n",
      "    Update 41601\tTP -168.812389\tHP -16.769120\tTA 0.932331\tHA 0.954887\t lr 0.003586\n",
      "    Update 41801\tTP -168.796175\tHP -16.759082\tTA 0.932331\tHA 0.954887\t lr 0.003529\n",
      "    Update 42001\tTP -168.784978\tHP -16.744784\tTA 0.932331\tHA 0.954887\t lr 0.003473\n",
      "    Update 42201\tTP -168.763660\tHP -16.752365\tTA 0.932331\tHA 0.954887\t lr 0.003418\n",
      "    Update 42401\tTP -168.751387\tHP -16.765935\tTA 0.932331\tHA 0.954887\t lr 0.003364\n",
      "Train logP=-168.751387 Test logP=-16.765935 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.158601 TestLoss=0.126060\n",
      "----------------------------\n",
      "Epoch:41\n",
      "    Update 42601\tTP -168.737269\tHP -16.773773\tTA 0.932331\tHA 0.954887\t lr 0.003310\n",
      "    Update 42801\tTP -168.720530\tHP -16.757215\tTA 0.932331\tHA 0.954887\t lr 0.003258\n",
      "    Update 43001\tTP -168.711208\tHP -16.745431\tTA 0.932331\tHA 0.954887\t lr 0.003206\n",
      "    Update 43201\tTP -168.693100\tHP -16.740420\tTA 0.932331\tHA 0.954887\t lr 0.003155\n",
      "    Update 43401\tTP -168.682526\tHP -16.768177\tTA 0.932331\tHA 0.954887\t lr 0.003105\n",
      "    Update 43601\tTP -168.668925\tHP -16.772814\tTA 0.932331\tHA 0.954887\t lr 0.003056\n",
      "Early Stop\n",
      "Train logP=-168.668925 Test logP=-16.772814 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.158523 TestLoss=0.126111\n",
      "Figure(1000x400)\n",
      "Saveing weights for epoch=40, Least Test Loss=0.126060\n",
      "Train logP=-168.751387 Test logP=-16.765935 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.158601 TestLoss=0.126060\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=100  --step=.1 --early_stop=8 --normalize=no \\\n",
    "    --log=yes --log_step=200 --plot_name='selected_features_no_normalization_with_scheduler_exp' \\\n",
    "    --save_weights_path='weights/selected_features_no_normalization_with_scheduler_exp' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy' \\\n",
    "    --learning_rate_scheduler='exp' --learning_rate_scheduler_params='{\"decay\":8e-5}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1cbe4-3c8d-4429-919f-38b51a626054",
   "metadata": {},
   "source": [
    "![selected no normalization exp](figures/selected_features_no_normalization_with_scheduler_exp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a050f7c-daec-4370-b667-e5d6dc5241da",
   "metadata": {},
   "source": [
    "### Training Selected Features with normalization with Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55847a0d-3dc3-4d26-a153-8a56538c19bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 1064 train and 133 test\n",
      "Epoch:1\n",
      "    Update 1\tTP -758.338190\tHP -91.177060\tTA 0.497180\tHA 0.533835\t lr 0.899991\n",
      "    Update 251\tTP -302.103660\tHP -27.012109\tTA 0.878759\tHA 0.917293\t lr 0.897744\n",
      "    Update 501\tTP -336.027331\tHP -39.111468\tTA 0.875940\tHA 0.857143\t lr 0.895502\n",
      "    Update 751\tTP -207.871286\tHP -22.830711\tTA 0.906955\tHA 0.924812\t lr 0.893266\n",
      "    Update 1001\tTP -182.635101\tHP -20.735672\tTA 0.921053\tHA 0.954887\t lr 0.891036\n",
      "Train logP=-182.635101 Test logP=-20.735672 Train Acc=0.921053 Test Acc=0.954887 TrainLoss=0.171650 TestLoss=0.155907\n",
      "----------------------------\n",
      "Epoch:2\n",
      "    Update 1251\tTP -174.748964\tHP -17.714307\tTA 0.921992\tHA 0.962406\t lr 0.888811\n",
      "    Update 1501\tTP -174.603311\tHP -20.818863\tTA 0.924812\tHA 0.939850\t lr 0.886592\n",
      "    Update 1751\tTP -186.219311\tHP -22.405427\tTA 0.916353\tHA 0.939850\t lr 0.884378\n",
      "    Update 2001\tTP -179.032657\tHP -25.467892\tTA 0.923872\tHA 0.917293\t lr 0.882170\n",
      "Train logP=-179.032657 Test logP=-25.467892 Train Acc=0.923872 Test Acc=0.917293 TrainLoss=0.168264 TestLoss=0.191488\n",
      "----------------------------\n",
      "Epoch:3\n",
      "    Update 2251\tTP -178.528321\tHP -21.138280\tTA 0.923872\tHA 0.939850\t lr 0.879967\n",
      "    Update 2501\tTP -177.369625\tHP -24.568762\tTA 0.897556\tHA 0.887218\t lr 0.877770\n",
      "    Update 2751\tTP -171.327376\tHP -20.463771\tTA 0.922932\tHA 0.924812\t lr 0.875578\n",
      "    Update 3001\tTP -165.638225\tHP -23.022108\tTA 0.929511\tHA 0.932331\t lr 0.873392\n",
      "Train logP=-165.638225 Test logP=-23.022108 Train Acc=0.929511 Test Acc=0.932331 TrainLoss=0.155675 TestLoss=0.173099\n",
      "----------------------------\n",
      "Epoch:4\n",
      "    Update 3251\tTP -169.788307\tHP -20.674089\tTA 0.927632\tHA 0.954887\t lr 0.871211\n",
      "    Update 3501\tTP -161.859016\tHP -19.840438\tTA 0.926692\tHA 0.932331\t lr 0.869036\n",
      "    Update 3751\tTP -172.345698\tHP -24.990889\tTA 0.906015\tHA 0.902256\t lr 0.866866\n",
      "    Update 4001\tTP -154.212895\tHP -21.602050\tTA 0.929511\tHA 0.947368\t lr 0.864702\n",
      "    Update 4251\tTP -301.806119\tHP -47.080592\tTA 0.864662\tHA 0.834586\t lr 0.862543\n",
      "Train logP=-301.806119 Test logP=-47.080592 Train Acc=0.864662 Test Acc=0.834586 TrainLoss=0.283652 TestLoss=0.353989\n",
      "----------------------------\n",
      "Epoch:5\n",
      "    Update 4501\tTP -157.935154\tHP -19.946292\tTA 0.927632\tHA 0.939850\t lr 0.860389\n",
      "    Update 4751\tTP -156.747394\tHP -18.555935\tTA 0.930451\tHA 0.954887\t lr 0.858241\n",
      "    Update 5001\tTP -201.719342\tHP -29.639923\tTA 0.902256\tHA 0.902256\t lr 0.856098\n",
      "    Update 5251\tTP -159.600353\tHP -20.911796\tTA 0.927632\tHA 0.917293\t lr 0.853960\n",
      "Train logP=-159.600353 Test logP=-20.911796 Train Acc=0.927632 Test Acc=0.917293 TrainLoss=0.150000 TestLoss=0.157232\n",
      "----------------------------\n",
      "Epoch:6\n",
      "    Update 5501\tTP -153.992469\tHP -20.573330\tTA 0.926692\tHA 0.954887\t lr 0.851828\n",
      "    Update 5751\tTP -148.566517\tHP -19.142629\tTA 0.935150\tHA 0.969925\t lr 0.849701\n",
      "    Update 6001\tTP -158.683859\tHP -19.991612\tTA 0.928571\tHA 0.947368\t lr 0.847580\n",
      "    Update 6251\tTP -159.356188\tHP -20.128098\tTA 0.925752\tHA 0.939850\t lr 0.845463\n",
      "Train logP=-159.356188 Test logP=-20.128098 Train Acc=0.925752 Test Acc=0.939850 TrainLoss=0.149771 TestLoss=0.151339\n",
      "----------------------------\n",
      "Epoch:7\n",
      "    Update 6501\tTP -169.282107\tHP -19.809813\tTA 0.928571\tHA 0.947368\t lr 0.843352\n",
      "    Update 6751\tTP -167.725309\tHP -24.696773\tTA 0.901316\tHA 0.902256\t lr 0.841247\n",
      "    Update 7001\tTP -159.535292\tHP -18.300720\tTA 0.927632\tHA 0.969925\t lr 0.839146\n",
      "    Update 7251\tTP -160.854542\tHP -24.718076\tTA 0.907895\tHA 0.902256\t lr 0.837051\n",
      "Train logP=-160.854542 Test logP=-24.718076 Train Acc=0.907895 Test Acc=0.902256 TrainLoss=0.151179 TestLoss=0.185850\n",
      "----------------------------\n",
      "Epoch:8\n",
      "    Update 7501\tTP -154.520729\tHP -20.502313\tTA 0.929511\tHA 0.939850\t lr 0.834961\n",
      "    Update 7751\tTP -152.402587\tHP -21.215484\tTA 0.926692\tHA 0.947368\t lr 0.832876\n",
      "    Update 8001\tTP -169.434845\tHP -22.537823\tTA 0.906955\tHA 0.902256\t lr 0.830796\n",
      "    Update 8251\tTP -149.181572\tHP -21.757043\tTA 0.932331\tHA 0.954887\t lr 0.828722\n",
      "    Update 8501\tTP -247.172965\tHP -39.888562\tTA 0.882519\tHA 0.849624\t lr 0.826653\n",
      "Train logP=-247.172965 Test logP=-39.888562 Train Acc=0.882519 Test Acc=0.849624 TrainLoss=0.232305 TestLoss=0.299914\n",
      "----------------------------\n",
      "Epoch:9\n",
      "    Update 8751\tTP -146.817431\tHP -20.152050\tTA 0.927632\tHA 0.947368\t lr 0.824589\n",
      "    Update 9001\tTP -172.229937\tHP -20.403781\tTA 0.923872\tHA 0.962406\t lr 0.822530\n",
      "    Update 9251\tTP -196.689642\tHP -29.221960\tTA 0.902256\tHA 0.902256\t lr 0.820476\n",
      "    Update 9501\tTP -154.987676\tHP -23.784452\tTA 0.929511\tHA 0.947368\t lr 0.818427\n",
      "Train logP=-154.987676 Test logP=-23.784452 Train Acc=0.929511 Test Acc=0.947368 TrainLoss=0.145665 TestLoss=0.178830\n",
      "----------------------------\n",
      "Epoch:10\n",
      "    Update 9751\tTP -150.742460\tHP -18.944807\tTA 0.931391\tHA 0.939850\t lr 0.816384\n",
      "    Update 10001\tTP -149.035232\tHP -18.519535\tTA 0.932331\tHA 0.954887\t lr 0.814346\n",
      "    Update 10251\tTP -154.549112\tHP -19.192620\tTA 0.930451\tHA 0.954887\t lr 0.812312\n",
      "    Update 10501\tTP -159.535646\tHP -19.882066\tTA 0.928571\tHA 0.939850\t lr 0.810284\n",
      "Train logP=-159.535646 Test logP=-19.882066 Train Acc=0.928571 Test Acc=0.939850 TrainLoss=0.149940 TestLoss=0.149489\n",
      "----------------------------\n",
      "Epoch:11\n",
      "    Update 10751\tTP -152.676513\tHP -18.798079\tTA 0.933271\tHA 0.947368\t lr 0.808261\n",
      "    Update 11001\tTP -182.446497\tHP -27.492785\tTA 0.897556\tHA 0.894737\t lr 0.806243\n",
      "    Update 11251\tTP -157.041254\tHP -18.399199\tTA 0.928571\tHA 0.969925\t lr 0.804230\n",
      "    Update 11501\tTP -158.109664\tHP -23.952262\tTA 0.910714\tHA 0.902256\t lr 0.802222\n",
      "Train logP=-158.109664 Test logP=-23.952262 Train Acc=0.910714 Test Acc=0.902256 TrainLoss=0.148599 TestLoss=0.180092\n",
      "----------------------------\n",
      "Epoch:12\n",
      "    Update 11751\tTP -145.637644\tHP -19.759176\tTA 0.932331\tHA 0.947368\t lr 0.800218\n",
      "    Update 12001\tTP -164.345509\tHP -25.802571\tTA 0.926692\tHA 0.932331\t lr 0.798220\n",
      "    Update 12251\tTP -160.198585\tHP -20.508166\tTA 0.906015\tHA 0.917293\t lr 0.796227\n",
      "    Update 12501\tTP -159.008081\tHP -23.523615\tTA 0.908835\tHA 0.902256\t lr 0.794239\n",
      "    Update 12751\tTP -202.577984\tHP -32.428949\tTA 0.892857\tHA 0.849624\t lr 0.792256\n",
      "Train logP=-202.577984 Test logP=-32.428949 Train Acc=0.892857 Test Acc=0.849624 TrainLoss=0.190393 TestLoss=0.243827\n",
      "----------------------------\n",
      "Epoch:13\n",
      "    Update 13001\tTP -143.478411\tHP -18.771783\tTA 0.932331\tHA 0.962406\t lr 0.790278\n",
      "    Update 13251\tTP -162.966492\tHP -19.973636\tTA 0.929511\tHA 0.969925\t lr 0.788305\n",
      "    Update 13501\tTP -188.582115\tHP -27.426797\tTA 0.903195\tHA 0.902256\t lr 0.786336\n",
      "    Update 13751\tTP -143.423673\tHP -20.109266\tTA 0.930451\tHA 0.962406\t lr 0.784373\n",
      "Train logP=-143.423673 Test logP=-20.109266 Train Acc=0.930451 Test Acc=0.962406 TrainLoss=0.134797 TestLoss=0.151197\n",
      "----------------------------\n",
      "Epoch:14\n",
      "    Update 14001\tTP -147.073798\tHP -18.898749\tTA 0.935150\tHA 0.947368\t lr 0.782415\n",
      "    Update 14251\tTP -145.380548\tHP -20.309606\tTA 0.930451\tHA 0.962406\t lr 0.780461\n",
      "    Update 14501\tTP -158.059637\tHP -18.846276\tTA 0.927632\tHA 0.954887\t lr 0.778512\n",
      "    Update 14751\tTP -153.674122\tHP -19.805387\tTA 0.927632\tHA 0.947368\t lr 0.776568\n",
      "Train logP=-153.674122 Test logP=-19.805387 Train Acc=0.927632 Test Acc=0.947368 TrainLoss=0.144431 TestLoss=0.148913\n",
      "----------------------------\n",
      "Epoch:15\n",
      "    Update 15001\tTP -143.628285\tHP -18.850312\tTA 0.934211\tHA 0.954887\t lr 0.774629\n",
      "    Update 15251\tTP -159.771807\tHP -24.128046\tTA 0.906015\tHA 0.909774\t lr 0.772695\n",
      "    Update 15501\tTP -159.387653\tHP -20.422799\tTA 0.908835\tHA 0.917293\t lr 0.770766\n",
      "    Update 15751\tTP -153.218470\tHP -23.009626\tTA 0.910714\tHA 0.917293\t lr 0.768841\n",
      "Train logP=-153.218470 Test logP=-23.009626 Train Acc=0.910714 Test Acc=0.917293 TrainLoss=0.144002 TestLoss=0.173005\n",
      "----------------------------\n",
      "Epoch:16\n",
      "    Update 16001\tTP -149.577982\tHP -20.005823\tTA 0.931391\tHA 0.947368\t lr 0.766922\n",
      "    Update 16251\tTP -142.678899\tHP -19.313212\tTA 0.932331\tHA 0.962406\t lr 0.765007\n",
      "    Update 16501\tTP -151.582592\tHP -19.120596\tTA 0.929511\tHA 0.969925\t lr 0.763097\n",
      "    Update 16751\tTP -157.517221\tHP -23.368830\tTA 0.908835\tHA 0.902256\t lr 0.761191\n",
      "    Update 17001\tTP -172.688277\tHP -27.685590\tTA 0.902256\tHA 0.887218\t lr 0.759291\n",
      "Train logP=-172.688277 Test logP=-27.685590 Train Acc=0.902256 Test Acc=0.887218 TrainLoss=0.162301 TestLoss=0.208162\n",
      "----------------------------\n",
      "Epoch:17\n",
      "    Update 17251\tTP -137.155126\tHP -19.285325\tTA 0.933271\tHA 0.969925\t lr 0.757395\n",
      "    Update 17501\tTP -154.992689\tHP -19.904445\tTA 0.906955\tHA 0.924812\t lr 0.755504\n",
      "    Update 17751\tTP -147.701833\tHP -19.875341\tTA 0.932331\tHA 0.962406\t lr 0.753617\n",
      "    Update 18001\tTP -144.683309\tHP -19.685214\tTA 0.931391\tHA 0.962406\t lr 0.751736\n",
      "Train logP=-144.683309 Test logP=-19.685214 Train Acc=0.931391 Test Acc=0.962406 TrainLoss=0.135981 TestLoss=0.148009\n",
      "----------------------------\n",
      "Epoch:18\n",
      "    Update 18251\tTP -143.056403\tHP -19.205954\tTA 0.935150\tHA 0.954887\t lr 0.749859\n",
      "    Update 18501\tTP -140.332790\tHP -19.338859\tTA 0.932331\tHA 0.969925\t lr 0.747986\n",
      "    Update 18751\tTP -144.418196\tHP -19.648568\tTA 0.932331\tHA 0.954887\t lr 0.746119\n",
      "    Update 19001\tTP -148.731941\tHP -19.685593\tTA 0.929511\tHA 0.962406\t lr 0.744256\n",
      "Train logP=-148.731941 Test logP=-19.685593 Train Acc=0.929511 Test Acc=0.962406 TrainLoss=0.139786 TestLoss=0.148012\n",
      "----------------------------\n",
      "Epoch:19\n",
      "    Update 19251\tTP -151.376991\tHP -19.790741\tTA 0.932331\tHA 0.947368\t lr 0.742397\n",
      "    Update 19501\tTP -144.177935\tHP -21.085855\tTA 0.930451\tHA 0.962406\t lr 0.740544\n",
      "    Update 19751\tTP -159.137206\tHP -21.067855\tTA 0.907895\tHA 0.917293\t lr 0.738695\n",
      "    Update 20001\tTP -156.780064\tHP -24.157438\tTA 0.908835\tHA 0.909774\t lr 0.736850\n",
      "Train logP=-156.780064 Test logP=-24.157438 Train Acc=0.908835 Test Acc=0.909774 TrainLoss=0.147350 TestLoss=0.181635\n",
      "----------------------------\n",
      "Epoch:20\n",
      "    Update 20251\tTP -136.632145\tHP -19.952895\tTA 0.932331\tHA 0.962406\t lr 0.735010\n",
      "    Update 20501\tTP -142.002212\tHP -19.492193\tTA 0.930451\tHA 0.954887\t lr 0.733175\n",
      "    Update 20751\tTP -142.108725\tHP -18.624892\tTA 0.930451\tHA 0.969925\t lr 0.731345\n",
      "    Update 21001\tTP -136.798357\tHP -20.012879\tTA 0.933271\tHA 0.969925\t lr 0.729519\n",
      "    Update 21251\tTP -142.647954\tHP -21.937902\tTA 0.930451\tHA 0.954887\t lr 0.727697\n",
      "Train logP=-142.647954 Test logP=-21.937902 Train Acc=0.930451 Test Acc=0.954887 TrainLoss=0.134068 TestLoss=0.164947\n",
      "----------------------------\n",
      "Epoch:21\n",
      "    Update 21501\tTP -137.003131\tHP -19.388060\tTA 0.933271\tHA 0.969925\t lr 0.725880\n",
      "    Update 21751\tTP -147.007652\tHP -21.522539\tTA 0.908835\tHA 0.924812\t lr 0.724068\n",
      "    Update 22001\tTP -164.239020\tHP -24.233818\tTA 0.907895\tHA 0.902256\t lr 0.722260\n",
      "    Update 22251\tTP -142.240943\tHP -20.657442\tTA 0.933271\tHA 0.962406\t lr 0.720456\n",
      "Train logP=-142.240943 Test logP=-20.657442 Train Acc=0.933271 Test Acc=0.962406 TrainLoss=0.133685 TestLoss=0.155319\n",
      "----------------------------\n",
      "Epoch:22\n",
      "    Update 22501\tTP -149.653214\tHP -19.700468\tTA 0.930451\tHA 0.954887\t lr 0.718657\n",
      "    Update 22751\tTP -147.034412\tHP -19.393865\tTA 0.933271\tHA 0.962406\t lr 0.716863\n",
      "    Update 23001\tTP -141.039044\tHP -19.575859\tTA 0.932331\tHA 0.962406\t lr 0.715073\n",
      "    Update 23251\tTP -147.124408\tHP -19.680565\tTA 0.929511\tHA 0.962406\t lr 0.713288\n",
      "Train logP=-147.124408 Test logP=-19.680565 Train Acc=0.929511 Test Acc=0.962406 TrainLoss=0.138275 TestLoss=0.147974\n",
      "----------------------------\n",
      "Epoch:23\n",
      "    Update 23501\tTP -137.577079\tHP -19.609364\tTA 0.935150\tHA 0.954887\t lr 0.711507\n",
      "    Update 23751\tTP -147.634105\tHP -20.127685\tTA 0.929511\tHA 0.962406\t lr 0.709730\n",
      "    Update 24001\tTP -150.930613\tHP -20.044633\tTA 0.930451\tHA 0.969925\t lr 0.707958\n",
      "    Update 24251\tTP -159.452399\tHP -25.026081\tTA 0.908835\tHA 0.909774\t lr 0.706190\n",
      "Train logP=-159.452399 Test logP=-25.026081 Train Acc=0.908835 Test Acc=0.909774 TrainLoss=0.149861 TestLoss=0.188166\n",
      "----------------------------\n",
      "Epoch:24\n",
      "    Update 24501\tTP -133.901634\tHP -20.671427\tTA 0.932331\tHA 0.977444\t lr 0.704427\n",
      "    Update 24751\tTP -141.538587\tHP -19.786147\tTA 0.931391\tHA 0.954887\t lr 0.702668\n",
      "    Update 25001\tTP -139.287454\tHP -18.943200\tTA 0.932331\tHA 0.977444\t lr 0.700914\n",
      "    Update 25251\tTP -140.568345\tHP -21.438669\tTA 0.911654\tHA 0.924812\t lr 0.699164\n",
      "    Update 25501\tTP -146.237228\tHP -19.975145\tTA 0.933271\tHA 0.954887\t lr 0.697418\n",
      "Train logP=-146.237228 Test logP=-19.975145 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.137441 TestLoss=0.150189\n",
      "----------------------------\n",
      "Epoch:25\n",
      "    Update 25751\tTP -137.613930\tHP -19.685665\tTA 0.936090\tHA 0.962406\t lr 0.695677\n",
      "    Update 26001\tTP -137.064753\tHP -19.865577\tTA 0.934211\tHA 0.969925\t lr 0.693939\n",
      "    Update 26251\tTP -159.227209\tHP -23.654597\tTA 0.907895\tHA 0.902256\t lr 0.692207\n",
      "    Update 26501\tTP -147.543835\tHP -20.157103\tTA 0.930451\tHA 0.962406\t lr 0.690478\n",
      "Train logP=-147.543835 Test logP=-20.157103 Train Acc=0.930451 Test Acc=0.962406 TrainLoss=0.138669 TestLoss=0.151557\n",
      "----------------------------\n",
      "Epoch:26\n",
      "    Update 26751\tTP -142.310038\tHP -19.828142\tTA 0.931391\tHA 0.962406\t lr 0.688754\n",
      "    Update 27001\tTP -144.460808\tHP -19.689086\tTA 0.933271\tHA 0.962406\t lr 0.687035\n",
      "    Update 27251\tTP -149.371794\tHP -23.033246\tTA 0.907895\tHA 0.909774\t lr 0.685319\n",
      "    Update 27501\tTP -145.001888\tHP -19.891333\tTA 0.929511\tHA 0.962406\t lr 0.683608\n",
      "Train logP=-145.001888 Test logP=-19.891333 Train Acc=0.929511 Test Acc=0.962406 TrainLoss=0.136280 TestLoss=0.149559\n",
      "----------------------------\n",
      "Epoch:27\n",
      "    Update 27751\tTP -133.501574\tHP -20.061174\tTA 0.936090\tHA 0.962406\t lr 0.681901\n",
      "    Update 28001\tTP -157.930260\tHP -20.806872\tTA 0.930451\tHA 0.947368\t lr 0.680199\n",
      "    Update 28251\tTP -147.708817\tHP -20.614328\tTA 0.908835\tHA 0.924812\t lr 0.678500\n",
      "    Update 28501\tTP -152.713534\tHP -23.881518\tTA 0.908835\tHA 0.909774\t lr 0.676806\n",
      "Train logP=-152.713534 Test logP=-23.881518 Train Acc=0.908835 Test Acc=0.909774 TrainLoss=0.143528 TestLoss=0.179560\n",
      "----------------------------\n",
      "Epoch:28\n",
      "    Update 28751\tTP -143.581779\tHP -23.349594\tTA 0.902256\tHA 0.924812\t lr 0.675116\n",
      "    Update 29001\tTP -151.378817\tHP -20.322506\tTA 0.930451\tHA 0.962406\t lr 0.673430\n",
      "    Update 29251\tTP -138.418524\tHP -19.379611\tTA 0.931391\tHA 0.977444\t lr 0.671749\n",
      "    Update 29501\tTP -154.208175\tHP -23.917732\tTA 0.909774\tHA 0.917293\t lr 0.670072\n",
      "    Update 29751\tTP -140.540530\tHP -19.988720\tTA 0.936090\tHA 0.962406\t lr 0.668399\n",
      "Train logP=-140.540530 Test logP=-19.988720 Train Acc=0.936090 Test Acc=0.962406 TrainLoss=0.132087 TestLoss=0.150291\n",
      "----------------------------\n",
      "Epoch:29\n",
      "    Update 30001\tTP -133.464686\tHP -20.003687\tTA 0.933271\tHA 0.969925\t lr 0.666730\n",
      "    Update 30251\tTP -135.912244\tHP -20.025257\tTA 0.934211\tHA 0.969925\t lr 0.665065\n",
      "    Update 30501\tTP -154.530081\tHP -23.141976\tTA 0.908835\tHA 0.909774\t lr 0.663404\n",
      "    Update 30751\tTP -138.271326\tHP -20.979796\tTA 0.934211\tHA 0.969925\t lr 0.661748\n",
      "Train logP=-138.271326 Test logP=-20.979796 Train Acc=0.934211 Test Acc=0.969925 TrainLoss=0.129954 TestLoss=0.157743\n",
      "----------------------------\n",
      "Epoch:30\n",
      "    Update 31001\tTP -134.122925\tHP -20.130081\tTA 0.933271\tHA 0.969925\t lr 0.660096\n",
      "    Update 31251\tTP -138.001233\tHP -19.861771\tTA 0.933271\tHA 0.962406\t lr 0.658447\n",
      "    Update 31501\tTP -141.136676\tHP -21.315082\tTA 0.932331\tHA 0.969925\t lr 0.656803\n",
      "    Update 31751\tTP -135.795355\tHP -20.427966\tTA 0.935150\tHA 0.969925\t lr 0.655163\n",
      "Train logP=-135.795355 Test logP=-20.427966 Train Acc=0.935150 Test Acc=0.969925 TrainLoss=0.127627 TestLoss=0.153594\n",
      "----------------------------\n",
      "Epoch:31\n",
      "    Update 32001\tTP -146.441952\tHP -20.882025\tTA 0.931391\tHA 0.954887\t lr 0.653528\n",
      "    Update 32251\tTP -145.120256\tHP -20.418526\tTA 0.929511\tHA 0.962406\t lr 0.651896\n",
      "    Update 32501\tTP -140.180541\tHP -19.700403\tTA 0.931391\tHA 0.969925\t lr 0.650268\n",
      "    Update 32751\tTP -166.296930\tHP -26.035157\tTA 0.908835\tHA 0.909774\t lr 0.648644\n",
      "Train logP=-166.296930 Test logP=-26.035157 Train Acc=0.908835 Test Acc=0.909774 TrainLoss=0.156294 TestLoss=0.195753\n",
      "----------------------------\n",
      "Epoch:32\n",
      "    Update 33001\tTP -152.561796\tHP -25.092091\tTA 0.902256\tHA 0.909774\t lr 0.647025\n",
      "    Update 33251\tTP -144.172312\tHP -20.301707\tTA 0.929511\tHA 0.962406\t lr 0.645409\n",
      "    Update 33501\tTP -138.426257\tHP -20.363437\tTA 0.933271\tHA 0.977444\t lr 0.643798\n",
      "    Update 33751\tTP -142.692483\tHP -22.418487\tTA 0.908835\tHA 0.917293\t lr 0.642190\n",
      "    Update 34001\tTP -161.692125\tHP -21.441003\tTA 0.931391\tHA 0.947368\t lr 0.640587\n",
      "Train logP=-161.692125 Test logP=-21.441003 Train Acc=0.931391 Test Acc=0.947368 TrainLoss=0.151966 TestLoss=0.161211\n",
      "----------------------------\n",
      "Epoch:33\n",
      "    Update 34251\tTP -131.888045\tHP -20.564887\tTA 0.933271\tHA 0.977444\t lr 0.638987\n",
      "    Update 34501\tTP -135.346992\tHP -20.703527\tTA 0.935150\tHA 0.969925\t lr 0.637392\n",
      "    Update 34751\tTP -142.914441\tHP -21.527370\tTA 0.907895\tHA 0.924812\t lr 0.635800\n",
      "    Update 35001\tTP -142.596993\tHP -22.264033\tTA 0.910714\tHA 0.924812\t lr 0.634213\n",
      "Train logP=-142.596993 Test logP=-22.264033 Train Acc=0.910714 Test Acc=0.924812 TrainLoss=0.134020 TestLoss=0.167399\n",
      "----------------------------\n",
      "Epoch:34\n",
      "    Update 35251\tTP -139.258192\tHP -20.492014\tTA 0.933271\tHA 0.954887\t lr 0.632629\n",
      "    Update 35501\tTP -134.451208\tHP -20.401310\tTA 0.932331\tHA 0.969925\t lr 0.631050\n",
      "    Update 35751\tTP -144.960026\tHP -19.797669\tTA 0.932331\tHA 0.954887\t lr 0.629474\n",
      "    Update 36001\tTP -135.382385\tHP -20.641201\tTA 0.935150\tHA 0.969925\t lr 0.627902\n",
      "Train logP=-135.382385 Test logP=-20.641201 Train Acc=0.935150 Test Acc=0.969925 TrainLoss=0.127239 TestLoss=0.155197\n",
      "----------------------------\n",
      "Epoch:35\n",
      "    Update 36251\tTP -137.996872\tHP -20.787425\tTA 0.934211\tHA 0.954887\t lr 0.626335\n",
      "    Update 36501\tTP -167.990026\tHP -22.294033\tTA 0.928571\tHA 0.947368\t lr 0.624771\n",
      "    Update 36751\tTP -142.854524\tHP -21.269119\tTA 0.910714\tHA 0.924812\t lr 0.623211\n",
      "    Update 37001\tTP -162.153195\tHP -25.503189\tTA 0.908835\tHA 0.909774\t lr 0.621655\n",
      "Train logP=-162.153195 Test logP=-25.503189 Train Acc=0.908835 Test Acc=0.909774 TrainLoss=0.152400 TestLoss=0.191753\n",
      "----------------------------\n",
      "Epoch:36\n",
      "    Update 37251\tTP -170.605478\tHP -28.303729\tTA 0.902256\tHA 0.902256\t lr 0.620102\n",
      "    Update 37501\tTP -141.993391\tHP -20.463123\tTA 0.929511\tHA 0.962406\t lr 0.618554\n",
      "    Update 37751\tTP -138.841967\tHP -20.901603\tTA 0.934211\tHA 0.969925\t lr 0.617010\n",
      "    Update 38001\tTP -142.173554\tHP -22.596073\tTA 0.908835\tHA 0.917293\t lr 0.615469\n",
      "    Update 38251\tTP -155.702146\tHP -21.232647\tTA 0.932331\tHA 0.954887\t lr 0.613932\n",
      "Train logP=-155.702146 Test logP=-21.232647 Train Acc=0.932331 Test Acc=0.954887 TrainLoss=0.146337 TestLoss=0.159644\n",
      "----------------------------\n",
      "Epoch:37\n",
      "    Update 38501\tTP -133.494323\tHP -21.470557\tTA 0.932331\tHA 0.969925\t lr 0.612399\n",
      "    Update 38751\tTP -134.683237\tHP -20.291740\tTA 0.934211\tHA 0.962406\t lr 0.610870\n",
      "    Update 39001\tTP -138.065789\tHP -20.553895\tTA 0.931391\tHA 0.969925\t lr 0.609345\n",
      "    Update 39251\tTP -135.605349\tHP -20.331904\tTA 0.933271\tHA 0.977444\t lr 0.607824\n",
      "Train logP=-135.605349 Test logP=-20.331904 Train Acc=0.933271 Test Acc=0.977444 TrainLoss=0.127449 TestLoss=0.152871\n",
      "----------------------------\n",
      "Epoch:38\n",
      "    Update 39501\tTP -134.180157\tHP -20.582683\tTA 0.935150\tHA 0.954887\t lr 0.606306\n",
      "    Update 39751\tTP -133.701158\tHP -20.693035\tTA 0.932331\tHA 0.969925\t lr 0.604792\n",
      "    Update 40001\tTP -139.024092\tHP -19.811182\tTA 0.933271\tHA 0.954887\t lr 0.603282\n",
      "    Update 40251\tTP -136.541324\tHP -20.342070\tTA 0.931391\tHA 0.962406\t lr 0.601776\n",
      "Train logP=-136.541324 Test logP=-20.342070 Train Acc=0.931391 Test Acc=0.962406 TrainLoss=0.128328 TestLoss=0.152948\n",
      "----------------------------\n",
      "Epoch:39\n",
      "    Update 40501\tTP -136.823065\tHP -20.955360\tTA 0.934211\tHA 0.954887\t lr 0.600273\n",
      "    Update 40751\tTP -134.037864\tHP -21.391685\tTA 0.932331\tHA 0.969925\t lr 0.598774\n",
      "    Update 41001\tTP -136.259092\tHP -20.348790\tTA 0.933271\tHA 0.977444\t lr 0.597279\n",
      "    Update 41251\tTP -148.930958\tHP -23.691608\tTA 0.908835\tHA 0.917293\t lr 0.595788\n",
      "Train logP=-148.930958 Test logP=-23.691608 Train Acc=0.908835 Test Acc=0.917293 TrainLoss=0.139973 TestLoss=0.178132\n",
      "----------------------------\n",
      "Epoch:40\n",
      "    Update 41501\tTP -155.081381\tHP -26.096717\tTA 0.903195\tHA 0.902256\t lr 0.594300\n",
      "    Update 41751\tTP -140.745750\tHP -20.645600\tTA 0.929511\tHA 0.962406\t lr 0.592816\n",
      "    Update 42001\tTP -135.490168\tHP -20.544596\tTA 0.934211\tHA 0.977444\t lr 0.591336\n",
      "    Update 42251\tTP -135.221817\tHP -21.444086\tTA 0.932331\tHA 0.969925\t lr 0.589860\n",
      "    Update 42501\tTP -144.507334\tHP -20.755339\tTA 0.933271\tHA 0.954887\t lr 0.588387\n",
      "Train logP=-144.507334 Test logP=-20.755339 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.135815 TestLoss=0.156055\n",
      "----------------------------\n",
      "Epoch:41\n",
      "    Update 42751\tTP -131.248810\tHP -21.134464\tTA 0.933271\tHA 0.977444\t lr 0.586918\n",
      "    Update 43001\tTP -132.719972\tHP -20.717910\tTA 0.936090\tHA 0.969925\t lr 0.585452\n",
      "    Update 43251\tTP -136.098504\tHP -20.355057\tTA 0.931391\tHA 0.969925\t lr 0.583991\n",
      "    Update 43501\tTP -135.455634\tHP -20.389981\tTA 0.932331\tHA 0.969925\t lr 0.582532\n",
      "Train logP=-135.455634 Test logP=-20.389981 Train Acc=0.932331 Test Acc=0.969925 TrainLoss=0.127308 TestLoss=0.153308\n",
      "----------------------------\n",
      "Epoch:42\n",
      "    Update 43751\tTP -131.114747\tHP -20.848416\tTA 0.933271\tHA 0.969925\t lr 0.581078\n",
      "    Update 44001\tTP -134.698427\tHP -21.506282\tTA 0.933271\tHA 0.969925\t lr 0.579627\n",
      "    Update 44251\tTP -137.763504\tHP -20.003209\tTA 0.933271\tHA 0.954887\t lr 0.578180\n",
      "    Update 44501\tTP -134.252037\tHP -20.822869\tTA 0.933271\tHA 0.969925\t lr 0.576736\n",
      "Train logP=-134.252037 Test logP=-20.822869 Train Acc=0.933271 Test Acc=0.969925 TrainLoss=0.126177 TestLoss=0.156563\n",
      "----------------------------\n",
      "Epoch:43\n",
      "    Update 44751\tTP -132.728903\tHP -21.135701\tTA 0.935150\tHA 0.954887\t lr 0.575296\n",
      "    Update 45001\tTP -133.171794\tHP -21.592572\tTA 0.932331\tHA 0.969925\t lr 0.573860\n",
      "    Update 45251\tTP -138.366374\tHP -21.344308\tTA 0.934211\tHA 0.969925\t lr 0.572427\n",
      "    Update 45501\tTP -141.337221\tHP -22.545865\tTA 0.908835\tHA 0.917293\t lr 0.570997\n",
      "    Update 45751\tTP -173.968464\tHP -29.072254\tTA 0.901316\tHA 0.902256\t lr 0.569572\n",
      "Train logP=-173.968464 Test logP=-29.072254 Train Acc=0.901316 Test Acc=0.902256 TrainLoss=0.163504 TestLoss=0.218588\n",
      "----------------------------\n",
      "Epoch:44\n",
      "    Update 46001\tTP -136.137311\tHP -20.727675\tTA 0.930451\tHA 0.962406\t lr 0.568150\n",
      "    Update 46251\tTP -134.473456\tHP -20.747357\tTA 0.933271\tHA 0.977444\t lr 0.566731\n",
      "    Update 46501\tTP -134.858410\tHP -21.679196\tTA 0.934211\tHA 0.969925\t lr 0.565316\n",
      "    Update 46751\tTP -137.077462\tHP -20.635914\tTA 0.935150\tHA 0.962406\t lr 0.563904\n",
      "Train logP=-137.077462 Test logP=-20.635914 Train Acc=0.935150 Test Acc=0.962406 TrainLoss=0.128832 TestLoss=0.155157\n",
      "----------------------------\n",
      "Epoch:45\n",
      "    Update 47001\tTP -130.792237\tHP -20.915658\tTA 0.933271\tHA 0.977444\t lr 0.562496\n",
      "    Update 47251\tTP -134.804216\tHP -20.636075\tTA 0.934211\tHA 0.962406\t lr 0.561092\n",
      "    Update 47501\tTP -138.631666\tHP -20.311589\tTA 0.929511\tHA 0.962406\t lr 0.559691\n",
      "    Update 47751\tTP -134.820270\tHP -20.547699\tTA 0.932331\tHA 0.969925\t lr 0.558293\n",
      "Train logP=-134.820270 Test logP=-20.547699 Train Acc=0.932331 Test Acc=0.969925 TrainLoss=0.126711 TestLoss=0.154494\n",
      "----------------------------\n",
      "Epoch:46\n",
      "    Update 48001\tTP -130.733811\tHP -21.050916\tTA 0.933271\tHA 0.969925\t lr 0.556899\n",
      "    Update 48251\tTP -151.118326\tHP -25.015942\tTA 0.904135\tHA 0.909774\t lr 0.555509\n",
      "    Update 48501\tTP -135.708969\tHP -20.177414\tTA 0.934211\tHA 0.962406\t lr 0.554122\n",
      "    Update 48751\tTP -135.298492\tHP -21.608758\tTA 0.935150\tHA 0.969925\t lr 0.552738\n",
      "Train logP=-135.298492 Test logP=-21.608758 Train Acc=0.935150 Test Acc=0.969925 TrainLoss=0.127160 TestLoss=0.162472\n",
      "----------------------------\n",
      "Epoch:47\n",
      "    Update 49001\tTP -132.036011\tHP -21.312291\tTA 0.935150\tHA 0.954887\t lr 0.551358\n",
      "    Update 49251\tTP -132.584337\tHP -21.739259\tTA 0.932331\tHA 0.969925\t lr 0.549982\n",
      "    Update 49501\tTP -137.395930\tHP -21.511453\tTA 0.935150\tHA 0.969925\t lr 0.548608\n",
      "    Update 49751\tTP -133.991710\tHP -21.382777\tTA 0.935150\tHA 0.977444\t lr 0.547239\n",
      "    Update 50001\tTP -169.871118\tHP -28.619279\tTA 0.901316\tHA 0.902256\t lr 0.545872\n",
      "Train logP=-169.871118 Test logP=-28.619279 Train Acc=0.901316 Test Acc=0.902256 TrainLoss=0.159653 TestLoss=0.215183\n",
      "----------------------------\n",
      "Epoch:48\n",
      "    Update 50251\tTP -132.751898\tHP -20.928951\tTA 0.934211\tHA 0.969925\t lr 0.544509\n",
      "    Update 50501\tTP -133.437962\tHP -20.658366\tTA 0.936090\tHA 0.969925\t lr 0.543150\n",
      "    Update 50751\tTP -156.379506\tHP -25.160836\tTA 0.909774\tHA 0.917293\t lr 0.541793\n",
      "    Update 51001\tTP -132.098720\tHP -20.965301\tTA 0.935150\tHA 0.977444\t lr 0.540441\n",
      "Train logP=-132.098720 Test logP=-20.965301 Train Acc=0.935150 Test Acc=0.977444 TrainLoss=0.124153 TestLoss=0.157634\n",
      "----------------------------\n",
      "Epoch:49\n",
      "    Update 51251\tTP -136.776607\tHP -21.079587\tTA 0.933271\tHA 0.954887\t lr 0.539091\n",
      "    Update 51501\tTP -133.101641\tHP -20.810407\tTA 0.934211\tHA 0.962406\t lr 0.537745\n",
      "    Update 51751\tTP -137.764530\tHP -20.491328\tTA 0.929511\tHA 0.962406\t lr 0.536402\n",
      "    Update 52001\tTP -138.330627\tHP -20.743775\tTA 0.933271\tHA 0.954887\t lr 0.535063\n",
      "Train logP=-138.330627 Test logP=-20.743775 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.130010 TestLoss=0.155968\n",
      "----------------------------\n",
      "Epoch:50\n",
      "    Update 52251\tTP -135.728880\tHP -21.324606\tTA 0.933271\tHA 0.954887\t lr 0.533727\n",
      "    Update 52501\tTP -140.025259\tHP -23.358741\tTA 0.908835\tHA 0.917293\t lr 0.532395\n",
      "    Update 52751\tTP -132.897868\tHP -20.709393\tTA 0.933271\tHA 0.977444\t lr 0.531065\n",
      "    Update 53001\tTP -140.335057\tHP -22.867817\tTA 0.908835\tHA 0.917293\t lr 0.529739\n",
      "Train logP=-140.335057 Test logP=-22.867817 Train Acc=0.908835 Test Acc=0.917293 TrainLoss=0.131894 TestLoss=0.171938\n",
      "----------------------------\n",
      "Epoch:51\n",
      "    Update 53251\tTP -131.342842\tHP -21.472942\tTA 0.935150\tHA 0.954887\t lr 0.528416\n",
      "    Update 53501\tTP -131.443956\tHP -22.228145\tTA 0.932331\tHA 0.977444\t lr 0.527097\n",
      "    Update 53751\tTP -136.197464\tHP -21.599558\tTA 0.935150\tHA 0.969925\t lr 0.525781\n",
      "    Update 54001\tTP -133.853804\tHP -21.721761\tTA 0.935150\tHA 0.977444\t lr 0.524468\n",
      "    Update 54251\tTP -146.477984\tHP -25.241122\tTA 0.905075\tHA 0.909774\t lr 0.523159\n",
      "Train logP=-146.477984 Test logP=-25.241122 Train Acc=0.905075 Test Acc=0.909774 TrainLoss=0.137667 TestLoss=0.189783\n",
      "----------------------------\n",
      "Epoch:52\n",
      "    Update 54501\tTP -137.023785\tHP -21.108731\tTA 0.933271\tHA 0.954887\t lr 0.521852\n",
      "    Update 54751\tTP -133.132543\tHP -21.174759\tTA 0.934211\tHA 0.977444\t lr 0.520549\n",
      "    Update 55001\tTP -154.282310\tHP -25.063919\tTA 0.909774\tHA 0.917293\t lr 0.519250\n",
      "    Update 55251\tTP -132.738621\tHP -21.087485\tTA 0.934211\tHA 0.977444\t lr 0.517953\n",
      "Train logP=-132.738621 Test logP=-21.087485 Train Acc=0.934211 Test Acc=0.977444 TrainLoss=0.124754 TestLoss=0.158553\n",
      "----------------------------\n",
      "Epoch:53\n",
      "    Update 55501\tTP -137.347751\tHP -21.336949\tTA 0.933271\tHA 0.954887\t lr 0.516660\n",
      "    Update 55751\tTP -131.091860\tHP -21.252306\tTA 0.932331\tHA 0.977444\t lr 0.515370\n",
      "    Update 56001\tTP -146.367968\tHP -21.133618\tTA 0.932331\tHA 0.954887\t lr 0.514083\n",
      "    Update 56251\tTP -141.811991\tHP -21.115508\tTA 0.933271\tHA 0.954887\t lr 0.512799\n",
      "Train logP=-141.811991 Test logP=-21.115508 Train Acc=0.933271 Test Acc=0.954887 TrainLoss=0.133282 TestLoss=0.158763\n",
      "----------------------------\n",
      "Epoch:54\n",
      "    Update 56501\tTP -134.930840\tHP -21.476253\tTA 0.934211\tHA 0.954887\t lr 0.511519\n",
      "    Update 56751\tTP -139.744020\tHP -23.515935\tTA 0.908835\tHA 0.917293\t lr 0.510242\n",
      "    Update 57001\tTP -132.177412\tHP -20.788030\tTA 0.932331\tHA 0.977444\t lr 0.508968\n",
      "    Update 57251\tTP -139.820287\tHP -22.978145\tTA 0.908835\tHA 0.917293\t lr 0.507697\n",
      "Train logP=-139.820287 Test logP=-22.978145 Train Acc=0.908835 Test Acc=0.917293 TrainLoss=0.131410 TestLoss=0.172768\n",
      "----------------------------\n",
      "Epoch:55\n",
      "    Update 57501\tTP -130.733294\tHP -21.632021\tTA 0.936090\tHA 0.954887\t lr 0.506429\n",
      "    Update 57751\tTP -132.661313\tHP -22.982386\tTA 0.931391\tHA 0.969925\t lr 0.505165\n",
      "    Update 58001\tTP -135.391861\tHP -21.720609\tTA 0.935150\tHA 0.969925\t lr 0.503903\n",
      "    Update 58251\tTP -140.200568\tHP -23.198221\tTA 0.908835\tHA 0.917293\t lr 0.502645\n",
      "    Update 58501\tTP -136.701521\tHP -23.706658\tTA 0.930451\tHA 0.962406\t lr 0.501390\n",
      "Train logP=-136.701521 Test logP=-23.706658 Train Acc=0.930451 Test Acc=0.962406 TrainLoss=0.128479 TestLoss=0.178246\n",
      "----------------------------\n",
      "Epoch:56\n",
      "    Update 58751\tTP -131.002438\tHP -21.164263\tTA 0.934211\tHA 0.969925\t lr 0.500138\n",
      "    Update 59001\tTP -134.251081\tHP -21.884748\tTA 0.910714\tHA 0.932331\t lr 0.498890\n",
      "    Update 59251\tTP -143.839705\tHP -23.738559\tTA 0.908835\tHA 0.917293\t lr 0.497644\n",
      "    Update 59501\tTP -132.288195\tHP -21.212825\tTA 0.933271\tHA 0.977444\t lr 0.496401\n",
      "Train logP=-132.288195 Test logP=-21.212825 Train Acc=0.933271 Test Acc=0.977444 TrainLoss=0.124331 TestLoss=0.159495\n",
      "----------------------------\n",
      "Epoch:57\n",
      "    Update 59751\tTP -136.195675\tHP -21.484278\tTA 0.933271\tHA 0.954887\t lr 0.495162\n",
      "    Update 60001\tTP -132.289105\tHP -21.905044\tTA 0.935150\tHA 0.977444\t lr 0.493926\n",
      "    Update 60251\tTP -141.166104\tHP -21.039892\tTA 0.933271\tHA 0.954887\t lr 0.492692\n",
      "    Update 60501\tTP -134.743747\tHP -21.101120\tTA 0.930451\tHA 0.962406\t lr 0.491462\n",
      "Train logP=-134.743747 Test logP=-21.101120 Train Acc=0.930451 Test Acc=0.962406 TrainLoss=0.126639 TestLoss=0.158655\n",
      "----------------------------\n",
      "Epoch:58\n",
      "    Update 60751\tTP -133.654010\tHP -21.576508\tTA 0.935150\tHA 0.954887\t lr 0.490235\n",
      "    Update 61001\tTP -131.679835\tHP -21.822057\tTA 0.933271\tHA 0.969925\t lr 0.489011\n",
      "    Update 61251\tTP -137.066035\tHP -22.179251\tTA 0.909774\tHA 0.924812\t lr 0.487790\n",
      "    Update 61501\tTP -139.063914\tHP -23.034131\tTA 0.908835\tHA 0.917293\t lr 0.486572\n",
      "Train logP=-139.063914 Test logP=-23.034131 Train Acc=0.908835 Test Acc=0.917293 TrainLoss=0.130699 TestLoss=0.173189\n",
      "----------------------------\n",
      "Epoch:59\n",
      "    Update 61751\tTP -131.598112\tHP -21.844117\tTA 0.935150\tHA 0.954887\t lr 0.485357\n",
      "    Update 62001\tTP -134.598402\tHP -21.353549\tTA 0.932331\tHA 0.962406\t lr 0.484145\n",
      "    Update 62251\tTP -132.336114\tHP -21.147888\tTA 0.934211\tHA 0.977444\t lr 0.482936\n",
      "    Update 62501\tTP -134.702596\tHP -22.441531\tTA 0.934211\tHA 0.977444\t lr 0.481730\n",
      "    Update 62751\tTP -131.472501\tHP -22.689550\tTA 0.933271\tHA 0.977444\t lr 0.480528\n",
      "Train logP=-131.472501 Test logP=-22.689550 Train Acc=0.933271 Test Acc=0.977444 TrainLoss=0.123564 TestLoss=0.170598\n",
      "----------------------------\n",
      "Epoch:60\n",
      "    Update 63001\tTP -130.039290\tHP -21.385374\tTA 0.934211\tHA 0.977444\t lr 0.479328\n",
      "    Update 63251\tTP -132.543246\tHP -21.830257\tTA 0.935150\tHA 0.977444\t lr 0.478131\n",
      "    Update 63501\tTP -133.174751\tHP -21.600085\tTA 0.935150\tHA 0.977444\t lr 0.476937\n",
      "    Update 63751\tTP -133.622418\tHP -21.205993\tTA 0.936090\tHA 0.962406\t lr 0.475746\n",
      "Train logP=-133.622418 Test logP=-21.205993 Train Acc=0.936090 Test Acc=0.962406 TrainLoss=0.125585 TestLoss=0.159444\n",
      "----------------------------\n",
      "Epoch:61\n",
      "    Update 64001\tTP -137.792028\tHP -21.777421\tTA 0.933271\tHA 0.954887\t lr 0.474558\n",
      "    Update 64251\tTP -130.171988\tHP -21.423447\tTA 0.933271\tHA 0.977444\t lr 0.473374\n",
      "    Update 64501\tTP -134.296637\tHP -20.899991\tTA 0.931391\tHA 0.962406\t lr 0.472192\n",
      "    Update 64751\tTP -133.859327\tHP -21.227632\tTA 0.932331\tHA 0.969925\t lr 0.471013\n",
      "Train logP=-133.859327 Test logP=-21.227632 Train Acc=0.932331 Test Acc=0.969925 TrainLoss=0.125808 TestLoss=0.159606\n",
      "----------------------------\n",
      "Epoch:62\n",
      "    Update 65001\tTP -133.693825\tHP -21.738317\tTA 0.935150\tHA 0.954887\t lr 0.469837\n",
      "    Update 65251\tTP -130.967674\tHP -21.640815\tTA 0.934211\tHA 0.977444\t lr 0.468663\n",
      "    Update 65501\tTP -132.462673\tHP -21.566209\tTA 0.934211\tHA 0.977444\t lr 0.467493\n",
      "    Update 65751\tTP -133.686178\tHP -22.208412\tTA 0.934211\tHA 0.969925\t lr 0.466326\n",
      "Train logP=-133.686178 Test logP=-22.208412 Train Acc=0.934211 Test Acc=0.969925 TrainLoss=0.125645 TestLoss=0.166981\n",
      "----------------------------\n",
      "Epoch:63\n",
      "    Update 66001\tTP -129.147809\tHP -22.392471\tTA 0.935150\tHA 0.977444\t lr 0.465162\n",
      "    Update 66251\tTP -134.189579\tHP -21.498053\tTA 0.932331\tHA 0.962406\t lr 0.464000\n",
      "    Update 66501\tTP -131.855990\tHP -21.289904\tTA 0.934211\tHA 0.977444\t lr 0.462842\n",
      "    Update 66751\tTP -131.727382\tHP -22.116524\tTA 0.935150\tHA 0.977444\t lr 0.461686\n",
      "    Update 67001\tTP -141.008140\tHP -21.698613\tTA 0.934211\tHA 0.954887\t lr 0.460533\n",
      "Train logP=-141.008140 Test logP=-21.698613 Train Acc=0.934211 Test Acc=0.954887 TrainLoss=0.132526 TestLoss=0.163147\n",
      "----------------------------\n",
      "Epoch:64\n",
      "    Update 67251\tTP -130.416997\tHP -21.533776\tTA 0.937030\tHA 0.962406\t lr 0.459383\n",
      "    Update 67501\tTP -129.984981\tHP -21.650879\tTA 0.936090\tHA 0.969925\t lr 0.458236\n",
      "    Update 67751\tTP -137.825442\tHP -22.638888\tTA 0.909774\tHA 0.932331\t lr 0.457092\n",
      "    Update 68001\tTP -131.388357\tHP -21.388694\tTA 0.934211\tHA 0.977444\t lr 0.455951\n",
      "Train logP=-131.388357 Test logP=-21.388694 Train Acc=0.934211 Test Acc=0.977444 TrainLoss=0.123485 TestLoss=0.160817\n",
      "----------------------------\n",
      "Epoch:65\n",
      "    Update 68251\tTP -137.187887\tHP -21.912166\tTA 0.933271\tHA 0.954887\t lr 0.454812\n",
      "    Update 68501\tTP -132.322462\tHP -21.386944\tTA 0.933271\tHA 0.962406\t lr 0.453677\n",
      "    Update 68751\tTP -132.821014\tHP -21.237793\tTA 0.933271\tHA 0.977444\t lr 0.452544\n",
      "    Update 69001\tTP -133.302675\tHP -21.340154\tTA 0.932331\tHA 0.969925\t lr 0.451414\n",
      "Train logP=-133.302675 Test logP=-21.340154 Train Acc=0.932331 Test Acc=0.969925 TrainLoss=0.125284 TestLoss=0.160452\n",
      "----------------------------\n",
      "Epoch:66\n",
      "    Update 69251\tTP -129.221988\tHP -21.842555\tTA 0.936090\tHA 0.962406\t lr 0.450287\n",
      "    Update 69501\tTP -135.745019\tHP -21.894101\tTA 0.935150\tHA 0.962406\t lr 0.449163\n",
      "    Update 69751\tTP -131.272176\tHP -21.449622\tTA 0.934211\tHA 0.977444\t lr 0.448041\n",
      "    Update 70001\tTP -138.055099\tHP -23.197392\tTA 0.908835\tHA 0.917293\t lr 0.446922\n",
      "Train logP=-138.055099 Test logP=-23.197392 Train Acc=0.908835 Test Acc=0.917293 TrainLoss=0.129751 TestLoss=0.174416\n",
      "----------------------------\n",
      "Epoch:67\n",
      "    Update 70251\tTP -129.083541\tHP -22.530882\tTA 0.935150\tHA 0.977444\t lr 0.445806\n",
      "    Update 70501\tTP -133.842052\tHP -21.633835\tTA 0.932331\tHA 0.962406\t lr 0.444693\n",
      "    Update 70751\tTP -131.404037\tHP -21.416055\tTA 0.934211\tHA 0.977444\t lr 0.443583\n",
      "    Update 71001\tTP -141.615357\tHP -24.044315\tTA 0.908835\tHA 0.917293\t lr 0.442475\n",
      "    Update 71251\tTP -139.664403\tHP -21.773543\tTA 0.934211\tHA 0.954887\t lr 0.441371\n",
      "Train logP=-139.664403 Test logP=-21.773543 Train Acc=0.934211 Test Acc=0.954887 TrainLoss=0.131264 TestLoss=0.163711\n",
      "----------------------------\n",
      "Epoch:68\n",
      "    Update 71501\tTP -132.435865\tHP -21.695262\tTA 0.936090\tHA 0.954887\t lr 0.440268\n",
      "    Update 71751\tTP -129.710718\tHP -21.618795\tTA 0.937030\tHA 0.969925\t lr 0.439169\n",
      "    Update 72001\tTP -137.024743\tHP -22.695037\tTA 0.909774\tHA 0.932331\t lr 0.438073\n",
      "    Update 72251\tTP -133.862446\tHP -21.490416\tTA 0.935150\tHA 0.954887\t lr 0.436979\n",
      "Train logP=-133.862446 Test logP=-21.490416 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.125811 TestLoss=0.161582\n",
      "----------------------------\n",
      "Epoch:69\n",
      "    Update 72501\tTP -131.698647\tHP -21.821457\tTA 0.936090\tHA 0.954887\t lr 0.435888\n",
      "    Update 72751\tTP -131.757029\tHP -21.513418\tTA 0.933271\tHA 0.962406\t lr 0.434799\n",
      "    Update 73001\tTP -134.810804\tHP -22.548879\tTA 0.933271\tHA 0.969925\t lr 0.433714\n",
      "    Update 73251\tTP -132.616516\tHP -21.457858\tTA 0.932331\tHA 0.969925\t lr 0.432631\n",
      "Train logP=-132.616516 Test logP=-21.457858 Train Acc=0.932331 Test Acc=0.969925 TrainLoss=0.124640 TestLoss=0.161337\n",
      "----------------------------\n",
      "Epoch:70\n",
      "    Update 73501\tTP -133.860517\tHP -22.154090\tTA 0.935150\tHA 0.954887\t lr 0.431551\n",
      "    Update 73751\tTP -139.185979\tHP -22.325199\tTA 0.933271\tHA 0.947368\t lr 0.430473\n",
      "    Update 74001\tTP -132.659461\tHP -22.000246\tTA 0.935150\tHA 0.977444\t lr 0.429398\n",
      "    Update 74251\tTP -137.416521\tHP -23.240792\tTA 0.909774\tHA 0.924812\t lr 0.428326\n",
      "Train logP=-137.416521 Test logP=-23.240792 Train Acc=0.909774 Test Acc=0.924812 TrainLoss=0.129151 TestLoss=0.174743\n",
      "----------------------------\n",
      "Epoch:71\n",
      "    Update 74501\tTP -135.314854\tHP -23.897738\tTA 0.906955\tHA 0.932331\t lr 0.427257\n",
      "    Update 74751\tTP -138.673531\tHP -22.003974\tTA 0.933271\tHA 0.954887\t lr 0.426190\n",
      "    Update 75001\tTP -132.453924\tHP -22.076270\tTA 0.935150\tHA 0.969925\t lr 0.425126\n",
      "    Update 75251\tTP -140.844939\tHP -24.063320\tTA 0.908835\tHA 0.917293\t lr 0.424064\n",
      "    Update 75501\tTP -136.812144\tHP -21.749635\tTA 0.934211\tHA 0.954887\t lr 0.423005\n",
      "Train logP=-136.812144 Test logP=-21.749635 Train Acc=0.934211 Test Acc=0.954887 TrainLoss=0.128583 TestLoss=0.163531\n",
      "----------------------------\n",
      "Epoch:72\n",
      "    Update 75751\tTP -129.959642\tHP -21.802793\tTA 0.937030\tHA 0.962406\t lr 0.421949\n",
      "    Update 76001\tTP -129.537435\tHP -21.729351\tTA 0.937030\tHA 0.969925\t lr 0.420896\n",
      "    Update 76251\tTP -132.571826\tHP -22.095893\tTA 0.935150\tHA 0.977444\t lr 0.419845\n",
      "    Update 76501\tTP -132.495421\tHP -22.420690\tTA 0.934211\tHA 0.977444\t lr 0.418796\n",
      "Train logP=-132.495421 Test logP=-22.420690 Train Acc=0.934211 Test Acc=0.977444 TrainLoss=0.124526 TestLoss=0.168577\n",
      "----------------------------\n",
      "Epoch:73\n",
      "    Update 76751\tTP -129.587779\tHP -21.942105\tTA 0.937030\tHA 0.962406\t lr 0.417751\n",
      "    Update 77001\tTP -129.229940\tHP -21.928421\tTA 0.934211\tHA 0.977444\t lr 0.416708\n",
      "    Update 77251\tTP -132.234060\tHP -21.926064\tTA 0.933271\tHA 0.969925\t lr 0.415667\n",
      "    Update 77501\tTP -131.181900\tHP -22.055686\tTA 0.934211\tHA 0.977444\t lr 0.414629\n",
      "Train logP=-131.181900 Test logP=-22.055686 Train Acc=0.934211 Test Acc=0.977444 TrainLoss=0.123291 TestLoss=0.165832\n",
      "----------------------------\n",
      "Epoch:74\n",
      "    Update 77751\tTP -130.485324\tHP -22.086086\tTA 0.936090\tHA 0.954887\t lr 0.413594\n",
      "    Update 78001\tTP -135.581256\tHP -22.245790\tTA 0.935150\tHA 0.947368\t lr 0.412561\n",
      "    Update 78251\tTP -133.799508\tHP -22.415433\tTA 0.935150\tHA 0.969925\t lr 0.411531\n",
      "    Update 78501\tTP -143.985644\tHP -24.418655\tTA 0.907895\tHA 0.917293\t lr 0.410504\n",
      "Train logP=-143.985644 Test logP=-24.418655 Train Acc=0.907895 Test Acc=0.917293 TrainLoss=0.135325 TestLoss=0.183599\n",
      "----------------------------\n",
      "Epoch:75\n",
      "    Update 78751\tTP -146.449226\tHP -25.909735\tTA 0.905075\tHA 0.909774\t lr 0.409479\n",
      "    Update 79001\tTP -135.228463\tHP -21.959356\tTA 0.935150\tHA 0.954887\t lr 0.408456\n",
      "    Update 79251\tTP -131.940380\tHP -22.149145\tTA 0.935150\tHA 0.977444\t lr 0.407436\n",
      "    Update 79501\tTP -134.764812\tHP -23.204194\tTA 0.910714\tHA 0.932331\t lr 0.406419\n",
      "    Update 79751\tTP -146.052600\tHP -22.485921\tTA 0.933271\tHA 0.947368\t lr 0.405404\n",
      "Train logP=-146.052600 Test logP=-22.485921 Train Acc=0.933271 Test Acc=0.947368 TrainLoss=0.137267 TestLoss=0.169067\n",
      "----------------------------\n",
      "Epoch:76\n",
      "    Update 80001\tTP -128.658020\tHP -22.001190\tTA 0.935150\tHA 0.977444\t lr 0.404392\n",
      "    Update 80251\tTP -129.384066\tHP -22.097954\tTA 0.936090\tHA 0.969925\t lr 0.403382\n",
      "    Update 80501\tTP -132.091246\tHP -22.176175\tTA 0.935150\tHA 0.977444\t lr 0.402375\n",
      "    Update 80751\tTP -130.044750\tHP -21.927758\tTA 0.935150\tHA 0.977444\t lr 0.401370\n",
      "Train logP=-130.044750 Test logP=-21.927758 Train Acc=0.935150 Test Acc=0.977444 TrainLoss=0.122223 TestLoss=0.164870\n",
      "----------------------------\n",
      "Epoch:77\n",
      "    Update 81001\tTP -132.431931\tHP -22.225086\tTA 0.936090\tHA 0.954887\t lr 0.400368\n",
      "    Update 81251\tTP -128.992782\tHP -22.032535\tTA 0.934211\tHA 0.977444\t lr 0.399369\n",
      "    Update 81501\tTP -130.406221\tHP -21.439758\tTA 0.936090\tHA 0.969925\t lr 0.398371\n",
      "    Update 81751\tTP -130.689244\tHP -22.108264\tTA 0.935150\tHA 0.977444\t lr 0.397377\n",
      "Train logP=-130.689244 Test logP=-22.108264 Train Acc=0.935150 Test Acc=0.977444 TrainLoss=0.122828 TestLoss=0.166228\n",
      "----------------------------\n",
      "Epoch:78\n",
      "    Update 82001\tTP -130.537291\tHP -22.295945\tTA 0.936090\tHA 0.954887\t lr 0.396385\n",
      "    Update 82251\tTP -146.401938\tHP -23.132822\tTA 0.933271\tHA 0.939850\t lr 0.395395\n",
      "    Update 82501\tTP -132.922384\tHP -22.416735\tTA 0.935150\tHA 0.969925\t lr 0.394408\n",
      "    Update 82751\tTP -143.082891\tHP -24.405059\tTA 0.907895\tHA 0.917293\t lr 0.393423\n",
      "Train logP=-143.082891 Test logP=-24.405059 Train Acc=0.907895 Test Acc=0.917293 TrainLoss=0.134476 TestLoss=0.183497\n",
      "----------------------------\n",
      "Epoch:79\n",
      "    Update 83001\tTP -145.052595\tHP -25.784200\tTA 0.906015\tHA 0.909774\t lr 0.392440\n",
      "    Update 83251\tTP -134.523329\tHP -22.051676\tTA 0.935150\tHA 0.954887\t lr 0.391461\n",
      "    Update 83501\tTP -129.851844\tHP -21.956875\tTA 0.937030\tHA 0.969925\t lr 0.390483\n",
      "    Update 83751\tTP -130.847694\tHP -22.577794\tTA 0.935150\tHA 0.977444\t lr 0.389508\n",
      "    Update 84001\tTP -141.948020\tHP -22.319430\tTA 0.934211\tHA 0.947368\t lr 0.388536\n",
      "Train logP=-141.948020 Test logP=-22.319430 Train Acc=0.934211 Test Acc=0.947368 TrainLoss=0.133410 TestLoss=0.167815\n",
      "----------------------------\n",
      "Epoch:80\n",
      "    Update 84251\tTP -128.384843\tHP -22.296199\tTA 0.934211\tHA 0.977444\t lr 0.387565\n",
      "    Update 84501\tTP -129.414517\tHP -21.915041\tTA 0.937030\tHA 0.969925\t lr 0.386598\n",
      "    Update 84751\tTP -130.468520\tHP -21.666822\tTA 0.933271\tHA 0.969925\t lr 0.385632\n",
      "    Update 85001\tTP -130.405323\tHP -21.732898\tTA 0.935150\tHA 0.969925\t lr 0.384670\n",
      "Train logP=-130.405323 Test logP=-21.732898 Train Acc=0.935150 Test Acc=0.969925 TrainLoss=0.122561 TestLoss=0.163405\n",
      "----------------------------\n",
      "Epoch:81\n",
      "    Update 85251\tTP -128.839587\tHP -22.282172\tTA 0.937030\tHA 0.962406\t lr 0.383709\n",
      "    Update 85501\tTP -129.724659\tHP -22.480176\tTA 0.934211\tHA 0.977444\t lr 0.382751\n",
      "    Update 85751\tTP -130.095325\tHP -21.560638\tTA 0.936090\tHA 0.969925\t lr 0.381795\n",
      "    Update 86001\tTP -130.016436\tHP -21.867915\tTA 0.935150\tHA 0.977444\t lr 0.380842\n",
      "Train logP=-130.016436 Test logP=-21.867915 Train Acc=0.935150 Test Acc=0.977444 TrainLoss=0.122196 TestLoss=0.164420\n",
      "----------------------------\n",
      "Epoch:82\n",
      "    Update 86251\tTP -130.027767\tHP -22.394197\tTA 0.937030\tHA 0.954887\t lr 0.379891\n",
      "    Update 86501\tTP -129.941104\tHP -22.650583\tTA 0.932331\tHA 0.962406\t lr 0.378943\n",
      "    Update 86751\tTP -130.301527\tHP -21.935099\tTA 0.934211\tHA 0.977444\t lr 0.377996\n",
      "    Update 87001\tTP -134.505738\tHP -23.101398\tTA 0.909774\tHA 0.932331\t lr 0.377053\n",
      "Train logP=-134.505738 Test logP=-23.101398 Train Acc=0.909774 Test Acc=0.932331 TrainLoss=0.126415 TestLoss=0.173695\n",
      "----------------------------\n",
      "Epoch:83\n",
      "    Update 87251\tTP -137.620401\tHP -24.687423\tTA 0.906015\tHA 0.924812\t lr 0.376111\n",
      "    Update 87501\tTP -134.139561\tHP -22.150972\tTA 0.935150\tHA 0.947368\t lr 0.375172\n",
      "    Update 87751\tTP -129.597824\tHP -22.061482\tTA 0.937030\tHA 0.969925\t lr 0.374235\n",
      "    Update 88001\tTP -130.964603\tHP -22.723905\tTA 0.935150\tHA 0.977444\t lr 0.373301\n",
      "    Update 88251\tTP -135.631976\tHP -22.047697\tTA 0.935150\tHA 0.954887\t lr 0.372369\n",
      "Train logP=-135.631976 Test logP=-22.047697 Train Acc=0.935150 Test Acc=0.954887 TrainLoss=0.127474 TestLoss=0.165772\n",
      "----------------------------\n",
      "Epoch:84\n",
      "    Update 88501\tTP -130.368764\tHP -22.161887\tTA 0.936090\tHA 0.954887\t lr 0.371439\n",
      "    Update 88751\tTP -128.759476\tHP -22.168271\tTA 0.937030\tHA 0.969925\t lr 0.370512\n",
      "    Update 89001\tTP -131.952700\tHP -21.684366\tTA 0.932331\tHA 0.962406\t lr 0.369586\n",
      "    Update 89251\tTP -130.317635\tHP -21.798866\tTA 0.935150\tHA 0.969925\t lr 0.368664\n",
      "Train logP=-130.317635 Test logP=-21.798866 Train Acc=0.935150 Test Acc=0.969925 TrainLoss=0.122479 TestLoss=0.163901\n",
      "----------------------------\n",
      "Epoch:85\n",
      "    Update 89501\tTP -128.704114\tHP -22.375293\tTA 0.937030\tHA 0.962406\t lr 0.367743\n",
      "    Update 89751\tTP -135.839721\tHP -23.882341\tTA 0.909774\tHA 0.917293\t lr 0.366825\n",
      "    Update 90001\tTP -129.659051\tHP -21.696557\tTA 0.933271\tHA 0.977444\t lr 0.365909\n",
      "    Update 90251\tTP -130.110947\tHP -22.255113\tTA 0.934211\tHA 0.977444\t lr 0.364995\n",
      "Early Stop\n",
      "Train logP=-130.110947 Test logP=-22.255113 Train Acc=0.934211 Test Acc=0.977444 TrainLoss=0.122285 TestLoss=0.167332\n",
      "Figure(1000x400)\n",
      "Saveing weights for epoch=22, Least Test Loss=0.147974\n",
      "Train logP=-147.124408 Test logP=-19.680565 Train Acc=0.929511 Test Acc=0.962406 TrainLoss=0.138275 TestLoss=0.147974\n"
     ]
    }
   ],
   "source": [
    "! python3 logreg.py --passes=100  --step=.9 --early_stop=40 --normalize=no \\\n",
    "    --log=yes --log_step=250 --plot_name='selected_features_with_normalization_with_scheduler_exp' \\\n",
    "    --save_weights_path='weights/selected_features_with_normalization_with_scheduler_exp' \\\n",
    "    --chosen_positive_indcies='data/chosen_p_indcies.npy' --chosen_negative_indcies='data/chosen_n_indcies.npy' \\\n",
    "    --learning_rate_scheduler='exp' --learning_rate_scheduler_params='{\"decay\":1e-5}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17256b85-d8fc-41ec-8a1a-3fbb1affd05d",
   "metadata": {},
   "source": [
    "![selected no normalization exp](figures/selected_features_with_normalization_with_scheduler_exp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac82e47-aad2-47f6-8f1c-b0f8dbc4f985",
   "metadata": {},
   "source": [
    "# Analyzing uingram Features of: logstic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ccde49-bd90-4c04-a2b2-05bfc6a7b7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from logreg import ExamplesDataset, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07e3e4c-328f-4a4f-9823-36d4f2fac687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   1.,   0.,   0.,   1.,   1.,   1.,   0.,   0.,   0.,\n",
       "          1.,   4.,  12.,  18.,  16.,  19.,  55.,  57., 107., 146., 180.,\n",
       "        248., 292., 356., 377., 423., 432., 397., 379., 338., 305., 259.,\n",
       "        196., 168., 121.,  86.,  59.,  31.,  25.,  11.,   7.,   4.,   2.,\n",
       "          0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([-5.82332404, -5.61263123, -5.40193843, -5.19124562, -4.98055282,\n",
       "        -4.76986001, -4.55916721, -4.34847441, -4.1377816 , -3.9270888 ,\n",
       "        -3.71639599, -3.50570319, -3.29501039, -3.08431758, -2.87362478,\n",
       "        -2.66293197, -2.45223917, -2.24154637, -2.03085356, -1.82016076,\n",
       "        -1.60946795, -1.39877515, -1.18808235, -0.97738954, -0.76669674,\n",
       "        -0.55600393, -0.34531113, -0.13461832,  0.07607448,  0.28676728,\n",
       "         0.49746009,  0.70815289,  0.9188457 ,  1.1295385 ,  1.3402313 ,\n",
       "         1.55092411,  1.76161691,  1.97230972,  2.18300252,  2.39369532,\n",
       "         2.60438813,  2.81508093,  3.02577374,  3.23646654,  3.44715934,\n",
       "         3.65785215,  3.86854495,  4.07923776,  4.28993056,  4.50062337,\n",
       "         4.71131617]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# playing with weights\n",
    "# index0-> bias\n",
    "weights = np.load('weights/all_features_no_normalization.npy')\n",
    "plt.hist(weights, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395028b5-1c88-4ff9-af48-9de0b6ceb298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Reading dataset'''\n",
    "dataset = ExamplesDataset('data/positive', 'data/negative', 'data/vocab')\n",
    "positive, negative, vocab = dataset.get_positive_negative_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f60478f3-d61e-4c55-89fa-a17997ecbca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:  (5137,)\n",
      "filter shape (7,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['BIAS_CONSTANT', 'hockey', 'next', 'runs', 'playoffs', 'biggest',\n",
       "       'pts'], dtype='<U17')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_arr = np.array(vocab)\n",
    "w = 3.5\n",
    "print('original shape: ', vocab_arr.shape)\n",
    "f = (weights<-w) | ( weights>w)\n",
    "f[0] = True\n",
    "print('filter shape', f[f==True].shape)\n",
    "vocab_arr[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a60c7e-fdcf-4566-aa01-4a8c02562542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit= 0.013825460636515907\n",
      "filter shape:  (325,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' positve words'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def most_important_features(positive ,negative ,vocab, threshold=1):\n",
    "    \"\"\"\n",
    "    :param data: list of Example objects\n",
    "    :param vocab list of words\n",
    "    :param threshold: the number at which we neglegt redudant features\n",
    "    :return: (pos_freq, neg_freq, words)\n",
    "    \"\"\"\n",
    "    # converting to numpy\n",
    "    \n",
    "    pos_arr = np.empty(shape=(len(positive), len(vocab)))\n",
    "    for i in range(len(positive)):\n",
    "        pos_arr[i] = positive[i].x\n",
    "        \n",
    "    neg_arr = np.empty(shape=(len(negative), len(vocab)))\n",
    "    for i in range(len(negative)):\n",
    "        neg_arr[i] = negative[i].x\n",
    "        \n",
    "    vocab_arr = np.array(vocab)\n",
    "    \n",
    "    return pos_arr, neg_arr, vocab_arr\n",
    "\n",
    "pos, neg, v_arr = most_important_features(positive, negative, vocab)\n",
    "\n",
    "# # playing \n",
    "# small = 1e-9\n",
    "# p_m = np.mean(pos, axis=0) + small\n",
    "# n_m = np.mean(neg, axis=0) + small\n",
    "\n",
    "# out = p_m /n_m\n",
    "# f = (out > 2.4e7) \n",
    "# print('filter shape: ', f[f==True].shape)\n",
    "# print(vocab_arr[f])\n",
    "# print(p_m[v_arr=='baseball'])\n",
    "# print(n_m[v_arr=='baseball'])\n",
    "# print(out[v_arr=='baseball'])\n",
    "# plt.hist(out, bins=50)\n",
    "\n",
    "\n",
    "# playing \n",
    "small = 0.0\n",
    "p_m = np.mean(pos, axis=0) + small\n",
    "n_m = np.mean(neg, axis=0) + small\n",
    "\n",
    "out = p_m - n_m\n",
    "\n",
    "\n",
    "counts, bins = np.histogram(out, bins=100)\n",
    "limit = bins[np.argmax(counts)+2]\n",
    "print('Limit=',limit)\n",
    "\n",
    "f = (out > limit) \n",
    "f_shape = f[f==True].shape\n",
    "print('filter shape: ', f[f==True].shape)\n",
    "# print(vocab_arr[f])\n",
    "\n",
    "plt.stairs(counts, bins)\n",
    "\n",
    "\n",
    "''' positve words'''\n",
    "# # Sortting words\n",
    "# indcies = np.argsort(out)[-f_shape[0]:]\n",
    "# chosen_indcies = []\n",
    "# for i in indcies[::-1]:\n",
    "#     print(vocab_arr[i])\n",
    "#     o = input()\n",
    "#     if o == 'y':\n",
    "#         chosen_indcies.append(i)\n",
    "    \n",
    "# np.save('data/chosen_p_indcies', np.array(chosen_indcies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "783019e5-9df5-4cdf-9ea9-700cdf03428f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baseball' 'hit' 'pitching' 'hitter' 'pitcher' 'hitting' 'pitch' 'base'\n",
      " 'pitchers' 'innings' 'bat' 'hits' 'pitched' 'batting' 'inning' 'field'\n",
      " 'cubs' 'catcher' 'tedward' 'offense' 'majors' 'mss' 'rotation' 'bases'\n",
      " 'bullpen' 'starters' 'swing' 'roger' 'rickert' 'runner' 'fielding'\n",
      " 'pinch' 'plate' 'homers' 'hitters' 'catchers' 'runners' 'fls' 'caught'\n",
      " 'relief' 'homer' 'strike' 'starter' 'scott' 'bob' 'stadium' 'pennant'\n",
      " 'catch' 'infield' 'platoon' 'stance' 'erics' 'gotten' 'edge' 'threw'\n",
      " 'demers' 'peak' 'luriem' 'fierkelab' 'vesterman' 'fielder' 'hung'\n",
      " 'admiral' 'sepinwal' 'philly' 'mjones' 'minors' 'lineup' 'pm' 'closer'\n",
      " 'batter' 'fly' 'pace' 'bats' 'nimaster' 'baseman' 'lankford' 'shortstops'\n",
      " 'outfield' 'rushed' 'prime' 'humor' 'balls' 'baserunning' 'kirsch'\n",
      " 'gspira' 'jtchern' 'cmk' 'shortstop' 'racking' 'batters' 'snichols'\n",
      " 'leagues' 'clutch' 'lame' 'jay' 'pitches' 'gajarsky' 'ted' 'rogoff'\n",
      " 'jrogoff' 'defensively' 'glove' 'liked' 'opener' 'waivers' 'davewood'\n",
      " 'rp' 'umpires' 'paula' 'uucp' 'traven' 'niguma']\n",
      "(113,)\n"
     ]
    }
   ],
   "source": [
    "chosen_p_indcies = np.load('data/chosen_p_indcies.npy')\n",
    "print(vocab_arr[chosen_p_indcies])\n",
    "print(chosen_p_indcies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf237ae-f297-4fc7-8b62-626d3517e807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit= -0.024242211055276353\n",
      "filter shape:  (235,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' negative words'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative class\n",
    "\n",
    "limit = bins[np.argmax(counts)-2]\n",
    "print('Limit=',limit)\n",
    "f = (out < limit) \n",
    "f_shape = f[f==True].shape\n",
    "print('filter shape: ', f[f==True].shape)\n",
    "\n",
    "''' negative words'''\n",
    "\n",
    "# # Sortting words\n",
    "# indcies = np.argsort(out)[:f_shape[0]]\n",
    "# print(vocab_arr[indcies])\n",
    "# chosen_indcies = []\n",
    "# for i in indcies:\n",
    "#     print(vocab_arr[i])\n",
    "#     o = input()\n",
    "#     if o == 'y':\n",
    "#         chosen_indcies.append(i)\n",
    "    \n",
    "# np.save('data/chosen_n_indcies', np.array(chosen_indcies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21457855-9a8c-44b9-a789-9989d44b1633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hockey' 'period' 'goal' 'points' 'playoffs' 'puck' 'pp' 'goals' 'ice'\n",
      " 'players' 'pts' 'playoff' 'gld' 'player' 'coach' 'penalty' 'round'\n",
      " 'draft' 'captain' 'pick' 'net' 'scoring' 'shot' 'goalie' 'shots'\n",
      " 'defenseman' 'series' 'tie' 'maynard' 'golchowy' 'penalties' 'line'\n",
      " 'mask' 'franchise' 'zone' 'gballent' 'dchhabra' 'kkeller' 'arena' 'ca'\n",
      " 'breaker' 'forwards' 'stat' 'point' 'roughing' 'sh' 'goaltender' 'slot'\n",
      " 'pool' 'forward' 'standings' 'etxonss' 'wing' 'unassisted' 'defensemen'\n",
      " 'goalies' 'conference' 'rm' 'passed' 'stick' 'farenebt' 'assists'\n",
      " 'instead' 'circle' 'match' 'club' 'nne' 'boards' 'cordially' 'tough'\n",
      " 'hell' 'hammerl' 'octopus' 'rebound' 'howl' 'goaltending' 'gargle'\n",
      " 'defensive' 'souviens' 'poll' 'fmsalvat' 'rauser' 'stop' 'ref' 'pluggers'\n",
      " 'willis' 'rights']\n",
      "(87,)\n"
     ]
    }
   ],
   "source": [
    "chosen_n_indcies = np.load('data/chosen_n_indcies.npy')\n",
    "print(vocab_arr[chosen_n_indcies])\n",
    "print(chosen_n_indcies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaa36b-9bd5-4902-8661-1e68f2286a54",
   "metadata": {},
   "source": [
    "## What words are good predictor of chosen words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a51d06-6b32-461d-8fea-e31fba11efe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9535449473858401\n",
      "0.925532031303504\n",
      "0.9507521827650778\n",
      "0.004335358001964811\n",
      "0.8536558644363252\n"
     ]
    }
   ],
   "source": [
    "def get_selected_prediction(word, val, vocab_arr, chosen_words_indcies, weights):\n",
    "    word_index = np.where(vocab_arr == word)[0]\n",
    "    new_index = np.where(chosen_words_indcies ==word_index)[0]\n",
    "    assert len(new_index)!=0, f'\"{word}\" is not in chosen words'\n",
    "    input_vector = np.zeros_like(chosen_words_indcies)\n",
    "    input_vector[new_index] = val\n",
    "    input_vector[0] = 1\n",
    "    \n",
    "    out = sigmoid(np.dot(input_vector, weights))\n",
    "    return out\n",
    "\n",
    "chosen_p_indcies = np.load('data/chosen_p_indcies.npy')\n",
    "chosen_n_indcies = np.load('data/chosen_n_indcies.npy')\n",
    "chosen_words_indcies = np.zeros(len(chosen_p_indcies) + len(chosen_n_indcies) +1, dtype=np.int32)\n",
    "chosen_words_indcies[1:len(chosen_p_indcies) +1] = chosen_p_indcies\n",
    "chosen_words_indcies[len(chosen_p_indcies) +1:] = chosen_n_indcies\n",
    "chosen_words_indcies.sort()\n",
    "chosen_words_indcies[0] = 0 # to chose bias\n",
    "# print(chosen_words_indcies)\n",
    "\n",
    "weights = np.load('weights/selected_features_no_normalization.npy')\n",
    "\n",
    "print(get_selected_prediction('baseball', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('bases', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('hit', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('hockey', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "# print(get_selected_prediction('think', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, chosen_words_indcies, weights))\n",
    "# print(get_selected_prediction('liked', 1, vocab_arr, chosen_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b729eaaa-1e11-42bd-8a45-1e9dfe22ffda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a bias towards p\n",
      "0.8536558644363252\n"
     ]
    }
   ],
   "source": [
    "print('There is a bias towards p')\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, chosen_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "640d7810-ac87-41d9-b181-b840ab07b434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 3 2]\n"
     ]
    }
   ],
   "source": [
    "def sort_with_keys(key_arr, val_arr):\n",
    "    '''\n",
    "    sort a val_arr using kesy_arr from ascendingly\n",
    "    :param key_arr: numpy array with keys\n",
    "    :param val_arr: numpy array to be sorted\n",
    "    :return: sorted numpy array  of val_arr\n",
    "    '''\n",
    "    index = np.lexsort((val_arr, key_arr))\n",
    "    return val_arr[index]\n",
    "out=sort_with_keys(np.array([0, 1, 2, 4]), np.array([1, 5, 3, 2]))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d51d3f6-41c7-4e9a-85be-7b85d23fb047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pitching' 'inning' 'pitcher' 'pitched' 'luriem' 'assists' 'stadium'\n",
      " 'batting' 'hitters' 'tedward' 'minors' 'rotation' 'pitchers' 'bob' 'club'\n",
      " 'baseman' 'bat' 'field' 'glove' 'lame' 'ted' 'edge' 'bullpen' 'rushed'\n",
      " 'cubs' 'majors' 'kirsch' 'balls' 'homers' 'strike' 'fielding' 'jay'\n",
      " 'fierkelab' 'rogoff' 'sepinwal' 'erics' 'catcher' 'homer' 'innings' 'mss'\n",
      " 'jtchern' 'waivers' 'paula' 'pitch' 'baseball' 'philly' 'leagues'\n",
      " 'pitches' 'scott' 'rickert' 'bats' 'hit' 'prime' 'hitter' 'base' 'roger'\n",
      " 'batters' 'stance' 'uucp' 'shortstops' 'stick' 'outfield' 'starters'\n",
      " 'scoring' 'demers' 'defensive' 'slot' 'mjones' 'rp' 'closer' 'pennant'\n",
      " 'cordially' 'infield' 'bases' 'catch' 'opener' 'lineup' 'hung' 'runner'\n",
      " 'fls' 'runners' 'batter' 'cmk' 'vesterman' 'caught' 'point' 'boards' 'pm'\n",
      " 'lankford' 'hits' 'pinch' 'clutch' 'gspira' 'admiral' 'offense'\n",
      " 'shortstop' 'peak' 'platoon' 'niguma' 'davewood' 'hitting' 'swing'\n",
      " 'fielder' 'nimaster' 'traven' 'threw' 'relief' 'racking' 'shot' 'humor'\n",
      " 'jrogoff' 'BIAS_CONSTANT' 'gotten' 'draft' 'starter' 'pp' 'breaker'\n",
      " 'defensemen' 'ref' 'baserunning' 'gajarsky' 'umpires' 'players' 'series'\n",
      " 'net' 'catchers' 'fly' 'snichols' 'pace' 'rauser' 'arena' 'plate' 'line'\n",
      " 'sh' 'defensively' 'souviens' 'rebound' 'instead' 'goaltender' 'liked'\n",
      " 'rm' 'player' 'stat' 'hell' 'goalies' 'ca' 'forward' 'gargle'\n",
      " 'unassisted' 'rights' 'passed' 'goaltending' 'pluggers' 'gballent'\n",
      " 'circle' 'shots' 'pool' 'points' 'tough' 'conference' 'howl' 'octopus'\n",
      " 'gld' 'fmsalvat' 'defenseman' 'zone' 'etxonss' 'wing']\n",
      "(168,)\n",
      "\n",
      "['pts' 'hockey' 'golchowy' 'playoffs' 'playoff' 'coach' 'goals' 'goal'\n",
      " 'ice' 'stop' 'round' 'puck' 'forwards' 'willis' 'poll' 'hammerl' 'goalie'\n",
      " 'penalty' 'captain' 'period' 'franchise' 'match' 'tie' 'farenebt' 'nne'\n",
      " 'dchhabra' 'pick' 'maynard' 'penalties' 'kkeller' 'standings' 'mask'\n",
      " 'roughing']\n",
      "(33,)\n"
     ]
    }
   ],
   "source": [
    "def get_selected_prediction_arr(val, vocab_arr, chosen_words_indcies, weights):\n",
    "    # we iclude bias as a seprate input\n",
    "    input_vector = np.diag(val * np.ones(len(chosen_words_indcies)))\n",
    "    input_vector[:, 0] = 1\n",
    "    \n",
    "    out = np.matmul(input_vector, weights.reshape([-1, 1]))\n",
    "    \n",
    "    #sigmoid\n",
    "    out = np.exp(out)\n",
    "    out = out/(1+out)\n",
    "    \n",
    "    #Sorting words\n",
    "    p_words_indcies = np.where(out>=0.5)[0]\n",
    "    p_vocab_indcies = sort_with_keys(out[p_words_indcies].reshape(-1), chosen_words_indcies[p_words_indcies])[::-1]\n",
    "    \n",
    "    n_words_indcies = np.where(out<0.5)[0]\n",
    "    n_vocab_indcies = sort_with_keys(out[n_words_indcies].reshape(-1), chosen_words_indcies[n_words_indcies])\n",
    "    return p_vocab_indcies, n_vocab_indcies\n",
    "\n",
    "p_vocab_indcies, n_vocab_indcies = get_selected_prediction_arr(1, vocab_arr, chosen_words_indcies, weights)\n",
    "\n",
    "print(vocab_arr[p_vocab_indcies])\n",
    "print(vocab_arr[p_vocab_indcies].shape)\n",
    "print()\n",
    "print(vocab_arr[n_vocab_indcies])\n",
    "print(vocab_arr[n_vocab_indcies].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366aaed-148f-4069-b5e1-3d7b411ae74b",
   "metadata": {},
   "source": [
    "## Analysing words are good predictor from all words (no selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62eca5b4-3871-4374-8c14-b2964c2ce7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9881682108478935\n",
      "0.872612101821902\n",
      "0.9781151812384982\n",
      "0.012423707456081646\n",
      "0.8096406371120461\n"
     ]
    }
   ],
   "source": [
    "all_words_indcies = np.arange(len(vocab_arr))\n",
    "weights = np.load('weights/all_features_no_normalization.npy')\n",
    "\n",
    "print(get_selected_prediction('baseball', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('bases', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('hit', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('hockey', 1, vocab_arr, all_words_indcies, weights))\n",
    "# print(get_selected_prediction('think', 1, vocab_arr, all_words_indcies, weights))\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, all_words_indcies, weights))\n",
    "# print(get_selected_prediction('liked', 1, vocab_arr, all_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec65a56d-629d-42a4-8421-2092e41d5363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a bias towards p\n",
      "0.8096406371120461\n"
     ]
    }
   ],
   "source": [
    "print('There is a bias towards p')\n",
    "print(get_selected_prediction('BIAS_CONSTANT', 1, vocab_arr, all_words_indcies, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec736927-e101-475c-b46e-e0d977115e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runs' 'justification' 'coveted' 'band' 'nicely' 'irony' 'info' 'girls'\n",
      " 'baseball' 'listing' 'sepinwal' 'steven' 'traditional' 'valued' 'central'\n",
      " 'books' 'thats' 'basically' 'liking' 'nuts' 'interpreted' 'infamous'\n",
      " 'staion' 'settled' 'evaluating' 'chip' 'mitt' 'educate' 'skate' 'likely'\n",
      " 'deny' 'helping' 'prime' 'expired' 'minimum' 'pitches' 'camelot' 'gotten'\n",
      " 'performed' 'starter' 'staff' 'gem' 'equivalent' 'pitch' 'subsidiary'\n",
      " 'fist' 'remember' 'rogoff' 'backhanded' 'water' 'exceed' 'ignorance'\n",
      " 'pour' 'anyone' 'delay' 'rating' 'bar' 'hit' 'fry' 'chuck' 'inception'\n",
      " 'resting' 'equal' 'logos']\n",
      "(4739,)\n",
      "\n",
      "['hockey' 'pts' 'playoffs' 'next' 'biggest' 'finals' 'broadcast'\n",
      " 'annoying' 'need' 'playoff' 'vs' 'polite' 'lucky' 'mmb' 'personal'\n",
      " 'period' 'golchowy' 'dkl' 'arrogant' 'combinations' 'non' 'coach' 'ext'\n",
      " 'embarrasing' 'saving' 'honoured' 'beat' 'allan' 'earning' 'khettry'\n",
      " 'stiff' 'goal' 'zones' 'shorts' 'helps' 'splits' 'wangr' 'peoples'\n",
      " 'standing' 'tvartiai' 'could' 'presently' 'h' 'angles' 'dishes' 'flat'\n",
      " 'slap' 'stamber' 'leaders' 'overtime' 'counts' 'dangerous' 'traded'\n",
      " 'wish' 'retort' 'round' 'cheese' 'producers' 'drew' 'vegetarian'\n",
      " 'younger' 'shoots' 'border' 'night' 'pro' 'cyberspace' 'adjusted' 'likes'\n",
      " 'expense' 'phrase' 'screen' 'fewer' 'values' 'conducive' 'jake' 'advice'\n",
      " 'understanding' 'occasional' 'excuses' 'fault' 'join' 'write' 'flagship'\n",
      " 'grew' 'shown' 'crucial' 'locker' 'cholesterol' 'graduated' 'duties'\n",
      " 'showing' 'programs' 'insgnia' 'preceding' 'immediate' 'standings'\n",
      " 'cents' 'newfound' 'personally' 'problems' 'cool' 'swinging' 'students'\n",
      " 'guys' 'pool' 'daniell' 'credence' 'penalized' 'knew' 'blank' 'stripped'\n",
      " 'production' 'shift' 'dispersal' 'office' 'victors' 'throws' 'relieved'\n",
      " 'goes' 'kkeller' 'stat' 'treated' 'interviews' 'vision' 'fight' 'sets'\n",
      " 'recognise' 'judge' 'tervio' 'tervo' 'specifications' 'speech' 'dmoney'\n",
      " 'ca' 'competent' 'onto' 'mount' 'galvint' 'asked' 'analysis' 'penalty'\n",
      " 'decibel' 'whether' 'continent' 'studio' 'gargle' 'players' 'blotted'\n",
      " 'hair' 'wing' 'ten' 'lets' 'ages' 'represents' 'interfered' 'eb'\n",
      " 'forwards' 'armor' 'died' 'prg' 'modest' 'almost' 'stark' 'wrote'\n",
      " 'replying' 'honor' 'golden' 'increased' 'triples' 'leaves' 'loss'\n",
      " 'scalpers' 'maniacal' 'flattering' 'file' 'fame' 'tactics' 'impressive'\n",
      " 'declined' 'must' 'abotu' 'tv' 'answer' 'considerably' 'populated'\n",
      " 'hilarious' 'bust' 'letter' 'stretcher' 'clearing' 'define' 'lights'\n",
      " 'shooting' 'retard' 'g' 'adjusting' 'minnesota' 'preceded' 'content'\n",
      " 'four' 'leafs' 'points' 'champion' 'header' 'coffee' 'proof' 'uptade'\n",
      " 'played' 'fashion' 'coincidence' 'followed' 'inexpensive' 'claiming'\n",
      " 'network' 'chosen' 'horizon' 'mask' 'physical' 'quoted' 'puts' 'tee'\n",
      " 'intelligence' 'fee' 'helmets' 'disciplined' 'fewest' 'whole' 'cartlidge'\n",
      " 'revenge' 'pick' 'filed' 'punches' 'sarcasm' 'carry' 'mid' 'assists'\n",
      " 'prove' 'dollar' 'fmsalvat' 'ing' 'guns' 'steady' 'rd' 'disclose' 'sheet'\n",
      " 'normally' 'evening' 'en' 'soured' 'skills' 'complained' 'blue' 'stomach'\n",
      " 'pigpens' 'keys' 'opposite' 'indicate' 'elementary' 'test' 'sort' 'begin'\n",
      " 'instant' 'tad' 'contributors' 'yell' 'adress' 'er' 'parts' 'spelling'\n",
      " 'bratt' 'concession' 'sent' 'contending' 'youth' 'tools' 'fire' 'full'\n",
      " 'deflected' 'cleared' 'assholes' 'selection' 'everywhere' 'measure'\n",
      " 'email' 'huot' 'possession' 'ques' 'chronological' 'memorable' 'protests'\n",
      " 'equally' 'drugs' 'package' 'disappointed' 'foolish' 'honest'\n",
      " 'appreciated' 'beautiful' 'advisedly' 'aggressive' 'read' 'networks'\n",
      " 'fair' 'hzazula' 'tremendously' 'house' 'quite' 'metro' 'dinger' 'unfair'\n",
      " 'doubter' 'slacelle' 'referring' 'saddled' 'garryola' 'sometimes'\n",
      " 'anyway' 'editor' 'gjp' 'generate' 'transplanted' 'draw' 'scorer'\n",
      " 'stuppid' 'copyright' 'contains' 'doubts' 'delaying' 'boards' 'simulcast'\n",
      " 'maynard' 'puck' 'circle' 'jerseys' 'quarters' 'rubbing' 'superb' 'crazy'\n",
      " 'tommorrow' 'pops' 'watch' 'passport' 'reading' 'topic' 'translation'\n",
      " 'shots' 'pete' 'card' 'conf' 'robin' 'unfortunate' 'married' 'breaker'\n",
      " 'concluded' 'center' 'massive' 'announced' 'history' 'circumstances'\n",
      " 'currently' 'simple' 'clip' 'rocket' 'pleasant' 'sp' 'dropping' 'shall'\n",
      " 'exact' 'admittedly' 'refer' 'interpretation' 'stacks' 'offensive'\n",
      " 'disputed' 'away' 'rebound' 'strikes' 'unhappy' 'fix' 'receiving'\n",
      " 'weather' 'appeared' 'approx' 'boutch' 'shining' 'legs' 'airwaves'\n",
      " 'trading' 'viable' 'mildly' 'severed' 'franchise' 'captain' 'taped'\n",
      " 'displaying' 'scored' 'participating' 'joking']\n",
      "(398,)\n"
     ]
    }
   ],
   "source": [
    "p_vocab_indcies, n_vocab_indcies = get_selected_prediction_arr(1, vocab_arr, all_words_indcies, weights)\n",
    "\n",
    "print(vocab_arr[p_vocab_indcies][:64])\n",
    "print(vocab_arr[p_vocab_indcies].shape)\n",
    "print()\n",
    "print(vocab_arr[n_vocab_indcies])\n",
    "print(vocab_arr[n_vocab_indcies].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ca310-6992-47d2-abba-6cfde0607313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
